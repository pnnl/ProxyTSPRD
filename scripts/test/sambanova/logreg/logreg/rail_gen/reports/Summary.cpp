{ // partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf
    TBuffer::Params partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_param;
    partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_param.buffer_depth = 1;
    // layout: <BF16[256, 136, 3, 1]RM/64@0x0> vec_order: {0, 1, 2, 3} vec_dim: 3 dims: {256, 136, 3, 32}
    partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{256, 136, 3, 1},{0, 1, 2, 3}).with_vector_transpose_dim(3).vector_align();
    auto partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_loc = loc("software/templates/src/templates/permute/rail/TBufferPermute.cpp", 69, 0);
    auto *partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf", kNoParent, partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_loc, &rail, partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_param);
    auto partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_W_loc = loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp", 469, 0);
    partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf->setup_addr(partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_W_loc, "W", [&](){
        auto x0 = imm(3);
        auto x1 = imm(1);
        auto x2 = imm(136);
        auto x3 = imm(32);
        auto x4 = imm(256);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, x1, x4, 1);
        auto x8 = dsl::iterator(x6, x1, x2, 1);
        auto x9 = dsl::iterator(x6, x1, x0, 1);
        auto x10 = dsl::iterator(x6, x3, x1, 1);
        // chain steps:  (104448 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x8, x9, x10});
        auto x12 = set_ctx_done_expr(x7.done());
        auto target = set_write_addr(std::vector<Value>{x7, x8, x9, x10});
        target.set_vec_dim(3);
        target.set_en_drop(x5);
        target.set_data( vector_in("8"));
        send(x12,  control_out("1000"));
        send(x12,  control_out("wdone__"));
    });
    auto partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_R_loc = loc("software/templates/src/templates/permute/rail/TBufferPermute.cpp", 73, 0);
    partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf->setup_addr(partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf_R_loc, "R", [&](){
        // pacing_window: default;
        enable_addr_splitting();
        auto x0 = imm(3);
        auto x1 = imm(1);
        auto x2 = imm(136);
        auto x3 = imm(32);
        auto x4 = imm(256);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, x1, x0, 1);
        auto x9 = dsl::iterator(x6, x1, x1, 1);
        auto x12 = dsl::iterator(x6, x1, x2, 1);
        auto x15 = dsl::iterator(x6, x3, x4, 1);
        auto x17 = ((((x7 < x0) & (x9 < x1)) & (x12 < x2)) & (x15 < x4));
        // chain steps:  (3264 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x9, x15, x12});
        auto x19 = set_ctx_done_expr(x7.done());
        auto target = set_read_addr(std::vector<Value>{x15, x12, x7, x9});
        target.set_vec_dim(0);
        target.set_en_zero(x5);
        target.set_en_drop(x17);
        target.set_data( vector_out("1001"));
        send(x19,  control_out("1000"));
        token_buffer( control_in("W_wdone_in__"), x19, 0);
    });
} // partition_0_0__LogregTorchSamba_partition_0_0__ptconvlstm__conv_layer__conv2d_weight_permute_tbuf
{ // partition_0_0__tbuf2u_0_0_197
    TBuffer::Params partition_0_0__tbuf2u_0_0_197_param;
    partition_0_0__tbuf2u_0_0_197_param.buffer_depth = 2;
    // layout: <BF16[60, 136]RM/64@0x0> vec_order: {0, 1} vec_dim: 1 dims: {60, 160}
    partition_0_0__tbuf2u_0_0_197_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{60, 136},{0, 1}).with_vector_transpose_dim(1).vector_align();
    auto partition_0_0__tbuf2u_0_0_197_loc = loc("LogregTorchSamba.cpp", 166, 0);
    auto *partition_0_0__tbuf2u_0_0_197 = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_0_0__tbuf2u_0_0_197", kNoParent, partition_0_0__tbuf2u_0_0_197_loc, &rail, partition_0_0__tbuf2u_0_0_197_param);
    auto partition_0_0__tbuf2u_0_0_197_kFrontDynamicWriteCtx_loc = loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp", 469, 0);
    partition_0_0__tbuf2u_0_0_197->setup_addr(partition_0_0__tbuf2u_0_0_197_kFrontDynamicWriteCtx_loc, "kFrontDynamicWriteCtx", [&](){
        auto x0 = imm(1);
        auto x2 = imm(32);
        auto x3 = imm(136);
        auto x4 = imm(60);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, x0, x4, 1);
        auto x8 = dsl::iterator(x6, x2, x3, 1);
        // chain steps:  (300 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x8});
        auto x10 = set_ctx_done_expr(x7.done());
        auto target = set_write_addr(std::vector<Value>{x7, x8});
        target.set_vec_dim(1);
        target.set_en_drop(x5);
        target.set_data( vector_in("8"));
        send(x10,  control_out("1000"));
        send(x10,  control_out("wdone__"));
    });
    auto partition_0_0__tbuf2u_0_0_197_LogregTorchSamba_partition_0_0__ptconvlstm__lambda_layer__indexselect_kLutRd_loc = loc("software/templates/src/templates/index/rail/Index.cpp", 137, 0);
    partition_0_0__tbuf2u_0_0_197->setup_addr(partition_0_0__tbuf2u_0_0_197_LogregTorchSamba_partition_0_0__ptconvlstm__lambda_layer__indexselect_kLutRd_loc, "LogregTorchSamba_partition_0_0__ptconvlstm__lambda_layer__indexselect_kLutRd", [&](){
        // pacing_window: default;
        enable_addr_splitting();
        auto x0 = imm(1);
        auto x2 = imm(32);
        auto x3 = imm(136);
        auto x4 = imm(60);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 =  scalar_in("idx");
        token_buffer(x7, 0);
        auto x8 = dsl::iterator(x6, x0, imm(3), 1);
        auto x9 = dsl::iterator(x6, x2, x3, 1);
        // chain steps:  (15 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x8, x9});
        auto x11 = set_ctx_done_expr(x8.done());
        auto target = set_read_addr(std::vector<Value>{x7, x9});
        target.set_vec_dim(1);
        target.set_en_zero((x7 < x4));
        target.set_en_drop(x5);
        target.set_data( vector_out("1001"));
        token_buffer( control_in("0"), x11, 1);
        token_buffer( control_in("1"), x11, 2);
        send(x11,  control_out("1000"));
        token_buffer( control_in("kFrontDynamicWriteCtx_wdone_in__"), x11, 0);
    });
} // partition_0_0__tbuf2u_0_0_197
{ // partition_0_0__tbuf1u_0_0_198
    TBuffer::Params partition_0_0__tbuf1u_0_0_198_param;
    partition_0_0__tbuf1u_0_0_198_param.buffer_depth = 1;
    // layout: <INT32[3]RM@0x0> vec_order: {0} vec_dim: 0 dims: {3}
    partition_0_0__tbuf1u_0_0_198_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("INT32"),{3},{0}).with_vector_transpose_dim(0);
    auto partition_0_0__tbuf1u_0_0_198_loc = loc("LogregTorchSamba.cpp", 177, 0);
    auto *partition_0_0__tbuf1u_0_0_198 = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_0_0__tbuf1u_0_0_198", kNoParent, partition_0_0__tbuf1u_0_0_198_loc, &rail, partition_0_0__tbuf1u_0_0_198_param);
    auto partition_0_0__tbuf1u_0_0_198_kFrontDynamicWriteCtx_loc = loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp", 469, 0);
    partition_0_0__tbuf1u_0_0_198->setup_addr(partition_0_0__tbuf1u_0_0_198_kFrontDynamicWriteCtx_loc, "kFrontDynamicWriteCtx", [&](){
        auto x2 = imm(3);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, imm(16), x2, 1);
        // chain steps:  (1 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7});
        auto x9 = set_ctx_done_expr(x7.done());
        auto target = set_write_addr(std::vector<Value>{x7});
        target.set_vec_dim(0);
        target.set_en_drop(x5);
        target.set_data( vector_in("8"));
        send(x9,  control_out("1000"));
        send(x9,  control_out("wdone__"));
    });
    auto partition_0_0__tbuf1u_0_0_198_LogregTorchSamba_partition_0_0__ptconvlstm__lambda_layer__indexselect_kIndexRd_loc = loc("software/templates/src/templates/index/rail/Index.cpp", 124, 0);
    partition_0_0__tbuf1u_0_0_198->setup_addr(partition_0_0__tbuf1u_0_0_198_LogregTorchSamba_partition_0_0__ptconvlstm__lambda_layer__indexselect_kIndexRd_loc, "LogregTorchSamba_partition_0_0__ptconvlstm__lambda_layer__indexselect_kIndexRd", [&](){
        // pacing_window: default;
        auto x2 = imm(3);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, imm(1), x2, 1);
        // chain steps:  (15 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, dsl::iterator(x6, imm(32), imm(136), 1)});
        auto x10 = set_ctx_done_expr(x7.done());
        auto target = set_read_addr(std::vector<Value>{x7});
        target.set_vec_dim(-1);
        target.set_en_zero(x5);
        target.set_en_drop(x5);
        target.set_data( scalar_out("1001"));
        send(x10,  control_out("1000"));
        token_buffer( control_in("kFrontDynamicWriteCtx_wdone_in__"), x10, 0);
    });
} // partition_0_0__tbuf1u_0_0_198
{ // partition_0_0__tbuf2u_0_0_202
    TBuffer::Params partition_0_0__tbuf2u_0_0_202_param;
    partition_0_0__tbuf2u_0_0_202_param.buffer_depth = 2;
    // layout: <BF16[136, 3]CM/64@0x0> vec_order: {1, 0} vec_dim: 0 dims: {160, 3}
    partition_0_0__tbuf2u_0_0_202_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{136, 3},{1, 0}).with_vector_transpose_dim(1).vector_align();
    auto partition_0_0__tbuf2u_0_0_202_loc = loc("LogregTorchSamba.cpp", 201, 0);
    auto *partition_0_0__tbuf2u_0_0_202 = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_0_0__tbuf2u_0_0_202", kNoParent, partition_0_0__tbuf2u_0_0_202_loc, &rail, partition_0_0__tbuf2u_0_0_202_param);
    auto partition_0_0__tbuf2u_0_0_202_LogregTorchSamba_partition_0_0__ptconvlstm__lambda_layer__indexselect_kOutWr_loc = loc("software/templates/src/templates/index/rail/Index.cpp", 161, 0);
    partition_0_0__tbuf2u_0_0_202->setup_addr(partition_0_0__tbuf2u_0_0_202_LogregTorchSamba_partition_0_0__ptconvlstm__lambda_layer__indexselect_kOutWr_loc, "LogregTorchSamba_partition_0_0__ptconvlstm__lambda_layer__indexselect_kOutWr", [&](){
        enable_addr_splitting();
        auto x0 = imm(32);
        auto x1 = imm(136);
        auto x2 = imm(1);
        auto x3 = imm(3);
        auto x4 = boolean(true);
        auto x5 = imm(0);
        auto x6 = dsl::iterator(x5, x2, x3, 1);
        auto x7 = dsl::iterator(x5, x0, x1, 1);
        // chain steps:  (15 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x6, x7});
        auto x9 = set_ctx_done_expr(x6.done());
        auto target = set_write_addr(std::vector<Value>{x7, x6});
        target.set_vec_dim(0);
        target.set_en_drop(x4);
        target.set_data( vector_in("8"));
        send(x9,  control_out("1000"));
        send(x9,  control_out("wdone__"));
    });
    auto partition_0_0__tbuf2u_0_0_202_kDefaultRead1_loc = loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp", 469, 0);
    partition_0_0__tbuf2u_0_0_202->setup_addr(partition_0_0__tbuf2u_0_0_202_kDefaultRead1_loc, "kDefaultRead1", [&](){
        // pacing_window: default;
        auto x0 = imm(32);
        auto x1 = imm(136);
        auto x2 = imm(1);
        auto x3 = imm(3);
        auto x4 = boolean(true);
        auto x5 = imm(0);
        auto x6 = dsl::iterator(x5, x0, x1, 1);
        auto x7 = dsl::iterator(x5, x2, x3, 1);
        // chain steps:  (15 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x6});
        auto x9 = set_ctx_done_expr(x7.done());
        auto target = set_read_addr(std::vector<Value>{x6, x7});
        target.set_vec_dim(0);
        target.set_en_zero(x4);
        target.set_en_drop(x4);
        target.set_data( vector_out("1001"));
        token_buffer( control_in("0"), x9, 2);
        token_buffer( control_in("1"), x9, 2);
        send(x9,  control_out("1000"));
        auto x14 =  control_in("LogregTorchSamba.partition_0_0_.ptconvlstm__lambda_layer__indexselect@kOutWr_wdone_in__");
        token_buffer(x14, x9, 0);
    });
} // partition_0_0__tbuf2u_0_0_202
{ // partition_1_0__tbuf2u_1_0_253
    TBuffer::Params partition_1_0__tbuf2u_1_0_253_param;
    partition_1_0__tbuf2u_1_0_253_param.buffer_depth = 2;
    // layout: <BF16[60, 136]RM/64@0x0> vec_order: {0, 1} vec_dim: 1 dims: {60, 160}
    partition_1_0__tbuf2u_1_0_253_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{60, 136},{0, 1}).with_vector_transpose_dim(1).vector_align();
    auto partition_1_0__tbuf2u_1_0_253_loc = loc("LogregTorchSamba.cpp", 1937, 0);
    auto *partition_1_0__tbuf2u_1_0_253 = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_1_0__tbuf2u_1_0_253", kNoParent, partition_1_0__tbuf2u_1_0_253_loc, &rail, partition_1_0__tbuf2u_1_0_253_param);
    auto partition_1_0__tbuf2u_1_0_253_kFrontDynamicWriteCtx_loc = loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp", 469, 0);
    partition_1_0__tbuf2u_1_0_253->setup_addr(partition_1_0__tbuf2u_1_0_253_kFrontDynamicWriteCtx_loc, "kFrontDynamicWriteCtx", [&](){
        auto x0 = imm(1);
        auto x2 = imm(32);
        auto x3 = imm(136);
        auto x4 = imm(60);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, x0, x4, 1);
        auto x8 = dsl::iterator(x6, x2, x3, 1);
        // chain steps:  (300 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x8});
        auto x10 = set_ctx_done_expr(x7.done());
        auto target = set_write_addr(std::vector<Value>{x7, x8});
        target.set_vec_dim(1);
        target.set_en_drop(x5);
        target.set_data( vector_in("8"));
        send(x10,  control_out("1000"));
        send(x10,  control_out("wdone__"));
    });
    auto partition_1_0__tbuf2u_1_0_253_LogregTorchSamba_partition_1_0__ptconvlstm__lambda_layer__indexselect_recompute__kLutRd_loc = loc("software/templates/src/templates/index/rail/Index.cpp", 137, 0);
    partition_1_0__tbuf2u_1_0_253->setup_addr(partition_1_0__tbuf2u_1_0_253_LogregTorchSamba_partition_1_0__ptconvlstm__lambda_layer__indexselect_recompute__kLutRd_loc, "LogregTorchSamba_partition_1_0__ptconvlstm__lambda_layer__indexselect_recompute__kLutRd", [&](){
        // pacing_window: default;
        enable_addr_splitting();
        auto x0 = imm(1);
        auto x2 = imm(32);
        auto x3 = imm(136);
        auto x4 = imm(60);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 =  scalar_in("idx");
        token_buffer(x7, 0);
        auto x8 = dsl::iterator(x6, x0, imm(3), 1);
        auto x9 = dsl::iterator(x6, x2, x3, 1);
        // chain steps:  (15 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x8, x9});
        auto x11 = set_ctx_done_expr(x8.done());
        auto target = set_read_addr(std::vector<Value>{x7, x9});
        target.set_vec_dim(1);
        target.set_en_zero((x7 < x4));
        target.set_en_drop(x5);
        target.set_data( vector_out("1001"));
        token_buffer( control_in("0"), x11, 1);
        token_buffer( control_in("1"), x11, 2);
        send(x11,  control_out("1000"));
        token_buffer( control_in("kFrontDynamicWriteCtx_wdone_in__"), x11, 0);
    });
} // partition_1_0__tbuf2u_1_0_253
{ // partition_1_0__tbuf1u_1_0_254
    TBuffer::Params partition_1_0__tbuf1u_1_0_254_param;
    partition_1_0__tbuf1u_1_0_254_param.buffer_depth = 1;
    // layout: <INT32[3]RM@0x0> vec_order: {0} vec_dim: 0 dims: {3}
    partition_1_0__tbuf1u_1_0_254_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("INT32"),{3},{0}).with_vector_transpose_dim(0);
    auto partition_1_0__tbuf1u_1_0_254_loc = loc("LogregTorchSamba.cpp", 1948, 0);
    auto *partition_1_0__tbuf1u_1_0_254 = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_1_0__tbuf1u_1_0_254", kNoParent, partition_1_0__tbuf1u_1_0_254_loc, &rail, partition_1_0__tbuf1u_1_0_254_param);
    auto partition_1_0__tbuf1u_1_0_254_kFrontDynamicWriteCtx_loc = loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp", 469, 0);
    partition_1_0__tbuf1u_1_0_254->setup_addr(partition_1_0__tbuf1u_1_0_254_kFrontDynamicWriteCtx_loc, "kFrontDynamicWriteCtx", [&](){
        auto x2 = imm(3);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, imm(16), x2, 1);
        // chain steps:  (1 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7});
        auto x9 = set_ctx_done_expr(x7.done());
        auto target = set_write_addr(std::vector<Value>{x7});
        target.set_vec_dim(0);
        target.set_en_drop(x5);
        target.set_data( vector_in("8"));
        send(x9,  control_out("1000"));
        send(x9,  control_out("wdone__"));
    });
    auto partition_1_0__tbuf1u_1_0_254_LogregTorchSamba_partition_1_0__ptconvlstm__lambda_layer__indexselect_recompute__kIndexRd_loc = loc("software/templates/src/templates/index/rail/Index.cpp", 124, 0);
    partition_1_0__tbuf1u_1_0_254->setup_addr(partition_1_0__tbuf1u_1_0_254_LogregTorchSamba_partition_1_0__ptconvlstm__lambda_layer__indexselect_recompute__kIndexRd_loc, "LogregTorchSamba_partition_1_0__ptconvlstm__lambda_layer__indexselect_recompute__kIndexRd", [&](){
        // pacing_window: default;
        auto x2 = imm(3);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, imm(1), x2, 1);
        // chain steps:  (15 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, dsl::iterator(x6, imm(32), imm(136), 1)});
        auto x10 = set_ctx_done_expr(x7.done());
        auto target = set_read_addr(std::vector<Value>{x7});
        target.set_vec_dim(-1);
        target.set_en_zero(x5);
        target.set_en_drop(x5);
        target.set_data( scalar_out("1001"));
        send(x10,  control_out("1000"));
        token_buffer( control_in("kFrontDynamicWriteCtx_wdone_in__"), x10, 0);
    });
} // partition_1_0__tbuf1u_1_0_254
{ // partition_1_0__tbuf2u_1_0_258
    TBuffer::Params partition_1_0__tbuf2u_1_0_258_param;
    partition_1_0__tbuf2u_1_0_258_param.buffer_depth = 2;
    // layout: <BF16[136, 3]CM/64@0x0> vec_order: {1, 0} vec_dim: 0 dims: {160, 3}
    partition_1_0__tbuf2u_1_0_258_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{136, 3},{1, 0}).with_vector_transpose_dim(1).vector_align();
    auto partition_1_0__tbuf2u_1_0_258_loc = loc("LogregTorchSamba.cpp", 1972, 0);
    auto *partition_1_0__tbuf2u_1_0_258 = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_1_0__tbuf2u_1_0_258", kNoParent, partition_1_0__tbuf2u_1_0_258_loc, &rail, partition_1_0__tbuf2u_1_0_258_param);
    auto partition_1_0__tbuf2u_1_0_258_LogregTorchSamba_partition_1_0__ptconvlstm__lambda_layer__indexselect_recompute__kOutWr_loc = loc("software/templates/src/templates/index/rail/Index.cpp", 161, 0);
    partition_1_0__tbuf2u_1_0_258->setup_addr(partition_1_0__tbuf2u_1_0_258_LogregTorchSamba_partition_1_0__ptconvlstm__lambda_layer__indexselect_recompute__kOutWr_loc, "LogregTorchSamba_partition_1_0__ptconvlstm__lambda_layer__indexselect_recompute__kOutWr", [&](){
        enable_addr_splitting();
        auto x0 = imm(32);
        auto x1 = imm(136);
        auto x2 = imm(1);
        auto x3 = imm(3);
        auto x4 = boolean(true);
        auto x5 = imm(0);
        auto x6 = dsl::iterator(x5, x2, x3, 1);
        auto x7 = dsl::iterator(x5, x0, x1, 1);
        // chain steps:  (15 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x6, x7});
        auto x9 = set_ctx_done_expr(x6.done());
        auto target = set_write_addr(std::vector<Value>{x7, x6});
        target.set_vec_dim(0);
        target.set_en_drop(x4);
        target.set_data( vector_in("8"));
        send(x9,  control_out("1000"));
        send(x9,  control_out("wdone__"));
    });
    auto partition_1_0__tbuf2u_1_0_258_kDefaultRead1_loc = loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp", 469, 0);
    partition_1_0__tbuf2u_1_0_258->setup_addr(partition_1_0__tbuf2u_1_0_258_kDefaultRead1_loc, "kDefaultRead1", [&](){
        // pacing_window: default;
        auto x0 = imm(32);
        auto x1 = imm(136);
        auto x2 = imm(1);
        auto x3 = imm(3);
        auto x4 = boolean(true);
        auto x5 = imm(0);
        auto x6 = dsl::iterator(x5, x0, x1, 1);
        auto x7 = dsl::iterator(x5, x2, x3, 1);
        // chain steps:  (15 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x6});
        auto x9 = set_ctx_done_expr(x7.done());
        auto target = set_read_addr(std::vector<Value>{x6, x7});
        target.set_vec_dim(0);
        target.set_en_zero(x4);
        target.set_en_drop(x4);
        target.set_data( vector_out("1001"));
        token_buffer( control_in("0"), x9, 2);
        token_buffer( control_in("1"), x9, 2);
        send(x9,  control_out("1000"));
        auto x14 =  control_in("LogregTorchSamba.partition_1_0_.ptconvlstm__lambda_layer__indexselect_recompute_@kOutWr_wdone_in__");
        token_buffer(x14, x9, 0);
    });
} // partition_1_0__tbuf2u_1_0_258
{ // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf
    TBuffer::Params partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_param;
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_param.buffer_depth = 1;
    // layout: <BF16[4096, 256]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 dims: {4096, 256}
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{4096, 256},{1, 0}).with_vector_transpose_dim(0).vector_align();
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 578, 0);
    auto *partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf", kNoParent, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_loc, &rail, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_param);
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_0_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_0_0_loc, "w_loop_0_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(0);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x35, x30, x0, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x35) & (x37 < x0)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_1_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_1_0_loc, "w_loop_1_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(1);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x0, x30, x33, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x0) & (x37 < x33)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_2_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_2_0_loc, "w_loop_2_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(2);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x33, x30, x1, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x33) & (x37 < x1)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_3_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_3_0_loc, "w_loop_3_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(3);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x1, x30, x2, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x1) & (x37 < x2)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_4_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_4_0_loc, "w_loop_4_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(4);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x2, x30, x3, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x2) & (x37 < x3)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_5_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_5_0_loc, "w_loop_5_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(5);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x3, x30, x4, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x3) & (x37 < x4)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_6_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_6_0_loc, "w_loop_6_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(6);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x4, x30, x5, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x4) & (x37 < x5)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_7_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_7_0_loc, "w_loop_7_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(7);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x5, x30, x6, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x5) & (x37 < x6)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_8_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_8_0_loc, "w_loop_8_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(8);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x6, x30, x7, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x6) & (x37 < x7)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_9_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_9_0_loc, "w_loop_9_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(9);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x7, x30, x8, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x7) & (x37 < x8)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_10_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_10_0_loc, "w_loop_10_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(10);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x8, x30, x9, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x8) & (x37 < x9)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_11_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_11_0_loc, "w_loop_11_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(11);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x9, x30, x10, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x9) & (x37 < x10)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_12_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_12_0_loc, "w_loop_12_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(12);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x10, x30, x11, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x10) & (x37 < x11)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_13_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_13_0_loc, "w_loop_13_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(13);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x11, x30, x12, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x11) & (x37 < x12)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_14_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_14_0_loc, "w_loop_14_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(14);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x12, x30, x13, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x12) & (x37 < x13)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_15_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_15_0_loc, "w_loop_15_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(15);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x13, x30, x14, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x13) & (x37 < x14)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_16_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_16_0_loc, "w_loop_16_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(16);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x14, x30, x15, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x14) & (x37 < x15)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_17_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_17_0_loc, "w_loop_17_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(17);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x15, x30, x16, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x15) & (x37 < x16)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_18_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_18_0_loc, "w_loop_18_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(18);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x16, x30, x17, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x16) & (x37 < x17)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_19_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_19_0_loc, "w_loop_19_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(19);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x17, x30, x18, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x17) & (x37 < x18)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_20_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_20_0_loc, "w_loop_20_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(20);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x18, x30, x19, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x18) & (x37 < x19)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_21_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_21_0_loc, "w_loop_21_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(21);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x19, x30, x20, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x19) & (x37 < x20)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_22_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_22_0_loc, "w_loop_22_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(22);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x20, x30, x21, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x20) & (x37 < x21)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_23_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_23_0_loc, "w_loop_23_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(23);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x21, x30, x22, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x21) & (x37 < x22)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_24_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_24_0_loc, "w_loop_24_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(24);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x22, x30, x23, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x22) & (x37 < x23)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_25_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_25_0_loc, "w_loop_25_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(25);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x23, x30, x24, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x23) & (x37 < x24)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_26_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_26_0_loc, "w_loop_26_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(26);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x24, x30, x25, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x24) & (x37 < x25)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_27_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_27_0_loc, "w_loop_27_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(27);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x25, x30, x26, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x25) & (x37 < x26)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_28_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_28_0_loc, "w_loop_28_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(28);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x26, x30, x27, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x26) & (x37 < x27)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_29_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_29_0_loc, "w_loop_29_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(29);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x27, x30, x28, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x27) & (x37 < x28)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_30_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_30_0_loc, "w_loop_30_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(30);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x28, x30, x29, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x28) & (x37 < x29)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_31_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_w_loop_31_0_loc, "w_loop_31_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(31);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x37 = dsl::iterator(x29, x30, x31, 1);
        auto x38 = dsl::iterator(x35, x32, x33, 1);
        auto x40 = set_ctx_done_expr(x37.done());
        // chain steps:  (1024 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x35, x32, x32, 1), x37, x38});
        auto target = set_write_addr(std::vector<Value>{x37, x38});
        target.set_vec_dim(0);
        target.set_en_drop(((x37 >= x29) & (x37 < x31)));
        target.set_data( vector_in("8"));
        send(x37.done(),  control_out("w_loop_done"));
        send(x40,  control_out("1000"));
        send(x40,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_kBackReadCtx_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 629, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf_kBackReadCtx_loc, "kBackReadCtx", [&](){
        disable_empty_stall(true, 0);
        set_pacing_window(8192);
        set_mem_port(node_constants::PmuOp::kRead0);
        auto x0 = imm(128);
        auto x1 = imm(384);
        auto x2 = imm(512);
        auto x3 = imm(640);
        auto x4 = imm(768);
        auto x5 = imm(896);
        auto x6 = imm(1024);
        auto x7 = imm(1152);
        auto x8 = imm(1280);
        auto x9 = imm(1408);
        auto x10 = imm(1536);
        auto x11 = imm(1664);
        auto x12 = imm(1792);
        auto x13 = imm(1920);
        auto x14 = imm(2048);
        auto x15 = imm(2176);
        auto x16 = imm(2304);
        auto x17 = imm(2432);
        auto x18 = imm(2560);
        auto x19 = imm(2688);
        auto x20 = imm(2816);
        auto x21 = imm(2944);
        auto x22 = imm(3072);
        auto x23 = imm(3200);
        auto x24 = imm(3328);
        auto x25 = imm(3456);
        auto x26 = imm(3584);
        auto x27 = imm(3712);
        auto x28 = imm(3840);
        auto x29 = imm(3968);
        auto x30 = imm(32);
        auto x31 = imm(4080);
        auto x32 = imm(1);
        auto x33 = imm(256);
        auto x34 = boolean(true);
        auto x35 = imm(0);
        auto x36 = dsl::iterator(x35, x30, x31, 1);
        auto x38 = set_ctx_done_expr(x36.done());
        auto x39 = dsl::iterator(x35, x32, x33, 1);
        // chain steps:  (32768 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x36, x39});
        auto target = set_read_addr(std::vector<Value>{x36, x39});
        target.set_vec_dim(0);
        target.set_en_zero(x34);
        target.set_en_drop(x34);
        target.set_data( vector_out("1001"));
        token_buffer( control_in("pacing_in"), get_pacing_done_expr(8192, x36), 1);
        send(x38,  control_out("1000"));
        token_buffer( control_in("w_loop_0_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_1_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_2_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_3_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_4_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_5_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_6_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_7_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_8_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_9_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_10_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_11_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_12_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_13_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_14_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_15_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_16_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_17_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_18_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_19_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_20_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_21_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_22_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_23_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_24_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_25_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_26_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_27_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_28_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_29_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_30_0_wdone_in__"), x38, 0);
        token_buffer( control_in("w_loop_31_0_wdone_in__"), x38, 0);
    });
} // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf
{ // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf
    TBuffer::Params partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_param;
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_param.buffer_depth = 1;
    // layout: <BF16[4096, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 dims: {4096, 1}
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{4096, 1},{1, 0}).with_vector_transpose_dim(0).vector_align();
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 578, 0);
    auto *partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf", kNoParent, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_loc, &rail, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_param);
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_w_loop_0_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_w_loop_0_0_loc, "w_loop_0_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(0);
        auto x0 = imm(32);
        auto x1 = imm(4096);
        auto x2 = imm(1);
        auto x3 = boolean(true);
        auto x4 = imm(0);
        auto x6 = dsl::iterator(x4, x0, x1, 1);
        auto x7 = dsl::iterator(x4, x2, x2, 1);
        auto x9 = set_ctx_done_expr(x6.done());
        // chain steps:  (128 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x4, x2, x2, 1), x6, x7});
        auto target = set_write_addr(std::vector<Value>{x6, x7});
        target.set_vec_dim(0);
        target.set_en_drop(((x6 >= x4) & (x6 < x1)));
        target.set_data( vector_in("8"));
        send(x6.done(),  control_out("w_loop_done"));
        send(x9,  control_out("1000"));
        send(x9,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_kBackReadCtx_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 629, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf_kBackReadCtx_loc, "kBackReadCtx", [&](){
        disable_empty_stall(true, 0);
        set_pacing_window(128);
        set_mem_port(node_constants::PmuOp::kRead0);
        auto x0 = imm(32);
        auto x1 = imm(4096);
        auto x2 = imm(1);
        auto x3 = boolean(true);
        auto x4 = imm(0);
        auto x5 = dsl::iterator(x4, x0, x1, 1);
        auto x7 = set_ctx_done_expr(x5.done());
        auto x8 = dsl::iterator(x4, x2, x2, 1);
        // chain steps:  (128 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x5, x8});
        auto target = set_read_addr(std::vector<Value>{x5, x8});
        target.set_vec_dim(0);
        target.set_en_zero(x3);
        target.set_en_drop(x3);
        target.set_data( vector_out("1001"));
        token_buffer( control_in("pacing_in"), get_pacing_done_expr(128, x5), 4);
        send(x7,  control_out("1000"));
        token_buffer( control_in("w_loop_0_0_wdone_in__"), x7, 0);
    });
} // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf
{ // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf
    TBuffer::Params partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_param;
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_param.buffer_depth = 1;
    // layout: <BF16[122880, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 dims: {122880, 1}
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{122880, 1},{1, 0}).with_vector_transpose_dim(0).vector_align();
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 578, 0);
    auto *partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf", kNoParent, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_loc, &rail, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_param);
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_w_loop_0_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_w_loop_0_0_loc, "w_loop_0_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(0);
        auto x0 = imm(32);
        auto x1 = imm(122880);
        auto x2 = imm(1);
        auto x3 = boolean(true);
        auto x4 = imm(0);
        auto x6 = dsl::iterator(x4, x0, x1, 1);
        auto x7 = dsl::iterator(x4, x2, x2, 1);
        auto x9 = set_ctx_done_expr(x6.done());
        // chain steps:  (3840 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x4, x2, x2, 1), x6, x7});
        auto target = set_write_addr(std::vector<Value>{x6, x7});
        target.set_vec_dim(0);
        target.set_en_drop(((x6 >= x4) & (x6 < x1)));
        target.set_data( vector_in("8"));
        send(x6.done(),  control_out("w_loop_done"));
        send(x9,  control_out("1000"));
        send(x9,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_kBackReadCtx_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 629, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf_kBackReadCtx_loc, "kBackReadCtx", [&](){
        disable_empty_stall(true, 0);
        // pacing_window: default;
        set_mem_port(node_constants::PmuOp::kRead0);
        auto x0 = imm(32);
        auto x1 = imm(122880);
        auto x2 = imm(1);
        auto x3 = boolean(true);
        auto x4 = imm(0);
        auto x5 = dsl::iterator(x4, x0, x1, 1);
        auto x7 = set_ctx_done_expr(x5.done());
        auto x8 = dsl::iterator(x4, x2, x2, 1);
        // chain steps:  (3840 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x5, x8});
        auto target = set_read_addr(std::vector<Value>{x5, x8});
        target.set_vec_dim(0);
        target.set_en_zero(x3);
        target.set_en_drop(x3);
        target.set_data( vector_out("1001"));
        send(x7,  control_out("1000"));
        token_buffer( control_in("w_loop_0_0_wdone_in__"), x7, 0);
    });
} // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_tbuf
{ // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf
    TBuffer::Params partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_param;
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_param.buffer_depth = 1;
    // layout: <BF16[256, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 dims: {256, 1}
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{256, 1},{1, 0}).with_vector_transpose_dim(0).vector_align();
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 578, 0);
    auto *partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf", kNoParent, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_loc, &rail, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_param);
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_w_loop_0_0_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 424, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_w_loop_0_0_loc, "w_loop_0_0", [&](){
        disable_full_stall();
        set_explicit_dispatch(0);
        auto x0 = imm(32);
        auto x1 = imm(256);
        auto x2 = imm(1);
        auto x3 = boolean(true);
        auto x4 = imm(0);
        auto x6 = dsl::iterator(x4, x0, x1, 1);
        auto x7 = dsl::iterator(x4, x2, x2, 1);
        auto x9 = set_ctx_done_expr(x6.done());
        // chain steps:  (8 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{dsl::iterator(x4, x2, x2, 1), x6, x7});
        auto target = set_write_addr(std::vector<Value>{x6, x7});
        target.set_vec_dim(0);
        target.set_en_drop(((x6 >= x4) & (x6 < x1)));
        target.set_data( vector_in("8"));
        send(x6.done(),  control_out("w_loop_done"));
        send(x9,  control_out("1000"));
        send(x9,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_kBackReadCtx_loc = loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp", 629, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf_kBackReadCtx_loc, "kBackReadCtx", [&](){
        disable_empty_stall(true, 0);
        set_pacing_window(8);
        set_mem_port(node_constants::PmuOp::kRead0);
        auto x0 = imm(32);
        auto x1 = imm(256);
        auto x2 = imm(1);
        auto x3 = boolean(true);
        auto x4 = imm(0);
        auto x5 = dsl::iterator(x4, x0, x1, 1);
        auto x7 = set_ctx_done_expr(x5.done());
        auto x8 = dsl::iterator(x4, x2, x2, 1);
        // chain steps:  (8 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x5, x8});
        auto target = set_read_addr(std::vector<Value>{x5, x8});
        target.set_vec_dim(0);
        target.set_en_zero(x3);
        target.set_en_drop(x3);
        target.set_data( vector_out("1001"));
        token_buffer( control_in("pacing_in"), get_pacing_done_expr(8, x5), 4);
        send(x7,  control_out("1000"));
        token_buffer( control_in("w_loop_0_0_wdone_in__"), x7, 0);
    });
} // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_bwd_accum_1_tbuf
{ // partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf
    TBuffer::Params partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_param;
    partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_param.buffer_depth = 1;
    // layout: <BF16[256, 3, 1, 136]RM/64@0x0> vec_order: {0, 1, 2, 3} vec_dim: 3 dims: {256, 3, 1, 160}
    partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{256, 3, 1, 136},{0, 1, 2, 3}).with_vector_transpose_dim(3).vector_align();
    auto partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_loc = loc("software/templates/src/templates/permute/rail/TBufferPermute.cpp", 69, 0);
    auto *partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf", kNoParent, partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_loc, &rail, partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_param);
    auto partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_W_loc = loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp", 469, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_W_loc, "W", [&](){
        auto x0 = imm(3);
        auto x1 = imm(1);
        auto x2 = imm(136);
        auto x3 = imm(32);
        auto x4 = imm(256);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, x1, x4, 1);
        auto x8 = dsl::iterator(x6, x1, x0, 1);
        auto x9 = dsl::iterator(x6, x1, x1, 1);
        auto x10 = dsl::iterator(x6, x3, x2, 1);
        // chain steps:  (3840 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x8, x9, x10});
        auto x12 = set_ctx_done_expr(x7.done());
        auto target = set_write_addr(std::vector<Value>{x7, x8, x9, x10});
        target.set_vec_dim(3);
        target.set_en_drop(x5);
        target.set_data( vector_in("8"));
        send(x12,  control_out("1000"));
        send(x12,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_R_loc = loc("software/templates/src/templates/permute/rail/TBufferPermute.cpp", 73, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf_R_loc, "R", [&](){
        // pacing_window: default;
        enable_addr_splitting();
        auto x0 = imm(3);
        auto x1 = imm(1);
        auto x2 = imm(136);
        auto x3 = imm(32);
        auto x4 = imm(256);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, x1, x0, 1);
        auto x9 = dsl::iterator(x6, x1, x1, 1);
        auto x12 = dsl::iterator(x6, x1, x2, 1);
        auto x15 = dsl::iterator(x6, x3, x4, 1);
        auto x17 = ((((x7 < x0) & (x9 < x1)) & (x12 < x2)) & (x15 < x4));
        // chain steps:  (3264 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x9, x15, x12});
        auto x19 = set_ctx_done_expr(x7.done());
        auto target = set_read_addr(std::vector<Value>{x15, x7, x9, x12});
        target.set_vec_dim(0);
        target.set_en_zero(x5);
        target.set_en_drop(x17);
        target.set_data( vector_out("1001"));
        send(x19,  control_out("1000"));
        token_buffer( control_in("W_wdone_in__"), x19, 0);
    });
} // partition_1_0__LogregTorchSamba_partition_1_0__permute_1_0_425_tbuf
{ // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf
    TBuffer::Params partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_param;
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_param.buffer_depth = 1;
    // layout: <BF16[3, 1, 136, 256]CVRM/64@0x0> vec_order: {0, 1, 2, 3} vec_dim: 2 dims: {3, 1, 160, 256}
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_param.layout = TensorLayout(node_constants::TemplateDataFormats::parse("BF16"),{3, 1, 136, 256},{0, 1, 3, 2}).with_vector_transpose_dim(2).vector_align();
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_loc = loc("software/templates/src/templates/permute/rail/TBufferPermute.cpp", 69, 0);
    auto *partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf = rail.plasma()->create_node<prism::plasma::TBuffer>("partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf", kNoParent, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_loc, &rail, partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_param);
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_W_loc = loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp", 469, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_W_loc, "W", [&](){
        auto x0 = imm(256);
        auto x1 = imm(136);
        auto x2 = imm(32);
        auto x3 = imm(3);
        auto x4 = imm(1);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, x4, x3, 1);
        auto x8 = dsl::iterator(x6, x4, x4, 1);
        auto x9 = dsl::iterator(x6, x2, x1, 1);
        auto x10 = dsl::iterator(x6, x4, x0, 1);
        // chain steps:  (3840 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x8, x9, x10});
        auto x12 = set_ctx_done_expr(x7.done());
        auto target = set_write_addr(std::vector<Value>{x7, x8, x9, x10});
        target.set_vec_dim(2);
        target.set_en_drop(x5);
        target.set_data( vector_in("8"));
        send(x12,  control_out("1000"));
        send(x12,  control_out("wdone__"));
    });
    auto partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_R_loc = loc("software/templates/src/templates/permute/rail/TBufferPermute.cpp", 73, 0);
    partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf->setup_addr(partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf_R_loc, "R", [&](){
        // pacing_window: default;
        enable_addr_splitting();
        auto x0 = imm(256);
        auto x1 = imm(136);
        auto x2 = imm(32);
        auto x3 = imm(3);
        auto x4 = imm(1);
        auto x5 = boolean(true);
        auto x6 = imm(0);
        auto x7 = dsl::iterator(x6, x4, x0, 1);
        auto x9 = dsl::iterator(x6, x4, x1, 1);
        auto x12 = dsl::iterator(x6, x2, x3, 1);
        auto x15 = dsl::iterator(x6, x4, x4, 1);
        auto x17 = ((((x7 < x0) & (x9 < x1)) & (x12 < x3)) & (x15 < x4));
        // chain steps:  (34816 # steps): 
        set_iterchain("addr_chain", std::vector<Value>{x7, x9, x12, x15});
        auto x19 = set_ctx_done_expr(x7.done());
        auto target = set_read_addr(std::vector<Value>{x12, x15, x9, x7});
        target.set_vec_dim(0);
        target.set_en_zero(x5);
        target.set_en_drop(x17);
        target.set_data( vector_out("1001"));
        send(x19,  control_out("1000"));
        token_buffer( control_in("W_wdone_in__"), x19, 0);
    });
} // partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__conv_layer__conv2d_weight_permute_bwd_tbuf
