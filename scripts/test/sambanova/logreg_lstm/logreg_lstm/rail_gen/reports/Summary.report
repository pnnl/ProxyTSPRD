========= Summary IR ========
tbuffer partition_0_0__tbuf2u_0_0_334
    loc("LogregTorchSamba.cpp":922:0)
    layout: <BF16[60, 64]RVCM/64@0x0> vec_order: {1, 0} vec_dim: 1 depth: 2 dims: {60, 64}
    ctx: kDefaultWrite
        loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp":469:0)
        type: write ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (120 # steps): 
              iter[0] : (0 until 60 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
            ctx done: kDefaultWrite_iter0.done (120 # steps)
        addresses:
            addr[0] : (0 until 60 by 1 par 1).i0 vec: 0
            addr[1] : (0 until 64 by 32 par 1).i1 vec: 1
            en_zero : 1
            en_drop: 1
        triggers:
            vin[8] - popif: ctx_en
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: LogregTorchSamba.partition_0_0_.ptconvlstm__indexselect@kLutRd
        loc("software/templates/src/templates/index/rail/Index.cpp":137:0)
        pacing_window: default type: read ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
            ctx done: LogregTorchSamba_partition_0_0__ptconvlstm__indexselect_kLutRd_iter0.done (2 # steps)
        addresses:
            addr[0] : sin[idx] vec: 0
            addr[1] : (0 until 64 by 32 par 1).i1 vec: 1
            en_zero : (sin[idx] < 60)
            en_drop: 1
        triggers:
            sin[idx] - popif: ctx_en
            tb init: 1 pushif: cin[0] popif: rail.ctxdone
            tb init: 2 pushif: cin[1] popif: rail.ctxdone
            tb init: 0 pushif: cin[kDefaultWrite_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_0_0__tbuf1u_0_0_335
    loc("LogregTorchSamba.cpp":933:0)
    layout: <INT32[1]RM@0x0> vec_order: {0} vec_dim: 0 depth: 1 dims: {1}
    ctx: kFrontDynamicWriteCtx
        loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp":469:0)
        type: write ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (1 # steps): 
              iter[0] : (0 until 1 by 16 par 1)
            ctx done: kFrontDynamicWriteCtx_iter0.done (1 # steps)
        addresses:
            addr[0] : (0 until 1 by 16 par 1).i0 vec: 1
            en_zero : 1
            en_drop: 1
        triggers:
            vin[8] - popif: ctx_en
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: LogregTorchSamba.partition_0_0_.ptconvlstm__indexselect@kIndexRd
        loc("software/templates/src/templates/index/rail/Index.cpp":124:0)
        pacing_window: default type: read ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
            ctx done: LogregTorchSamba_partition_0_0__ptconvlstm__indexselect_kIndexRd_iter0.done (2 # steps)
        addresses:
            addr[0] : (0 until 1 by 1 par 1).i0 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[kFrontDynamicWriteCtx_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_0_0__tbuf2u_0_0_342
    loc("LogregTorchSamba.cpp":957:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 2 dims: {64, 1}
    ctx: LogregTorchSamba.partition_0_0_.ptconvlstm__indexselect@kOutWr
        loc("software/templates/src/templates/index/rail/Index.cpp":161:0)
        type: write ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
            ctx done: LogregTorchSamba_partition_0_0__ptconvlstm__indexselect_kOutWr_iter0.done (2 # steps)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i0 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            vin[8] - popif: ctx_en
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: LogregTorchSamba.partition_0_0_.ptconvlstm__dense_layer__linear_wo_bias@kB
        loc("software/templates/src/templates/gemm/prism/BigGemm.cpp":4221:0)
        pacing_window: default type: read ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: LogregTorchSamba_partition_0_0__ptconvlstm__dense_layer__linear_wo_bias_kB_iter0.done (14 # steps)
            addr_chain  (14 # steps): 
              iter[0] : (0 until 7 by 1 par 1)
              iter[1] : (0 until 1 by 1 par 1)
              iter[2] : (0 until 64 by 32 par 1)
              iter[3] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i2 vec: 1
            addr[1] : ((0 until 1 by 1 par 1).i1 + (0 - (0 until 1 by 1 par 1).i3)) vec: 0
            en_zero : 1
            en_drop: (((0 until 1 by 1 par 1).i1 + (0 - (0 until 1 by 1 par 1).i3)) < 1)
        triggers:
            tb init: 2 pushif: cin[0] popif: rail.ctxdone
            tb init: 2 pushif: cin[1] popif: rail.ctxdone
            tb init: 0 pushif: cin[LogregTorchSamba.partition_0_0_.ptconvlstm__indexselect@kOutWr_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__tbuf2u_1_0_394
    loc("LogregTorchSamba.cpp":2501:0)
    layout: <BF16[60, 64]RVCM/64@0x0> vec_order: {1, 0} vec_dim: 1 depth: 2 dims: {60, 64}
    ctx: kDefaultWrite
        loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp":469:0)
        type: write ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (120 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 60 by 1 par 1)
            ctx done: kDefaultWrite_iter0.done (120 # steps)
        addresses:
            addr[0] : (0 until 60 by 1 par 1).i1 vec: 0
            addr[1] : (0 until 64 by 32 par 1).i0 vec: 1
            en_zero : 1
            en_drop: 1
        triggers:
            vin[8] - popif: ctx_en
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: LogregTorchSamba.partition_1_0_.ptconvlstm__indexselect_recompute_@kLutRd
        loc("software/templates/src/templates/index/rail/Index.cpp":137:0)
        pacing_window: default type: read ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
            ctx done: LogregTorchSamba_partition_1_0__ptconvlstm__indexselect_recompute__kLutRd_iter0.done (2 # steps)
        addresses:
            addr[0] : sin[idx] vec: 0
            addr[1] : (0 until 64 by 32 par 1).i1 vec: 1
            en_zero : (sin[idx] < 60)
            en_drop: 1
        triggers:
            sin[idx] - popif: ctx_en
            tb init: 1 pushif: cin[0] popif: rail.ctxdone
            tb init: 1 pushif: cin[1] popif: rail.ctxdone
            tb init: 2 pushif: cin[2] popif: rail.ctxdone
            tb init: 2 pushif: cin[3] popif: rail.ctxdone
            tb init: 0 pushif: cin[kDefaultWrite_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__tbuf1a_1_0_832
    loc("LogregTorchSamba.cpp":2512:0)
    layout: <INT32[1]RM@0x0> vec_order: {0} vec_dim: 0 depth: 1 dims: {1}
    ctx: kDefaultWrite
        loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp":469:0)
        type: write ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (1 # steps): 
              iter[0] : (0 until 1 by 16 par 1)
            ctx done: kDefaultWrite_iter0.done (1 # steps)
        addresses:
            addr[0] : (0 until 1 by 16 par 1).i0 vec: 1
            en_zero : 1
            en_drop: 1
        triggers:
            vin[8] - popif: ctx_en
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: LogregTorchSamba.partition_1_0_.ptconvlstm__indexselect_recompute_@kIndexRd
        loc("software/templates/src/templates/index/rail/Index.cpp":124:0)
        pacing_window: default type: read ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
            ctx done: LogregTorchSamba_partition_1_0__ptconvlstm__indexselect_recompute__kIndexRd_iter0.done (2 # steps)
        addresses:
            addr[0] : (0 until 1 by 1 par 1).i0 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[kDefaultWrite_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__tbuf2u_1_0_399
    loc("LogregTorchSamba.cpp":2536:0)
    layout: <BF16[1, 64]RVCM/64@0x0> vec_order: {1, 0} vec_dim: 1 depth: 2 dims: {1, 64}
    ctx: LogregTorchSamba.partition_1_0_.ptconvlstm__indexselect_recompute_@kOutWr
        loc("software/templates/src/templates/index/rail/Index.cpp":161:0)
        type: write ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
            ctx done: LogregTorchSamba_partition_1_0__ptconvlstm__indexselect_recompute__kOutWr_iter0.done (2 # steps)
        addresses:
            addr[0] : (0 until 1 by 1 par 1).i0 vec: 0
            addr[1] : (0 until 64 by 32 par 1).i1 vec: 1
            en_zero : 1
            en_drop: 1
        triggers:
            vin[8] - popif: ctx_en
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kDefaultRead1
        loc("software/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp":469:0)
        pacing_window: default type: read ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (2 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
            ctx done: kDefaultRead1_iter0.done (2 # steps)
        addresses:
            addr[0] : (0 until 1 by 1 par 1).i1 vec: 0
            addr[1] : (0 until 64 by 32 par 1).i0 vec: 1
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 1 pushif: cin[0] popif: rail.ctxdone
            tb init: 2 pushif: cin[1] popif: rail.ctxdone
            tb init: 0 pushif: cin[LogregTorchSamba.partition_1_0_.ptconvlstm__indexselect_recompute_@kOutWr_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__tbuf2u_1_0_402
    loc("LogregTorchSamba.cpp":2558:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 2 dims: {64, 1}
    ctx: LogregTorchSamba.partition_1_0_.ptconvlstm__indexselect_recompute_@kOutWr
        loc("software/templates/src/templates/index/rail/Index.cpp":161:0)
        type: write ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
            ctx done: LogregTorchSamba_partition_1_0__ptconvlstm__indexselect_recompute__kOutWr_iter0.done (2 # steps)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i0 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            vin[8] - popif: ctx_en
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: LogregTorchSamba.partition_1_0_.ptconvlstm__dense_layer__linear_recompute_@kB
        loc("software/templates/src/templates/gemm/prism/BigGemm.cpp":4221:0)
        pacing_window: default type: read ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_recompute__kB_iter0.done (86 # steps)
            addr_chain  (86 # steps): 
              iter[0] : (0 until 43 by 1 par 1)
              iter[1] : (0 until 1 by 1 par 1)
              iter[2] : (0 until 64 by 32 par 1)
              iter[3] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i2 vec: 1
            addr[1] : ((0 until 1 by 1 par 1).i1 + (0 - (0 until 1 by 1 par 1).i3)) vec: 0
            en_zero : 1
            en_drop: (((0 until 1 by 1 par 1).i1 + (0 - (0 until 1 by 1 par 1).i3)) < 1)
        triggers:
            tb init: 1 pushif: cin[0] popif: rail.ctxdone
            tb init: 2 pushif: cin[1] popif: rail.ctxdone
            tb init: 0 pushif: cin[LogregTorchSamba.partition_1_0_.ptconvlstm__indexselect_recompute_@kOutWr_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[4096, 64]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {4096, 64}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (1024 # steps)
            addr_chain  (1024 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 512 by 32 par 1)
              iter[2] : (0 until 64 by 1 par 1)
        addresses:
            addr[0] : (0 until 512 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 64 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 512 by 32 par 1).i1 >= 0) && ((0 until 512 by 32 par 1).i1 < 512))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: w_loop_1_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {1}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_1_0_iter1.done (1024 # steps)
            addr_chain  (1024 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (512 until 1024 by 32 par 1)
              iter[2] : (0 until 64 by 1 par 1)
        addresses:
            addr[0] : (512 until 1024 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 64 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((512 until 1024 by 32 par 1).i1 >= 512) && ((512 until 1024 by 32 par 1).i1 < 1024))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_1_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: w_loop_2_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {2}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_2_0_iter1.done (1024 # steps)
            addr_chain  (1024 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (1024 until 1536 by 32 par 1)
              iter[2] : (0 until 64 by 1 par 1)
        addresses:
            addr[0] : (1024 until 1536 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 64 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((1024 until 1536 by 32 par 1).i1 >= 1024) && ((1024 until 1536 by 32 par 1).i1 < 1536))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_2_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: w_loop_3_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {3}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_3_0_iter1.done (1024 # steps)
            addr_chain  (1024 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (1536 until 2048 by 32 par 1)
              iter[2] : (0 until 64 by 1 par 1)
        addresses:
            addr[0] : (1536 until 2048 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 64 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((1536 until 2048 by 32 par 1).i1 >= 1536) && ((1536 until 2048 by 32 par 1).i1 < 2048))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_3_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: w_loop_4_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {4}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_4_0_iter1.done (1024 # steps)
            addr_chain  (1024 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (2048 until 2560 by 32 par 1)
              iter[2] : (0 until 64 by 1 par 1)
        addresses:
            addr[0] : (2048 until 2560 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 64 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((2048 until 2560 by 32 par 1).i1 >= 2048) && ((2048 until 2560 by 32 par 1).i1 < 2560))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_4_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: w_loop_5_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {5}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_5_0_iter1.done (1024 # steps)
            addr_chain  (1024 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (2560 until 3072 by 32 par 1)
              iter[2] : (0 until 64 by 1 par 1)
        addresses:
            addr[0] : (2560 until 3072 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 64 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((2560 until 3072 by 32 par 1).i1 >= 2560) && ((2560 until 3072 by 32 par 1).i1 < 3072))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_5_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: w_loop_6_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {6}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_6_0_iter1.done (1024 # steps)
            addr_chain  (1024 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (3072 until 3584 by 32 par 1)
              iter[2] : (0 until 64 by 1 par 1)
        addresses:
            addr[0] : (3072 until 3584 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 64 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((3072 until 3584 by 32 par 1).i1 >= 3072) && ((3072 until 3584 by 32 par 1).i1 < 3584))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_6_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: w_loop_7_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {7}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_7_0_iter1.done (1024 # steps)
            addr_chain  (1024 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (3584 until 4080 by 32 par 1)
              iter[2] : (0 until 64 by 1 par 1)
        addresses:
            addr[0] : (3584 until 4080 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 64 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((3584 until 4080 by 32 par 1).i1 >= 3584) && ((3584 until 4080 by 32 par 1).i1 < 4080))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_7_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: 8192 disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (8192 # steps)
            addr_chain  (8192 # steps): 
              iter[0] : (0 until 4080 by 32 par 1)
              iter[1] : (0 until 64 by 1 par 1)
        addresses:
            addr[0] : (0 until 4080 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 64 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 1 pushif: cin[pacing_in] popif: pacing_done(8192, reset_by=(0 until 4080 by 32 par 1).i0)
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            tb init: 0 pushif: cin[w_loop_1_0_wdone_in__] popif: rail.ctxdone
            tb init: 0 pushif: cin[w_loop_2_0_wdone_in__] popif: rail.ctxdone
            tb init: 0 pushif: cin[w_loop_3_0_wdone_in__] popif: rail.ctxdone
            tb init: 0 pushif: cin[w_loop_4_0_wdone_in__] popif: rail.ctxdone
            tb init: 0 pushif: cin[w_loop_5_0_wdone_in__] popif: rail.ctxdone
            tb init: 0 pushif: cin[w_loop_6_0_wdone_in__] popif: rail.ctxdone
            tb init: 0 pushif: cin[w_loop_7_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__dense_layer__linear_bwd_weight_accum_1_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[4096, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {4096, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 4096 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 4096 by 32 par 1).i1 >= 0) && ((0 until 4096 by 32 par 1).i1 < 4096))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: 128 disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 4096 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 4 pushif: cin[pacing_in] popif: pacing_done(128, reset_by=(0 until 4096 by 32 par 1).i0)
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[4096, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {4096, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 4096 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 4096 by 32 par 1).i1 >= 0) && ((0 until 4096 by 32 par 1).i1 < 4096))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 4096 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_1_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[4096, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {4096, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 4096 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 4096 by 32 par 1).i1 >= 0) && ((0 until 4096 by 32 par 1).i1 < 4096))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 4096 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_2_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[4096, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {4096, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 4096 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 4096 by 32 par 1).i1 >= 0) && ((0 until 4096 by 32 par 1).i1 < 4096))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 4096 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_3_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[4096, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {4096, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 4096 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 4096 by 32 par 1).i1 >= 0) && ((0 until 4096 by 32 par 1).i1 < 4096))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (128 # steps)
            addr_chain  (128 # steps): 
              iter[0] : (0 until 4096 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 4096 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_4_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[8704, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {8704, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (272 # steps)
            addr_chain  (272 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 8704 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 8704 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 8704 by 32 par 1).i1 >= 0) && ((0 until 8704 by 32 par 1).i1 < 8704))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (272 # steps)
            addr_chain  (272 # steps): 
              iter[0] : (0 until 8704 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 8704 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_5_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[8704, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {8704, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (272 # steps)
            addr_chain  (272 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 8704 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 8704 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 8704 by 32 par 1).i1 >= 0) && ((0 until 8704 by 32 par 1).i1 < 8704))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (272 # steps)
            addr_chain  (272 # steps): 
              iter[0] : (0 until 8704 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 8704 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_6_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[8704, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {8704, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (272 # steps)
            addr_chain  (272 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 8704 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 8704 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 8704 by 32 par 1).i1 >= 0) && ((0 until 8704 by 32 par 1).i1 < 8704))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (272 # steps)
            addr_chain  (272 # steps): 
              iter[0] : (0 until 8704 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 8704 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_7_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[8704, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {8704, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (272 # steps)
            addr_chain  (272 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 8704 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 8704 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 8704 by 32 par 1).i1 >= 0) && ((0 until 8704 by 32 par 1).i1 < 8704))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (272 # steps)
            addr_chain  (272 # steps): 
              iter[0] : (0 until 8704 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 8704 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_8_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {64, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 64 by 32 par 1).i1 >= 0) && ((0 until 64 by 32 par 1).i1 < 64))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_9_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {64, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 64 by 32 par 1).i1 >= 0) && ((0 until 64 by 32 par 1).i1 < 64))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_10_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {64, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 64 by 32 par 1).i1 >= 0) && ((0 until 64 by 32 par 1).i1 < 64))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_11_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {64, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 64 by 32 par 1).i1 >= 0) && ((0 until 64 by 32 par 1).i1 < 64))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_12_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {64, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 64 by 32 par 1).i1 >= 0) && ((0 until 64 by 32 par 1).i1 < 64))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_13_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {64, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 64 by 32 par 1).i1 >= 0) && ((0 until 64 by 32 par 1).i1 < 64))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_14_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {64, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 64 by 32 par 1).i1 >= 0) && ((0 until 64 by 32 par 1).i1 < 64))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
========= Summary IR ========
tbuffer partition_1_0__LogregTorchSamba_partition_1_0__ptconvlstm__lstm_layer__lstm_bwd_accum_15_tbuf
    loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":578:0)
    layout: <BF16[64, 1]CVRM/64@0x0> vec_order: {0, 1} vec_dim: 0 depth: 1 dims: {64, 1}
    ctx: w_loop_0_0
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":424:0)
        disable_full_stall type: write ctxid: 0 dispatches: {0}metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: w_loop_0_0_iter1.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 1 by 1 par 1)
              iter[1] : (0 until 64 by 32 par 1)
              iter[2] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i1 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i2 vec: 0
            en_zero : 1
            en_drop: (((0 until 64 by 32 par 1).i1 >= 0) && ((0 until 64 by 32 par 1).i1 < 64))
        triggers:
            vin[8] - popif: ctx_en
            cout[w_loop_done] - src: w_loop_0_0_iter1.done
            cout[1000] - src: rail.ctxdone
            cout[wdone__] - src: rail.ctxdone
    ctx: kBackReadCtx
        loc("software/templates/src/templates/accumulator/rail/ParAccum.cpp":629:0)
        pacing_window: default disable_empty_stall port: READ0 ctxid: 0 metapipe_iter (post divider): {1} 
        iter_chains:
            ctx done: kBackReadCtx_iter0.done (2 # steps)
            addr_chain  (2 # steps): 
              iter[0] : (0 until 64 by 32 par 1)
              iter[1] : (0 until 1 by 1 par 1)
        addresses:
            addr[0] : (0 until 64 by 32 par 1).i0 vec: 1
            addr[1] : (0 until 1 by 1 par 1).i1 vec: 0
            en_zero : 1
            en_drop: 1
        triggers:
            tb init: 0 pushif: cin[w_loop_0_0_wdone_in__] popif: rail.ctxdone
            cout[1000] - src: rail.ctxdone
