{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import nvtx\n",
    "\n",
    "import argparse\n",
    "\n",
    "import math\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "# ------------------------------- CUSTOM FUNCTIONS ------------------------------------------------\n",
    "# Custom Library\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "    \n",
    "from proxy_apps.apps.timeseries_prediction import deepDMD, proxyDeepDMD, proxyDeepDMDMGPU\n",
    "\n",
    "from proxy_apps.utils.tf import TimingCallback\n",
    "from proxy_apps.utils.data.main import NpEncoder\n",
    "from proxy_apps.utils import file_reader, path_handler\n",
    "from proxy_apps.utils.data.grid import GridNetworkDataHandler, GridNetworkTFDataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "  yield 1\n",
    "  yield 2\n",
    "  yield 3\n",
    "\n",
    "ds = tf.data.Dataset.from_generator(gen, output_shapes=(), output_types=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.27032422, 0.9842136 , 0.52210045])>, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.14151258])>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.08276985, 0.84096998, 0.42592973])>, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.50855823])>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.32299807, 0.66025254, 0.61698509])>, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.07733951])>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.52116062, 0.5127115 , 0.71421772])>, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.88130115])>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.23998335, 0.98973505, 0.51559999])>, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.33366177])>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.44490224, 0.7784491 , 0.29790844])>, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.11856389])>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.48067353, 0.09555268, 0.64782161])>, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.45078976])>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.26923359, 0.48223606, 0.1933173 ])>, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.60870225])>)\n",
      "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.30021011, 0.46844089, 0.469092  ])>, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.14073682])>)\n"
     ]
    }
   ],
   "source": [
    "ds1 = tf.data.Dataset.from_tensor_slices(np.random.random((9, 3)))\n",
    "ds2 = tf.data.Dataset.from_tensor_slices(np.random.random((9, 1)))\n",
    "\n",
    "ds = tf.data.Dataset.zip((ds1, ds2))\n",
    "for a in ds:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(3, 3), dtype=float64, numpy=\n",
      "array([[0.48067353, 0.09555268, 0.64782161],\n",
      "       [0.26923359, 0.48223606, 0.1933173 ],\n",
      "       [0.30021011, 0.46844089, 0.469092  ]])>, <tf.Tensor: shape=(3, 1), dtype=float64, numpy=\n",
      "array([[0.45078976],\n",
      "       [0.60870225],\n",
      "       [0.14073682]])>)\n",
      "(<tf.Tensor: shape=(3, 3), dtype=float64, numpy=\n",
      "array([[0.52116062, 0.5127115 , 0.71421772],\n",
      "       [0.23998335, 0.98973505, 0.51559999],\n",
      "       [0.44490224, 0.7784491 , 0.29790844]])>, <tf.Tensor: shape=(3, 1), dtype=float64, numpy=\n",
      "array([[0.88130115],\n",
      "       [0.33366177],\n",
      "       [0.11856389]])>)\n",
      "(<tf.Tensor: shape=(3, 3), dtype=float64, numpy=\n",
      "array([[0.27032422, 0.9842136 , 0.52210045],\n",
      "       [0.08276985, 0.84096998, 0.42592973],\n",
      "       [0.32299807, 0.66025254, 0.61698509]])>, <tf.Tensor: shape=(3, 1), dtype=float64, numpy=\n",
      "array([[0.14151258],\n",
      "       [0.50855823],\n",
      "       [0.07733951]])>)\n"
     ]
    }
   ],
   "source": [
    "shuffled_and_batched = ds.batch(3).shuffle(5)\n",
    "for a in shuffled_and_batched:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tensorflow version:  2.4.0\n"
     ]
    }
   ],
   "source": [
    "# System Setup\n",
    "config = file_reader.read_config()\n",
    "\n",
    "_N_EPOCHS = 2\n",
    "_BATCH_SIZE = 64000\n",
    "_APP_NAME = config[\"info\"][\"app_name\"]\n",
    "_NROWS = int(config[\"data\"][\"n_rows\"])\n",
    "_NCOLS = int(config[\"data\"][\"n_cols\"])\n",
    "_REPEAT_COLS = int(config[\"data\"][\"repeat_cols\"])\n",
    "_DTYPE = config[\"model\"][\"dtype\"]\n",
    "\n",
    "_LABEL = \"Baseline\"\n",
    "_SUFFIX =  \"gpu\" + '_' + \\\n",
    "            \"a100\" + '_' + \\\n",
    "            'ng' + str(1) + '_' + \\\n",
    "            'nc' + str(-1) + '_' + \\\n",
    "            'e' + str(_N_EPOCHS) + '_' + \\\n",
    "            'b' + str(_BATCH_SIZE) + '_' + \\\n",
    "            'r' + str(_REPEAT_COLS) + '_' + _LABEL\n",
    "\n",
    "performance_dict = dict()\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# current directory\n",
    "curr_dir = \"./\"\n",
    "\n",
    "# output directory\n",
    "output_dir = path_handler.get_absolute_path(curr_dir, config[\"info\"][\"output_dir\"] + config[\"info\"][\"name\"] + \"/\" + config[\"info\"][\"app_name\"] + \"/\" + _DTYPE + \"/R\" + str(_REPEAT_COLS) + \"/\")\n",
    "if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "\n",
    "# TensorFlow Setup\n",
    "print(\"[INFO] Tensorflow version: \", tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Eager mode:  True\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Eager mode: \", tf.executing_eagerly()) # For easy reset of notebook state.\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "# tf.config.optimizer.set_jit(True) # Enable XLA.\n",
    "\n",
    "# if _LABEL in [\"Baseline\"]: tf.keras.backend.set_floatx('float64')\n",
    "# elif _LABEL in [\"TFDataOpt\", \"TFDataOptMGPU\"]:\n",
    "#     tf.keras.backend.set_floatx(_DTYPE)\n",
    "# elif _LABEL in [\"TFDataOptMP\", \"TFDataOptMGPUMP\"]:\n",
    "#     _DTYPE = \"float32\"\n",
    "#     policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "#     tf.keras.mixed_precision.set_global_policy(policy)\n",
    "#     # tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "\n",
    "# if _LABEL in [\"TFDataOpt\", \"TFDataOptMP\", \"TFDataOptMGPU\", \"TFDataOptMGPUMP\"]:\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Loading the datasets from the directory: /qfs/people/jain432/pacer/data/TrainingDataIEEE68bus\n",
      "[INFO]: Loading data for 30 scenarios ...\n",
      "[INFO]: Total number of scenarios loaded: 30\n",
      "[INFO]: Shape of each scenario loaded:  (1400, 136)\n",
      "[INFO]: Done ...\n",
      "[INFO]: Time taken for loading datasets: 2.04144287109375 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- DATA LOADING ------------------------------------------------   \n",
    "\n",
    "data_loading = nvtx.start_range(\"Data Loading\")\n",
    "l_start = time.time()\n",
    "data_handler = GridNetworkDataHandler(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                        n_rows=_NROWS,\n",
    "                                        n_cols=_NCOLS,\n",
    "                                        repeat_cols=_REPEAT_COLS,\n",
    "                                        dtype=_DTYPE\n",
    "                                     ) \n",
    "\n",
    "scenario_data = data_handler.load_grid_data()\n",
    "l_stop = time.time()\n",
    "print('[INFO]: Time taken for loading datasets:', l_stop - l_start, 'seconds')\n",
    "performance_dict['data_loading_time'] = l_stop-l_start\n",
    "nvtx.end_range(data_loading)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Original dataset size: 1400\n",
      "[INFO]: Chosen dataset size: 800\n",
      "[INFO]: Length of X_data:  1800\n",
      "[INFO]: Length of each window after down sampling:  (800, 136)\n",
      "[INFO]: Time taken for creating X datasets: 2.759248971939087 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- DATA PREPROCESSING ------------------------------------------------\n",
    "\n",
    "data_processing = nvtx.start_range(\"Data Processing\")\n",
    "i_start = time.time()\n",
    "X_data, Y_data, U_data, V_data, Yp, Yf = data_handler.create_windows(scenario_data)\n",
    "i_stop = time.time()\n",
    "print('[INFO]: Time taken for creating X datasets:', i_stop - i_start, 'seconds')\n",
    "performance_dict['data_processing_time'] = i_stop-i_start\n",
    "nvtx.end_range(data_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Yp_array shape:  (41970, 136)\n",
      "[INFO]: Yf_array shape:  (41970, 136)\n",
      "[INFO]: X_array shape:  (1440000, 136)\n",
      "[INFO]: Y_array shape:  (1440000, 136)\n",
      "[INFO]: U_array shape:  (1440000, 136)\n",
      "[INFO]: V_array shape:  (1440000, 136)\n",
      "[INFO]: Time taken for normalization: 8.252182960510254 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- DATA NORMALIZATION ------------------------------------------------\n",
    "\n",
    "data_norm = nvtx.start_range(\"Data Normalization\")\n",
    "n_start = time.time()\n",
    "X_array, Y_array, U_array, V_array, Yp_array, Yf_array = data_handler.scale_data(X_data, Y_data, U_data, V_data, Yp, Yf)\n",
    "n_stop = time.time()\n",
    "print('[INFO]: Time taken for normalization:', n_stop - n_start, 'seconds')\n",
    "\n",
    "performance_dict['data_scaling_time'] = n_stop-n_start\n",
    "nvtx.end_range(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- MODEL SETUP ------------------------------------------------\n",
    "# timing callback\n",
    "timing_cb = TimingCallback()\n",
    "\n",
    "# Stopping criteria if the training loss doesn't go down by 1e-3\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', min_delta = 1e-3, verbose = 1, mode='min', patience = 3, \n",
    "    baseline=None, restore_best_weights=True)\n",
    "\n",
    "# Create a TensorBoard Profiler\n",
    "logs = path_handler.get_absolute_path(curr_dir, config[\"model\"][\"tb_log_dir\"] + _APP_NAME + \"/\" + _DTYPE + \"/R\" + str(_REPEAT_COLS) + \"/tensorboard/\" + _SUFFIX)\n",
    "# tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logs, histogram_freq=1, embeddings_freq=1, profile_batch=(5,15))\n",
    "\n",
    "# all callbacks\n",
    "callbacks=[early_stop_cb, timing_cb]#, tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hyperparameters - we can keep it as a dict instead of creating a separate class\n",
    "hyper_param_dict = config[\"model\"][\"hyperparameters\"]\n",
    "hyper_param_dict['original_dim']       = _REPEAT_COLS * _NCOLS   # input data dimension\n",
    "hyper_param_dict['num_epochs']         = _N_EPOCHS  # Number of epochs  \n",
    "hyper_param_dict['batch_size']         = _BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = Yp_array.shape[0]\n",
    "\n",
    "Xd_array = tf.data.Dataset.from_tensor_slices(X_array)\n",
    "Yd_array = tf.data.Dataset.from_tensor_slices(Y_array)\n",
    "Ud_array = tf.data.Dataset.from_tensor_slices(U_array)\n",
    "Vd_array = tf.data.Dataset.from_tensor_slices(V_array)\n",
    "Ydp_array = tf.data.Dataset.from_tensor_slices(Yp_array)\n",
    "Ydf_array = tf.data.Dataset.from_tensor_slices(Yf_array)\n",
    "\n",
    "hyper_param_dict['dtype']         = _DTYPE\n",
    "hp = proxyDeepDMD.HyperParameters(hyper_param_dict)\n",
    "hp.model_name         = _LABEL\n",
    "\n",
    "performance_dict[\"n_epochs\"] = hp.ep\n",
    "performance_dict[\"batch_size\"] = hp.bs\n",
    "# performance_dict[\"n_training_batches\"] = n_batches_training\n",
    "# performance_dict[\"n_val_batches\"] = n_batches - n_batches_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- MODEL TRAINING ------------------------------------------------\n",
    "# Initialize, build, and fit the model\n",
    "# with mirrored_strategy.scope():\n",
    "with mirrored_strategy.scope():\n",
    "    K_model = proxyDeepDMDMGPU.Encoder(hp)\n",
    "    optimizer = tf.optimizers.Adagrad(hp.lr)\n",
    "\n",
    "BATCH_SIZE = hp.bs # * mirrored_strategy.num_replicas_in_sync\n",
    "zip_data = tf.data.Dataset.zip((Xd_array, Yd_array)).batch(BATCH_SIZE)\n",
    "\n",
    "training_dataset = zip_data.cache()\n",
    "training_dataset = training_dataset.shuffle(buffer_size=BATCH_SIZE)\n",
    "training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "training_dataset = mirrored_strategy.experimental_distribute_dataset(training_dataset)#.cache()#.with_options(options)# .take(n_batches_training)\n",
    "# training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "K_model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.keras.engine import data_adapter\n",
    "# from tensorflow.python.keras.engine import training_utils\n",
    "# from tensorflow.python.keras import callbacks as callbacks_module\n",
    "# from tensorflow.python.profiler import trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neural Network\n",
    "# class NeuralNetworkModel(): \n",
    "#     def __init__(self, hp, model):\n",
    "#         self.encoder = model\n",
    "\n",
    "#         # other parameters\n",
    "#         self.rf = hp.rf \n",
    "#         self.d_type = hp.d_type\n",
    "#         self.model_name = hp.model_name\n",
    "        \n",
    "#     def fit(self, training_dataset, n_epochs, batch_size, steps_per_epoch, callbacks, initial_epoch=0, verbose=1):\n",
    "#         m_start = time.time()\n",
    "#         for epoch in range(n_epochs):\n",
    "#             print(\"\\nEpoch {}/{}\".format(epoch+1, n_epochs))\n",
    "#             pb_i = Progbar(steps_per_epoch, stateful_metrics=['loss'])\n",
    "\n",
    "#             # Iterate over the batches of the dataset.\n",
    "#             total_loss = 0.0\n",
    "#             num_batches = 0\n",
    "#             for step, inp_data in enumerate(training_dataset):\n",
    "#                 total_loss += self.distributed_train_step(inp_data)\n",
    "#                 num_batches += 1\n",
    "\n",
    "#                 loss_value = total_loss / num_batches\n",
    "\n",
    "#                 # Log every 200 batches.\n",
    "#                 pb_i.add(batch_size//batch_size, values=[('loss', loss_value)])\n",
    "                \n",
    "#         m_stop = time.time()\n",
    "#         print(m_stop-m_start)\n",
    "                    \n",
    "    \n",
    "# #     def fit(self, training_dataset, n_epochs, batch_size, steps_per_epoch, callbacks, initial_epoch=0, verbose=1):\n",
    "# #         m_start = time.time()\n",
    "# #         with self.encoder.distribute_strategy.scope(), \\\n",
    "# #          training_utils.RespectCompiledTrainableState(self.encoder):\n",
    "# #             data_handler = data_adapter.DataHandler(\n",
    "# #                 x=training_dataset,\n",
    "# #                 y=None,\n",
    "# #                 steps_per_epoch=steps_per_epoch,\n",
    "# #                 batch_size=batch_size,\n",
    "# #                 model=self.encoder,\n",
    "# #                 epochs=n_epochs,\n",
    "# #                 initial_epoch=initial_epoch,\n",
    "# #                 steps_per_execution=self.encoder._steps_per_execution)\n",
    "\n",
    "# #             if not isinstance(callbacks, callbacks_module.CallbackList):\n",
    "# #                 callbacks = callbacks_module.CallbackList(\n",
    "# #                     callbacks,\n",
    "# #                     add_history=True,\n",
    "# #                     add_progbar=verbose != 0,\n",
    "# #                     model=self.encoder,\n",
    "# #                     verbose=verbose,\n",
    "# #                     epochs=n_epochs,\n",
    "# #                     steps=data_handler.inferred_steps)\n",
    "\n",
    "# #             self.stop_training = False\n",
    "# #             callbacks.on_train_begin()\n",
    "            \n",
    "# #             training_logs = None\n",
    "# #             # Handle fault-tolerance for multi-worker.\n",
    "# #             # TODO(omalleyt): Fix the ordering issues that mean this has to\n",
    "# #             # happen after `callbacks.on_train_begin`.\n",
    "# #             data_handler._initial_epoch = (  # pylint: disable=protected-access\n",
    "# #                 self.encoder._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n",
    "# #             logs = None\n",
    "# #             for epoch, iterator in data_handler.enumerate_epochs():\n",
    "# #                 print(epoch, iterator)\n",
    "# #                 callbacks.on_epoch_begin(epoch)\n",
    "# #                 with data_handler.catch_stop_iteration():\n",
    "# #                     for step, inp_data in enumerate(training_dataset):\n",
    "# #                         print(step)\n",
    "# #                         with trace.Trace('train', epoch_num=n_epochs, step_num=step, batch_size=batch_size, _r=1):\n",
    "# #                             callbacks.on_train_batch_begin(step)\n",
    "# #                             loss = self.distributed_train_step(inp_data)\n",
    "                            \n",
    "# #                             if data_handler.should_sync:\n",
    "# #                                 context.async_wait()\n",
    "                            \n",
    "# #                             logs = tmp_logs  # No error, now safe to assign to logs.\n",
    "# #                             end_step = step + data_handler.step_increment\n",
    "# #                             callbacks.on_train_batch_end(end_step, logs)\n",
    "# #                             if self.stop_training: break\n",
    "                \n",
    "# #                 print(\"\\nepoch {}/{}\".format(epoch+1, n_epochs))\n",
    "# #                 pb_i = Progbar(math.ceil(1440000//batch_size)+1, stateful_metrics=['loss'])\n",
    "\n",
    "# #                 # Iterate over the batches of the dataset.\n",
    "# #                 total_loss = 0.0\n",
    "# #                 num_batches = 0\n",
    "# #                 for step, inp_data in enumerate(training_dataset):\n",
    "# #                     total_loss += self.distributed_train_step(inp_data)\n",
    "# #                     num_batches += 1\n",
    "\n",
    "# #                     loss_value = total_loss / num_batches\n",
    "\n",
    "# #                     # Log every 200 batches.\n",
    "# #                     pb_i.add(batch_size//batch_size, values=[('loss', loss_value)])\n",
    "\n",
    "# #         callbacks.on_train_end(logs=training_logs)\n",
    "# #         m_stop = time.time()\n",
    "# #         print(m_stop-m_start)\n",
    "        \n",
    "#     @tf.function(experimental_compile=True)\n",
    "#     def distributed_train_step(self, dist_inputs):\n",
    "#         per_replica_losses = self.encoder.distribute_strategy.run(self.train_step, args=(dist_inputs,))\n",
    "#         return self.encoder.distribute_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "#     # @tf.function(experimental_compile=True) # (input_signature=(tf.TensorSpec(shape=[None], dtype=tf.float64),))\n",
    "#     def train_step(self, inputs):       \n",
    "#         X, Y        = inputs\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             Psi_X    = self.encoder(X, training=True)\n",
    "#             Psi_Y    = self.encoder(Y, training=False)    \n",
    "\n",
    "#             PSI_X    = tf.concat([X, tf.cast(Psi_X, self.d_type)], 1)\n",
    "#             PSI_Y    = tf.concat([Y, tf.cast(Psi_Y, self.d_type)], 1) \n",
    "\n",
    "#             # 1-time step evolution on observable space:\n",
    "#             K_PSI_X  = tf.matmul(PSI_X, self.encoder.KO) \n",
    "            \n",
    "#             # 1-step Koopman loss on observable space:        \n",
    "#             K_loss   = tf.norm(PSI_Y - K_PSI_X, axis = [0,1], ord = 'fro')\n",
    "\n",
    "#             # Regularization loss on Koopman operator:\n",
    "#             Reg_loss= tf.math.scalar_mul(self.rf, tf.norm(self.encoder.KO, axis = [0,1], ord = 'fro'))      \n",
    "        \n",
    "#             # Total loss:\n",
    "#             loss = K_loss + Reg_loss\n",
    "#             if self.model_name in [\"TFDataOptMP\", \"TFDataOptMGPUMP\"]:\n",
    "#                 loss = self.optimizer.get_scaled_loss(loss)\n",
    "            \n",
    "#             # tf.print(\"K Loss: \", K_loss, \"Reg Loss: \", Reg_loss, \"Total Loss: \", loss)\n",
    "#             # loss += sum(self.encoder.losses)\n",
    "            \n",
    "#         # Compute gradients\n",
    "#         trainable_vars = self.encoder.trainable_variables\n",
    "#         gradients = tape.gradient(loss, trainable_vars)\n",
    "#         if self.model_name in [\"TFDataOptMP\", \"TFDataOptMGPUMP\"]:\n",
    "#             gradients = self.encoder.optimizer.get_unscaled_gradients(gradients)\n",
    "\n",
    "#         # Update weights\n",
    "#         self.encoder.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "#         # Return a dict mapping metric names to current value.\n",
    "#         # Note that it will include the loss (tracked in self.metrics).\n",
    "#         return loss\n",
    "        \n",
    "#     # @tf.function(experimental_compile=True)\n",
    "#     def test_step(self, inputs):       \n",
    "#         X, Y        = inputs\n",
    "\n",
    "#         Psi_X    = self.encoder(X, training=False)\n",
    "#         Psi_Y    = self.encoder(Y, training=False)    \n",
    "\n",
    "#         PSI_X    = tf.concat([X, tf.cast(Psi_X, self.d_type)], 1)\n",
    "#         PSI_Y    = tf.concat([Y, tf.cast(Psi_Y, self.d_type)], 1) \n",
    "            \n",
    "#         # 1-time step evolution on observable space:\n",
    "#         K_PSI_X  = tf.matmul(PSI_X, self.encoder.KO) \n",
    "\n",
    "#         # 1-step Koopman loss on observable space:        \n",
    "#         K_loss   = tf.norm(PSI_Y - K_PSI_X, axis = [0,1], ord = 'fro')\n",
    "\n",
    "#         # Regularization loss on Koopman operator:\n",
    "#         Reg_loss= tf.math.scalar_mul(self.rf, tf.norm(self.encoder.KO, axis = [0,1], ord = 'fro'))        \n",
    "\n",
    "#         # Total loss:\n",
    "#         loss = K_loss + Reg_loss\n",
    "#         if self.model_name in [\"TFDataOptMP\", \"TFDataOptMGPUMP\"]:\n",
    "#             loss = self.optimizer.get_scaled_loss(loss)\n",
    "#         # loss += sum(self.encoder.losses)\n",
    "            \n",
    "#         # Return a dict mapping metric names to current value.\n",
    "#         # Note that it will include the loss (tracked in self.metrics).\n",
    "#         return loss\n",
    "        \n",
    "#     # @tf.function(experimental_compile=True)\n",
    "#     def predict_step(self, inputs):       \n",
    "#         X, Y        = inputs\n",
    "\n",
    "#         Psi_X    = self.encoder(X, training=False)\n",
    "#         Psi_Y    = self.encoder(Y, training=False)    \n",
    "\n",
    "#         PSI_X    = tf.concat([X, tf.cast(Psi_X, self.d_type)], 1)\n",
    "#         PSI_Y    = tf.concat([Y, tf.cast(Psi_Y, self.d_type)], 1) \n",
    "\n",
    "#         # 1-time step evolution on observable space:\n",
    "#         K_PSI_X  = tf.matmul(PSI_X, self.encoder.KO) \n",
    "\n",
    "#         # 1-step Koopman loss on observable space:        \n",
    "#         K_loss   = tf.norm(PSI_Y - K_PSI_X, axis = [0,1], ord = 'fro')\n",
    "        \n",
    "#         # Regularization loss on Koopman operator:\n",
    "#         Reg_loss= tf.math.scalar_mul(self.rf, tf.norm(self.encoder.KO, axis = [0,1], ord = 'fro'))        \n",
    "\n",
    "#         return Psi_X, PSI_X, Psi_Y, PSI_Y, K_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1/2\n",
      "23/23 [==============================] - 31s 834ms/step - loss: 738.6355\n",
      "\n",
      "epoch 2/2\n",
      "23/23 [==============================] - 17s 704ms/step - loss: 496.4543\n",
      "48.07583665847778\n"
     ]
    }
   ],
   "source": [
    "trainer = proxyDeepDMDMGPU.NeuralNetworkModel(hp, K_model)\n",
    "trainer.fit(training_dataset, n_epochs=hp.ep, batch_size=hp.bs, steps_per_epoch=math.ceil(X_array.shape[0]//hp.bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.zip((Ydp_array, Ydf_array)).batch(Yp_array.shape[0], drop_remainder=True)\n",
    "test_data = test_data.cache()\n",
    "test_data = test_data.shuffle(buffer_size=Yp_array.shape[0])\n",
    "test_data = test_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "for t in test_data.take(1):\n",
    "    Psi_X, PSI_X, Psi_Y, PSI_Y, Kloss = trainer.predict_step(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = deepDMD.HyperParameters(hyper_param_dict)\n",
    "hp.model_name         = _LABEL\n",
    "\n",
    "performance_dict[\"n_epochs\"] = hp.ep\n",
    "performance_dict[\"batch_size\"] = hp.bs\n",
    "performance_dict[\"n_training_batches\"] = 1 - hp.vs\n",
    "performance_dict[\"n_val_batches\"] = hp.vs\n",
    "\n",
    "# ------------------------------- MODEL TRAINING ------------------------------------------------\n",
    "# Initialize, build, and fit the model\n",
    "m_start = time.time()\n",
    "BaselineModel = deepDMD.NeuralNetworkModel(hp)\n",
    "BaselineModel.compile(optimizer=tf.optimizers.Adagrad(hp.lr))\n",
    "\n",
    "history = BaselineModel.fit([X_array, Y_array], batch_size=hp.bs, \n",
    "                  epochs=hp.ep, \n",
    "                  callbacks=callbacks, \n",
    "                  shuffle=True)\n",
    "\n",
    "m_stop = time.time()\n",
    "\n",
    "# print info\n",
    "print('[INFO]: Time taken for model training (time module):', m_stop - m_start, 'seconds')\n",
    "print('[INFO]: Time taken for model training (Keras):', sum(timing_cb.logs), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaselineModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @tf.function\n",
    "# def train_step(X, Y):\n",
    "#     # Open a GradientTape to record the operations run\n",
    "#     # during the forward pass, which enables auto-differentiation.\n",
    "#     with tf.GradientTape() as tape:\n",
    "\n",
    "#         # Run the forward pass of the layer.\n",
    "#         # The operations that the layer applies\n",
    "#         # to its inputs are going to be recorded\n",
    "#         # on the GradientTape.\n",
    "#         Psi_X    = K_model(X, training=True)\n",
    "#         Psi_Y    = K_model(Y, training=True)    \n",
    "\n",
    "#         PSI_X    = tf.concat([X, Psi_X], 1)\n",
    "#         PSI_Y    = tf.concat([Y, Psi_Y], 1) \n",
    "\n",
    "#         # 1-time step evolution on observable space:\n",
    "#         K_PSI_X  = tf.matmul(PSI_X, K_model.KO) \n",
    "\n",
    "#         # 1-step Koopman loss on observable space:        \n",
    "#         K_loss   = tf.norm(PSI_Y - K_PSI_X, axis = [0,1], ord = 'fro')\n",
    "\n",
    "#         # Regularization loss on Koopman operator:\n",
    "#         Reg_loss= tf.math.scalar_mul(hp.rf, tf.norm(K_model.KO, axis = [0,1], ord = 'fro'))   \n",
    "\n",
    "#         # Compute the loss value for this minibatch.\n",
    "#         loss_value = K_loss + Reg_loss\n",
    "\n",
    "#     # Use the gradient tape to automatically retrieve\n",
    "#     # the gradients of the trainable variables with respect to the loss.\n",
    "#     grads = tape.gradient(loss_value, K_model.trainable_weights)\n",
    "\n",
    "#     # Run one step of gradient descent by updating\n",
    "#     # the value of the variables to minimize the loss.\n",
    "#     optimizer.apply_gradients(zip(grads, K_model.trainable_weights))\n",
    "    \n",
    "#     return loss_value\n",
    "\n",
    "# @tf.function\n",
    "# def distributed_train_step(dist_inputs):\n",
    "#     per_replica_losses = mirrored_strategy.run(train_step, args=(dist_inputs[0],dist_inputs[1]))\n",
    "#     print(per_replica_losses)\n",
    "#     return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "#                          axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_start = time.time()\n",
    "# for epoch in range(hp.ep):\n",
    "#     print(\"\\nepoch {}/{}\".format(epoch+1, hp.ep))\n",
    "#     pb_i = Progbar(math.ceil(1440000//hp.bs)+1, stateful_metrics=['loss'])\n",
    "\n",
    "#     # Iterate over the batches of the dataset.\n",
    "#     total_loss = 0.0\n",
    "#     num_batches = 0\n",
    "#     for step, inp_data in enumerate(training_dataset):\n",
    "#         total_loss += distributed_train_step(inp_data)\n",
    "#         num_batches += 1\n",
    "        \n",
    "#         loss_value = total_loss / num_batches\n",
    "\n",
    "#         # Log every 200 batches.\n",
    "#         pb_i.add(hp.bs//hp.bs, values=[('loss', loss_value)])\n",
    "\n",
    "# m_stop = time.time()\n",
    "# print(m_stop-m_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
