{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import nvtx\n",
    "\n",
    "import argparse\n",
    "\n",
    "import math\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "# ------------------------------- CUSTOM FUNCTIONS ------------------------------------------------\n",
    "# Custom Library\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "    \n",
    "from proxy_apps.apps.timeseries_prediction import deepDMD, proxyDeepDMD, proxyDeepDMDMGPU\n",
    "\n",
    "from proxy_apps.utils.tf import TimingCallback\n",
    "from proxy_apps.utils.data.main import NpEncoder\n",
    "from proxy_apps.utils import file_reader, path_handler\n",
    "from proxy_apps.utils.data.grid import GridNetworkDataHandler, GridNetworkTFDataHandler, GridNetworkNewGen, ReadFileAsDataset, TransientDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tensorflow version:  2.4.0\n"
     ]
    }
   ],
   "source": [
    "# System Setup\n",
    "config = file_reader.read_config()\n",
    "\n",
    "_N_EPOCHS = 2\n",
    "_BATCH_SIZE = 32\n",
    "_APP_NAME = config[\"info\"][\"app_name\"]\n",
    "_NROWS = int(config[\"data\"][\"n_rows\"])\n",
    "_NCOLS = int(config[\"data\"][\"n_cols\"])\n",
    "_REPEAT_COLS = int(config[\"data\"][\"repeat_cols\"])\n",
    "_DTYPE = config[\"model\"][\"dtype\"]\n",
    "\n",
    "_LABEL = \"TFDataOpt\"\n",
    "_SUFFIX =  \"gpu\" + '_' + \\\n",
    "            \"a100\" + '_' + \\\n",
    "            'ng' + str(1) + '_' + \\\n",
    "            'nc' + str(-1) + '_' + \\\n",
    "            'e' + str(_N_EPOCHS) + '_' + \\\n",
    "            'b' + str(_BATCH_SIZE) + '_' + \\\n",
    "            'r' + str(_REPEAT_COLS) + '_' + _LABEL\n",
    "\n",
    "performance_dict = dict()\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# current directory\n",
    "curr_dir = \"./\"\n",
    "\n",
    "# output directory\n",
    "output_dir = path_handler.get_absolute_path(curr_dir, config[\"info\"][\"output_dir\"] + config[\"info\"][\"name\"] + \"/\" + config[\"info\"][\"app_name\"] + \"/\" + _DTYPE + \"/R\" + str(_REPEAT_COLS) + \"/\")\n",
    "if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "\n",
    "# TensorFlow Setup\n",
    "print(\"[INFO] Tensorflow version: \", tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dataset, num_epochs=2, steps=10):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for s, sample in enumerate(dataset):\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            if s == steps - 1:\n",
    "                break\n",
    "            \n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Eager mode:  True\n"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "print(\"[INFO] Eager mode: \", tf.executing_eagerly()) # For easy reset of notebook state.\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "[INFO]: Time taken for loading datasets: 0.00025916099548339844 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- DATA LOADING ------------------------------------------------   \n",
    "l_start = time.time()\n",
    "if _LABEL == \"Baseline\":\n",
    "    data_handler = GridNetworkDataHandler(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                            n_rows=_NROWS,\n",
    "                                            n_cols=_NCOLS,\n",
    "                                            repeat_cols=_REPEAT_COLS,\n",
    "                                            dtype=_DTYPE\n",
    "                                         ) \n",
    "\n",
    "    scenario_data = data_handler.load_grid_data()\n",
    "elif _LABEL == \"TFDataOptPrev\":\n",
    "    data_handler = GridNetworkTFDataHandler(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                            n_rows=_NROWS,\n",
    "                                            n_cols=_NCOLS,\n",
    "                                            repeat_cols=_REPEAT_COLS,\n",
    "                                            dtype=_DTYPE\n",
    "                                         ) \n",
    "\n",
    "    scenario_data = data_handler.load_grid_data()\n",
    "elif _LABEL == \"TFDataOpt\":\n",
    "#     data_handler = GridNetworkNewGen(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "#                                             n_rows=_NROWS,\n",
    "#                                             n_cols=_NCOLS,\n",
    "#                                             repeat_cols=_REPEAT_COLS,\n",
    "#                                             d_type=_DTYPE\n",
    "#                                          )\n",
    "\n",
    "#     scenario_data = data_handler.load_grid_data()\n",
    "    print(\"hello\")\n",
    "\n",
    "l_stop = time.time()\n",
    "print('[INFO]: Time taken for loading datasets:', l_stop - l_start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 800\n",
    "# shift_size = 10\n",
    "# stride = 1\n",
    "# N = 3\n",
    "\n",
    "# training_data = data_handler.get_training_data(scenario_data, window_size, shift_size, stride, N, 1)\n",
    "# benchmark(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataGenerator(tf.data.Dataset):\n",
    "    def _generator(file_name, n_rows, n_cols, n_repeat, x_indexer, y_indexer, norm, scale_factor):\n",
    "        # Opening the file\n",
    "        dataset = TransientDataset(file_name.decode('utf-8'))\n",
    "        raw_data = np.repeat(np.concatenate([dataset.F, dataset.Vm], axis=1), n_repeat, axis=1)[:n_rows, :]\n",
    "        split_index = (n_cols * n_repeat) // 2\n",
    "        \n",
    "        flat_X_data = raw_data[x_indexer].reshape(-1, n_cols)\n",
    "        flat_Y_data = raw_data[y_indexer].reshape(-1, n_cols)\n",
    "        if norm:\n",
    "            flat_X_data[:, :split_index] = scale_factor*(flat_X_data[:, :split_index] - 60)\n",
    "            flat_X_data[:, split_index:] = 10*(flat_X_data[:, :split_index] - 1)\n",
    "            \n",
    "            flat_Y_data[:, :split_index] = scale_factor*(flat_Y_data[:, :split_index] - 60)\n",
    "            flat_Y_data[:, split_index:] = 10*(flat_Y_data[:, :split_index] - 1)\n",
    "            \n",
    "        yield (flat_X_data, flat_Y_data)\n",
    "\n",
    "    def __new__(cls, file_name, n_rows, flat_n_rows, n_cols, n_repeat, x_indexer, y_indexer, d_type, norm=True, scale_factor=2*np.pi):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_signature = (tf.TensorSpec(shape=(flat_n_rows, n_cols*n_repeat), dtype=d_type),\n",
    "                                tf.TensorSpec(shape=(flat_n_rows, n_cols*n_repeat), dtype=d_type)),\n",
    "            args=(file_name, n_rows, n_cols, n_repeat, x_indexer, y_indexer, norm, scale_factor,)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexer(n_rows, window_size, shift_size, start_point, leave_last):\n",
    "    return np.arange(window_size)[None, :] + start_point + shift_size*np.arange(((n_rows - window_size - leave_last - start_point) // shift_size) + 1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 8.604403546079993\n"
     ]
    }
   ],
   "source": [
    "window_size = 800\n",
    "shift_size = 10\n",
    "stride = 1\n",
    "N = 3\n",
    "\n",
    "scenario_dir = path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"])\n",
    "dir_list = [scenario_dir + \"/\" + f + \"/\" for f in os.listdir(scenario_dir)]\n",
    "list_files = tf.data.Dataset.from_tensor_slices(dir_list)\n",
    "\n",
    "x_indexer = get_indexer(_NROWS, window_size, shift_size, 0, 3)\n",
    "y_indexer = get_indexer(_NROWS, window_size, shift_size, 1, 0)\n",
    "\n",
    "trimmed_scenarios = list_files.interleave(lambda x: ReadFileAsDataset(x, _NROWS, x_indexer.shape[0] * x_indexer.shape[1], \n",
    "                                                                      _NCOLS, _REPEAT_COLS, \n",
    "                                                                      x_indexer, y_indexer,\n",
    "                                                                      _DTYPE\n",
    "                                                                     ).prefetch(tf.data.AUTOTUNE),\n",
    "                                          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "                                          cycle_length=30, \n",
    "                                          block_length=1 \n",
    "                                         )\n",
    "\n",
    "flat_data = trimmed_scenarios.flat_map(lambda x, y: tf.data.Dataset.from_tensor_slices((x, y))).cache()\n",
    "benchmark(flat_data.batch(65536))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  0.08234169   0.08398943   0.08258069   0.08214684   0.08244791\n",
      "   0.08239397   0.08254649   0.08293292   0.08758576   0.08190964\n",
      "   0.08206848   0.08195436   0.08184469   0.08165109   0.08036628\n",
      "   0.0798736    0.08542501   0.08431926   0.04962437   0.03824485\n",
      "   0.08420936   0.09290615   0.09679139   0.08237581   0.08692641\n",
      "   0.11023702   0.09832751   0.14486176   0.15468657   0.08604158\n",
      "   0.08542665   0.09650718   0.09626098   0.09627406   0.09861495\n",
      "   0.09202704   0.09158971   0.09184748   0.09850423   0.03992778\n",
      "  -0.03307551  -0.0259497    0.09590663   0.09609054   0.10489532\n",
      "   0.10175143   0.06886592   0.05863196   0.1120856    0.12620923\n",
      "   0.1111895    0.14195852   0.08204857   0.08240602   0.08138131\n",
      "   0.03664793   0.02118596   0.09608962   0.12064654   0.07192819\n",
      "   0.17851272   0.07751199   0.10213777   0.09318556   0.09092183\n",
      "  -0.03392203  -0.02954703   0.15240775  -9.176583    -9.160106\n",
      "  -9.174193    -9.178532    -9.175521    -9.176061    -9.174535\n",
      "  -9.1706705   -9.124143    -9.180903    -9.179316    -9.180456\n",
      "  -9.181553    -9.183489    -9.196337    -9.201264    -9.14575\n",
      "  -9.156807    -9.5037565   -9.617552    -9.157907    -9.070938\n",
      "  -9.032086    -9.176242    -9.130736    -8.89763     -9.016725\n",
      "  -8.551382    -8.453135    -9.139585    -9.145734    -9.034928\n",
      "  -9.03739     -9.037259    -9.01385     -9.07973     -9.084103\n",
      "  -9.081525    -9.014957    -9.600722   -10.330755   -10.259497\n",
      "  -9.040934    -9.039095    -8.951047    -8.982486    -9.311341\n",
      "  -9.41368     -8.879144    -8.737907    -8.888105    -8.580415\n",
      "  -9.179514    -9.17594     -9.186187    -9.633521    -9.78814\n",
      "  -9.0391035   -8.793534    -9.280718    -8.214872    -9.22488\n",
      "  -8.978622    -9.068145    -9.090782   -10.33922    -10.29547\n",
      "  -8.475923  ], shape=(136,), dtype=float32) tf.Tensor(\n",
      "[  0.08447362   0.08528276   0.08643728   0.08843495   0.0894673\n",
      "   0.08964626   0.08942667   0.08959882   0.09156869   0.08968288\n",
      "   0.08966096   0.0895297    0.08940382   0.08870724   0.08765882\n",
      "   0.08727239   0.09003592   0.08864394   0.05428025   0.04096223\n",
      "   0.09558921   0.10806642   0.11199738   0.09086271   0.08610138\n",
      "   0.10588443   0.09818548   0.13109815   0.13791448   0.08902642\n",
      "   0.087425     0.10122013   0.10040137   0.10026527   0.10245107\n",
      "   0.09617433   0.09598552   0.09419773   0.10238668   0.03715621\n",
      "  -0.04515854  -0.02990614   0.09998163   0.10015204   0.10831916\n",
      "   0.10362215   0.06943594   0.0580182    0.11346359   0.12827642\n",
      "   0.11419643   0.14323674   0.083405     0.09214475   0.0911978\n",
      "   0.04109496   0.02159243   0.11508607   0.14106171   0.06936136\n",
      "   0.15488316   0.07711892   0.10858377   0.09714738   0.09551673\n",
      "  -0.0463694   -0.03347268   0.15390065  -9.155264    -9.147172\n",
      "  -9.135627    -9.11565     -9.105327    -9.103538    -9.105733\n",
      "  -9.104012    -9.084313    -9.103171    -9.103391    -9.104703\n",
      "  -9.105962    -9.112927    -9.123412    -9.127276    -9.099641\n",
      "  -9.113561    -9.457197    -9.590378    -9.044108    -8.919335\n",
      "  -8.880026    -9.0913725   -9.138987    -8.941155    -9.018146\n",
      "  -8.689018    -8.620855    -9.1097355   -9.12575     -8.987799\n",
      "  -8.995986    -8.997347    -8.97549     -9.038257    -9.040145\n",
      "  -9.0580225   -8.976133    -9.628438   -10.451586   -10.299062\n",
      "  -9.000184    -8.99848     -8.916808    -8.9637785   -9.30564\n",
      "  -9.419818    -8.865364    -8.717236    -8.858036    -8.567633\n",
      "  -9.16595     -9.078552    -9.088022    -9.58905     -9.784076\n",
      "  -8.849139    -8.589383    -9.306386    -8.451168    -9.228811\n",
      "  -8.914163    -9.028526    -9.044833   -10.463694   -10.334727\n",
      "  -8.460994  ], shape=(136,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in flat_data.take(1):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark(flat_data.batch(65536))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark(trimmed_scenarios.batch(65536))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LABEL in [\"Baseline\", \"TFDataOptPrev\"]:\n",
    "    # ------------------------------- DATA PREPROCESSING ------------------------------------------------\n",
    "    i_start = time.time()\n",
    "    X_data, Y_data, U_data, V_data, Yp, Yf = data_handler.create_windows(scenario_data)\n",
    "    i_stop = time.time()\n",
    "    print('[INFO]: Time taken for creating X datasets:', i_stop - i_start, 'seconds')\n",
    "    \n",
    "    # ------------------------------- DATA NORMALIZATION ------------------------------------------------\n",
    "    n_start = time.time()\n",
    "    X_array, Y_array, U_array, V_array, Yp_array, Yf_array = data_handler.scale_data(X_data, Y_data, U_data, V_data, Yp, Yf)\n",
    "    n_stop = time.time()\n",
    "    print('[INFO]: Time taken for normalization:', n_stop - n_start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing callback\n",
    "timing_cb = TimingCallback()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.build(input_shape=(None,136))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LABEL == \"Baseline\":\n",
    "    model.fit(X_array, Y_array, epochs=2, batch_size=_BATCH_SIZE, callbacks=[timing_cb])\n",
    "    m_stop = time.time()\n",
    "elif _LABEL == \"TFDataOptPrev\":\n",
    "    # tensorflow dataset conversion\n",
    "    tf_conv_start = time.time()\n",
    "    training_dataset = tf.data.Dataset.zip((X_array, Y_array)).batch(_BATCH_SIZE)\n",
    "    training_dataset = training_dataset.cache()\n",
    "    training_dataset = training_dataset.shuffle(buffer_size=22)\n",
    "    training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    tf_conv_stop = time.time()\n",
    "    print('[INFO]: Tensorflow Conversion Time:', tf_conv_stop - tf_conv_start, 'seconds')\n",
    "    \n",
    "    m_start = time.time()\n",
    "    model.fit(training_dataset, epochs=_N_EPOCHS, callbacks=[timing_cb])\n",
    "    m_stop = time.time()\n",
    "elif _LABEL == \"TFDataOpt\":\n",
    "    window_size = 800\n",
    "    shift_size = 10\n",
    "    stride = 1\n",
    "    N = 3\n",
    "    training_data_gen = data_handler.get_training_data(scenario_data, window_size, shift_size, stride, N, 1)\n",
    "    training_dataset = training_data_gen.batch(_BATCH_SIZE)\n",
    "    training_dataset = training_dataset.cache()\n",
    "    training_dataset = training_dataset.shuffle(buffer_size=22)\n",
    "    training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    m_start = time.time()\n",
    "    model.fit(training_dataset, epochs=_N_EPOCHS, callbacks=[timing_cb], workers=64, use_multiprocessing=True)\n",
    "    m_stop = time.time()\n",
    "    \n",
    "# print info\n",
    "print('[INFO]: Time taken for model training (time module):', m_stop - m_start, 'seconds')\n",
    "print('[INFO]: Time taken for model training (Keras):', sum(timing_cb.logs), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.sequence.TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 800\n",
    "# shift_size = 10\n",
    "# stride = 1\n",
    "# N = 3\n",
    "\n",
    "# data_handler_seqgen = GridNetworkSequentialGen(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "#                                             n_rows=_NROWS,\n",
    "#                                             n_cols=_NCOLS,\n",
    "#                                             repeat_cols=_REPEAT_COLS,\n",
    "#                                             d_type=_DTYPE\n",
    "#                                          )\n",
    "# trimmed_scenarios_seqgen = data_handler_seqgen.load_grid_data()\n",
    "# training_data_seqgen = data_handler_seqgen.get_training_data(trimmed_scenarios_seqgen, window_size, shift_size, stride, N, 1)\n",
    "# benchmark(training_data_seqgen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
