{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import nvtx\n",
    "\n",
    "import argparse\n",
    "\n",
    "import math\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "# ------------------------------- CUSTOM FUNCTIONS ------------------------------------------------\n",
    "# Custom Library\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "    \n",
    "from proxy_apps.apps.timeseries_prediction import deepDMD, proxyDeepDMD, proxyDeepDMDMGPU\n",
    "\n",
    "from proxy_apps.utils.tf import TimingCallback\n",
    "from proxy_apps.utils.data.main import NpEncoder\n",
    "from proxy_apps.utils import file_reader, path_handler\n",
    "from proxy_apps.utils.data.grid import GridNetworkDataHandler, GridNetworkTFDataHandler, GridNetworkNewGen, TransientDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tensorflow version:  2.4.0\n"
     ]
    }
   ],
   "source": [
    "# System Setup\n",
    "config = file_reader.read_config()\n",
    "\n",
    "_N_EPOCHS = 2\n",
    "_BATCH_SIZE = 65536\n",
    "_APP_NAME = config[\"info\"][\"app_name\"]\n",
    "_NROWS = int(config[\"data\"][\"n_rows\"])\n",
    "_NCOLS = int(config[\"data\"][\"n_cols\"])\n",
    "_REPEAT_COLS = int(config[\"data\"][\"repeat_cols\"])\n",
    "_WINDOW_SIZE = int(config[\"data\"][\"window_size\"])\n",
    "_SHIFT_SIZE = int(config[\"data\"][\"shift_size\"])\n",
    "_STRIDE = int(config[\"data\"][\"stride\"])\n",
    "_N_SIGNALS = int(config[\"data\"][\"n_signals\"])\n",
    "\n",
    "_DTYPE = config[\"model\"][\"dtype\"]\n",
    "\n",
    "_LABEL = \"TFDataOpt\"\n",
    "_SUFFIX =  \"gpu\" + '_' + \\\n",
    "            \"a100\" + '_' + \\\n",
    "            'ng' + str(1) + '_' + \\\n",
    "            'nc' + str(-1) + '_' + \\\n",
    "            'e' + str(_N_EPOCHS) + '_' + \\\n",
    "            'b' + str(_BATCH_SIZE) + '_' + \\\n",
    "            'r' + str(_REPEAT_COLS) + '_' + _LABEL\n",
    "\n",
    "performance_dict = dict()\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# current directory\n",
    "curr_dir = \"./\"\n",
    "\n",
    "# output directory\n",
    "output_dir = path_handler.get_absolute_path(curr_dir, config[\"info\"][\"output_dir\"] + config[\"info\"][\"name\"] + \"/\" + config[\"info\"][\"app_name\"] + \"/\" + _DTYPE + \"/R\" + str(_REPEAT_COLS) + \"/\")\n",
    "if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "\n",
    "# TensorFlow Setup\n",
    "print(\"[INFO] Tensorflow version: \", tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dataset, num_epochs=2, steps=10):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for s, sample in enumerate(dataset):\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            if s == steps - 1:\n",
    "                break\n",
    "            \n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Eager mode:  True\n"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "print(\"[INFO] Eager mode: \", tf.executing_eagerly()) # For easy reset of notebook state.\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_floatx(_DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexer(n_rows, window_size, shift_size, start_point, leave_last):\n",
    "    return np.arange(window_size)[None, :] + start_point + shift_size*np.arange(((n_rows - window_size - leave_last - start_point) // shift_size) + 1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Loading the datasets from the directory: /qfs/people/jain432/pacer/data/TrainingDataIEEE68bus\n",
      "[INFO]: Loading data for 30 scenarios ...\n",
      "[INFO]: Total number of scenarios loaded: 30\n",
      "[INFO]: Shape of each scenario loaded:  (1400, 136)\n",
      "[INFO]: Done ...\n",
      "[INFO]: Original dataset size: 1400\n",
      "[INFO]: Chosen dataset size: 800\n",
      "[INFO]: Length of X_data:  1800\n",
      "[INFO]: Length of each window after down sampling:  (800, 136)\n",
      "[INFO]: Yp_array shape:  (41970, 136)\n",
      "[INFO]: Yf_array shape:  (41970, 136)\n",
      "[INFO]: X_array shape:  (1440000, 136)\n",
      "[INFO]: Y_array shape:  (1440000, 136)\n",
      "[INFO]: U_array shape:  (1440000, 136)\n",
      "[INFO]: V_array shape:  (1440000, 136)\n"
     ]
    }
   ],
   "source": [
    "data_handler = GridNetworkDataHandler(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                        n_rows=_NROWS,\n",
    "                                        n_cols=_NCOLS,\n",
    "                                        repeat_cols=_REPEAT_COLS,\n",
    "                                        dtype=_DTYPE\n",
    "                                     ) \n",
    "\n",
    "scenario_data = data_handler.load_grid_data()\n",
    "X_data, Y_data, U_data, V_data, Yp, Yf = data_handler.create_windows(scenario_data)\n",
    "X_array, Y_array, U_array, V_array, Yp_array, Yf_array = data_handler.scale_data(X_data, Y_data, U_data, V_data, Yp, Yf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = GridNetworkNewGen(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                        n_rows=_NROWS,\n",
    "                                        n_cols=_NCOLS,\n",
    "                                        repeat_cols=_REPEAT_COLS,\n",
    "                                        d_type=_DTYPE\n",
    "                                     )\n",
    "\n",
    "x_indexer = get_indexer(_NROWS, _WINDOW_SIZE, _SHIFT_SIZE, 0, _N_SIGNALS)\n",
    "y_indexer = get_indexer(_NROWS, _WINDOW_SIZE, _SHIFT_SIZE, 1, 0)\n",
    "\n",
    "scenario_data = data_handler.get_training_data(x_indexer, y_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08234169,  0.08398943,  0.08258068,  0.08214683,  0.08244791,\n",
       "        0.08239396,  0.0825465 ,  0.08293292,  0.08758576,  0.08190964,\n",
       "        0.08206848,  0.08195436,  0.08184469,  0.08165109,  0.08036628,\n",
       "        0.0798736 ,  0.08542501,  0.08431926,  0.04962437,  0.03824485,\n",
       "        0.08420936,  0.09290614,  0.09679138,  0.08237581,  0.08692641,\n",
       "        0.11023701,  0.09832751,  0.14486176,  0.15468656,  0.08604157,\n",
       "        0.08542665,  0.09650717,  0.09626098,  0.09627406,  0.09861494,\n",
       "        0.09202703,  0.09158971,  0.09184748,  0.09850423,  0.03992778,\n",
       "       -0.0330755 , -0.0259497 ,  0.09590663,  0.09609054,  0.10489531,\n",
       "        0.10175143,  0.06886591,  0.05863196,  0.1120856 ,  0.12620922,\n",
       "        0.1111895 ,  0.14195852,  0.08204857,  0.08240602,  0.08138131,\n",
       "        0.03664793,  0.02118596,  0.09608962,  0.12064654,  0.07192818,\n",
       "        0.17851271,  0.07751198,  0.10213777,  0.09318556,  0.09092183,\n",
       "       -0.03392203, -0.02954702,  0.15240774,  0.51401846,  0.45662286,\n",
       "        0.28197889,  0.03202051,  0.05080782,  0.07356128, -0.02742099,\n",
       "       -0.0344381 ,  0.33781984,  0.16818737,  0.12378973,  0.53534958,\n",
       "        0.13955134,  0.11290741,  0.1490473 ,  0.31196725,  0.32417093,\n",
       "        0.29508933,  0.49477901, -0.09654854,  0.3091571 ,  0.48803628,\n",
       "        0.43697025,  0.36607046,  0.5443112 ,  0.46018748,  0.35890223,\n",
       "        0.36081654,  0.33845018,  0.46958149,  0.50588277,  0.45693768,\n",
       "        0.50485959,  0.59630839,  0.07521955,  0.37971179,  0.25988892,\n",
       "        0.48817438, -0.01334113,  0.49820169, -0.03491766, -0.01695102,\n",
       "        0.09108868,  0.08407211,  0.08876128,  0.24290215,  0.61529802,\n",
       "        0.61445735,  0.02746932,  0.0115505 ,  0.11516817, -0.10339538,\n",
       "        0.40136834, -0.20413694, -0.1737056 , -0.03177508,  0.11413896,\n",
       "        0.48537678,  0.6193842 ,  0.26409545,  0.0749543 ,  0.05804106,\n",
       "       -0.03904635,  0.12245984,  0.09539023, -0.02341259, -0.00577598,\n",
       "       -0.02499106])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.12206114 -0.13795394 -0.13999129 -0.13877701 -0.13675519 -0.13687104\n",
      " -0.13526483 -0.13446715 -0.12152589 -0.13873522 -0.13813075 -0.13862319\n",
      " -0.13911997 -0.14013874 -0.14458367 -0.14651475 -0.14420605 -0.14259088\n",
      " -0.15075401 -0.15236286 -0.14850716 -0.15057178 -0.15054028 -0.14708136\n",
      " -0.14027052 -0.14327165 -0.14314481 -0.14571935 -0.14655097 -0.12253022\n",
      " -0.12711685 -0.12019089 -0.12233241 -0.12326352 -0.12609909 -0.11877959\n",
      " -0.11593057 -0.12876138 -0.12529942 -0.07787391  0.00333725 -0.02453103\n",
      " -0.12180074 -0.12204847 -0.13365506 -0.13630499 -0.10730083 -0.09660477\n",
      " -0.14428683 -0.15993669 -0.14141032 -0.18326793 -0.13871331 -0.13854983\n",
      " -0.13982429 -0.15246719 -0.15453239 -0.15269285 -0.1530478  -0.14339387\n",
      " -0.14837856 -0.13988188 -0.1158767  -0.11910663 -0.11320636  0.00489531\n",
      " -0.02173361 -0.19421897  0.63451151  0.5317788   0.33944751  0.06799646\n",
      "  0.08201882  0.10255614  0.00699209  0.00289895  0.42210129  0.19097658\n",
      "  0.14857335  0.56093779  0.16398019  0.14160925  0.1764139   0.33918326\n",
      "  0.37256187  0.34680685  0.50197031 -0.09543947  0.32852363  0.50308411\n",
      "  0.45123442  0.39072056  0.61244345  0.5700713   0.44335617  0.52476169\n",
      "  0.51271356  0.57386743  0.6125307   0.53945152  0.59520333  0.68593674\n",
      "  0.17866324  0.44776717  0.30687994  0.60028794  0.10280536  0.80252845\n",
      "  0.01547215 -0.00555249  0.18229836  0.17687813  0.23855136  0.37536631\n",
      "  0.81746802  0.86597999  0.17531942  0.19428631  0.29316407 -0.04184314\n",
      "  0.46300321 -0.19690738 -0.16850186 -0.03067453  0.10904569  0.50046058\n",
      "  0.62901879  0.3114633   0.25095037  0.12631445  0.0182324   0.17301279\n",
      "  0.11730553  0.01712186  0.00191231  0.01362574], shape=(136,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for x, y in scenario_data.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- DATA LOADING ------------------------------------------------   \n",
    "l_start = time.time()\n",
    "if _LABEL == \"Baseline\":\n",
    "    data_handler = GridNetworkDataHandler(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                            n_rows=_NROWS,\n",
    "                                            n_cols=_NCOLS,\n",
    "                                            repeat_cols=_REPEAT_COLS,\n",
    "                                            dtype=_DTYPE\n",
    "                                         ) \n",
    "\n",
    "    scenario_data = data_handler.load_grid_data()\n",
    "elif _LABEL == \"TFDataOptPrev\":\n",
    "    data_handler = GridNetworkTFDataHandler(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                            n_rows=_NROWS,\n",
    "                                            n_cols=_NCOLS,\n",
    "                                            repeat_cols=_REPEAT_COLS,\n",
    "                                            dtype=_DTYPE\n",
    "                                         ) \n",
    "\n",
    "    scenario_data = data_handler.load_grid_data()\n",
    "elif _LABEL == \"TFDataOpt\":\n",
    "    data_handler = GridNetworkNewGen(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                            n_rows=_NROWS,\n",
    "                                            n_cols=_NCOLS,\n",
    "                                            repeat_cols=_REPEAT_COLS,\n",
    "                                            d_type=_DTYPE\n",
    "                                         )\n",
    "\n",
    "    x_indexer = get_indexer(_NROWS, _WINDOW_SIZE, _SHIFT_SIZE, 0, _N_SIGNALS)\n",
    "    y_indexer = get_indexer(_NROWS, _WINDOW_SIZE, _SHIFT_SIZE, 1, 0)\n",
    "\n",
    "    scenario_data = data_handler.get_training_data(x_indexer, y_indexer)\n",
    "\n",
    "l_stop = time.time()\n",
    "print('[INFO]: Time taken for loading datasets:', l_stop - l_start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 800\n",
    "# shift_size = 10\n",
    "# stride = 1\n",
    "# N = 3\n",
    "\n",
    "# x_indexer = get_indexer(_NROWS, window_size, shift_size, 0, 3)\n",
    "# y_indexer = get_indexer(_NROWS, window_size, shift_size, 1, 0)\n",
    "\n",
    "# training_data = data_handler.get_training_data(x_indexer, y_indexer)\n",
    "# benchmark(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LABEL in [\"Baseline\", \"TFDataOptPrev\"]:\n",
    "    # ------------------------------- DATA PREPROCESSING ------------------------------------------------\n",
    "    i_start = time.time()\n",
    "    X_data, Y_data, U_data, V_data, Yp, Yf = data_handler.create_windows(scenario_data)\n",
    "    i_stop = time.time()\n",
    "    print('[INFO]: Time taken for creating X datasets:', i_stop - i_start, 'seconds')\n",
    "    \n",
    "    # ------------------------------- DATA NORMALIZATION ------------------------------------------------\n",
    "    n_start = time.time()\n",
    "    X_array, Y_array, U_array, V_array, Yp_array, Yf_array = data_handler.scale_data(X_data, Y_data, U_data, V_data, Yp, Yf)\n",
    "    n_stop = time.time()\n",
    "    print('[INFO]: Time taken for normalization:', n_stop - n_start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing callback\n",
    "timing_cb = TimingCallback()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.build(input_shape=(None,136))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LABEL == \"Baseline\":\n",
    "    m_start = time.time()\n",
    "    model.fit(X_array, Y_array, epochs=2, batch_size=_BATCH_SIZE, callbacks=[timing_cb])\n",
    "    m_stop = time.time()\n",
    "elif _LABEL == \"TFDataOptPrev\":\n",
    "    # tensorflow dataset conversion\n",
    "    tf_conv_start = time.time()\n",
    "    training_dataset = tf.data.Dataset.zip((X_array, Y_array)).batch(_BATCH_SIZE)\n",
    "    training_dataset = training_dataset.cache()\n",
    "    training_dataset = training_dataset.shuffle(buffer_size=22)\n",
    "    training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    tf_conv_stop = time.time()\n",
    "    print('[INFO]: Tensorflow Conversion Time:', tf_conv_stop - tf_conv_start, 'seconds')\n",
    "    \n",
    "    m_start = time.time()\n",
    "    model.fit(training_dataset, epochs=_N_EPOCHS, callbacks=[timing_cb])\n",
    "    m_stop = time.time()\n",
    "elif _LABEL == \"TFDataOpt\":\n",
    "    training_dataset = scenario_data.batch(_BATCH_SIZE)\n",
    "    training_dataset = training_dataset.cache()\n",
    "    training_dataset = training_dataset.shuffle(buffer_size=x_indexer.shape[0]*x_indexer.shape[1]*len(data_handler.dir_list)//_BATCH_SIZE)\n",
    "    training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    m_start = time.time()\n",
    "    model.fit(training_dataset, epochs=_N_EPOCHS, callbacks=[timing_cb], workers=64, use_multiprocessing=True)\n",
    "    m_stop = time.time()\n",
    "    \n",
    "# print info\n",
    "print('[INFO]: Time taken for model training (time module):', m_stop - m_start, 'seconds')\n",
    "print('[INFO]: Time taken for model training (Keras):', sum(timing_cb.logs), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 800\n",
    "# shift_size = 10\n",
    "# stride = 1\n",
    "# N = 3\n",
    "\n",
    "# data_handler_seqgen = GridNetworkSequentialGen(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "#                                             n_rows=_NROWS,\n",
    "#                                             n_cols=_NCOLS,\n",
    "#                                             repeat_cols=_REPEAT_COLS,\n",
    "#                                             d_type=_DTYPE\n",
    "#                                          )\n",
    "# trimmed_scenarios_seqgen = data_handler_seqgen.load_grid_data()\n",
    "# training_data_seqgen = data_handler_seqgen.get_training_data(trimmed_scenarios_seqgen, window_size, shift_size, stride, N, 1)\n",
    "# benchmark(training_data_seqgen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
