{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6086d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import nvtx\n",
    "\n",
    "import argparse\n",
    "\n",
    "import math\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "# ------------------------------- CUSTOM FUNCTIONS ------------------------------------------------\n",
    "# Custom Library\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "    \n",
    "from proxy_apps.apps.timeseries_prediction import deepDMD, proxyDeepDMD, proxyDeepDMDMGPU\n",
    "\n",
    "from proxy_apps.utils.tf import TimingCallback\n",
    "from proxy_apps.utils.data.main import NpEncoder\n",
    "from proxy_apps.utils import file_reader, path_handler\n",
    "from proxy_apps.utils.data.grid import GridNetworkDataHandler, GridNetworkTFDataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0a0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen():\n",
    "#   yield 1\n",
    "#   yield 2\n",
    "#   yield 3\n",
    "\n",
    "# ds = tf.data.Dataset.from_generator(gen, output_shapes=(), output_types=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f71b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds1 = tf.data.Dataset.from_tensor_slices(np.random.random((9, 3)))\n",
    "# ds2 = tf.data.Dataset.from_tensor_slices(np.random.random((9, 1)))\n",
    "\n",
    "# ds = tf.data.Dataset.zip((ds1, ds2))\n",
    "# for a in ds:\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7ba8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_and_batched = ds.batch(3).shuffle(5)\n",
    "# for a in shuffled_and_batched:\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7f195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tensorflow version:  2.4.0\n"
     ]
    }
   ],
   "source": [
    "# System Setup\n",
    "config = file_reader.read_config()\n",
    "\n",
    "_N_EPOCHS = 2\n",
    "_BATCH_SIZE = 64000\n",
    "_APP_NAME = config[\"info\"][\"app_name\"]\n",
    "_NROWS = int(config[\"data\"][\"n_rows\"])\n",
    "_NCOLS = int(config[\"data\"][\"n_cols\"])\n",
    "_REPEAT_COLS = 1 # int(config[\"data\"][\"repeat_cols\"])\n",
    "_DTYPE = config[\"model\"][\"dtype\"]\n",
    "\n",
    "_LABEL = \"Baseline\"\n",
    "_SUFFIX =  \"gpu\" + '_' + \\\n",
    "            \"a100\" + '_' + \\\n",
    "            'ng' + str(1) + '_' + \\\n",
    "            'nc' + str(-1) + '_' + \\\n",
    "            'e' + str(_N_EPOCHS) + '_' + \\\n",
    "            'b' + str(_BATCH_SIZE) + '_' + \\\n",
    "            'r' + str(_REPEAT_COLS) + '_' + _LABEL\n",
    "\n",
    "performance_dict = dict()\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# current directory\n",
    "curr_dir = \"./\"\n",
    "\n",
    "# output directory\n",
    "output_dir = path_handler.get_absolute_path(curr_dir, config[\"info\"][\"output_dir\"] + config[\"info\"][\"name\"] + \"/\" + config[\"info\"][\"app_name\"] + \"/\" + _DTYPE + \"/R\" + str(_REPEAT_COLS) + \"/\")\n",
    "if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "\n",
    "# TensorFlow Setup\n",
    "print(\"[INFO] Tensorflow version: \", tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05c4930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Eager mode:  True\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Eager mode: \", tf.executing_eagerly()) # For easy reset of notebook state.\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "# tf.config.optimizer.set_jit(True) # Enable XLA.\n",
    "\n",
    "# if _LABEL in [\"Baseline\"]: tf.keras.backend.set_floatx('float64')\n",
    "# elif _LABEL in [\"TFDataOpt\", \"TFDataOptMGPU\"]:\n",
    "#     tf.keras.backend.set_floatx(_DTYPE)\n",
    "# elif _LABEL in [\"TFDataOptMP\", \"TFDataOptMGPUMP\"]:\n",
    "#     _DTYPE = \"float32\"\n",
    "#     policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "#     tf.keras.mixed_precision.set_global_policy(policy)\n",
    "#     # tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "\n",
    "# if _LABEL in [\"TFDataOpt\", \"TFDataOptMP\", \"TFDataOptMGPU\", \"TFDataOptMGPUMP\"]:\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925f7c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Loading the datasets from the directory: /qfs/people/jain432/pacer/data/TrainingDataIEEE68bus\n",
      "[INFO]: Loading data for 30 scenarios ...\n",
      "[INFO]: Total number of scenarios loaded: 30\n",
      "[INFO]: Shape of each scenario loaded:  (1400, 136)\n",
      "[INFO]: Done ...\n",
      "[INFO]: Time taken for loading datasets: 2.3854751586914062 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- DATA LOADING ------------------------------------------------   \n",
    "\n",
    "data_loading = nvtx.start_range(\"Data Loading\")\n",
    "l_start = time.time()\n",
    "data_handler = GridNetworkDataHandler(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                        n_rows=_NROWS,\n",
    "                                        n_cols=_NCOLS,\n",
    "                                        repeat_cols=_REPEAT_COLS,\n",
    "                                        dtype=_DTYPE\n",
    "                                     ) \n",
    "\n",
    "scenario_data = data_handler.load_grid_data()\n",
    "l_stop = time.time()\n",
    "print('[INFO]: Time taken for loading datasets:', l_stop - l_start, 'seconds')\n",
    "performance_dict['data_loading_time'] = l_stop-l_start\n",
    "nvtx.end_range(data_loading)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de434ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Original dataset size: 1400\n",
      "[INFO]: Chosen dataset size: 800\n",
      "[INFO]: Length of X_data:  1800\n",
      "[INFO]: Length of each window after down sampling:  (800, 136)\n",
      "[INFO]: Time taken for creating X datasets: 2.529825448989868 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- DATA PREPROCESSING ------------------------------------------------\n",
    "\n",
    "data_processing = nvtx.start_range(\"Data Processing\")\n",
    "i_start = time.time()\n",
    "X_data, Y_data, U_data, V_data, Yp, Yf = data_handler.create_windows(scenario_data)\n",
    "i_stop = time.time()\n",
    "print('[INFO]: Time taken for creating X datasets:', i_stop - i_start, 'seconds')\n",
    "performance_dict['data_processing_time'] = i_stop-i_start\n",
    "nvtx.end_range(data_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b58bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Yp_array shape:  (41970, 136)\n",
      "[INFO]: Yf_array shape:  (41970, 136)\n",
      "[INFO]: X_array shape:  (1440000, 136)\n",
      "[INFO]: Y_array shape:  (1440000, 136)\n",
      "[INFO]: U_array shape:  (1440000, 136)\n",
      "[INFO]: V_array shape:  (1440000, 136)\n",
      "[INFO]: Time taken for normalization: 7.973409414291382 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- DATA NORMALIZATION ------------------------------------------------\n",
    "\n",
    "data_norm = nvtx.start_range(\"Data Normalization\")\n",
    "n_start = time.time()\n",
    "X_array, Y_array, U_array, V_array, Yp_array, Yf_array = data_handler.scale_data(X_data, Y_data, U_data, V_data, Yp, Yf)\n",
    "n_stop = time.time()\n",
    "print('[INFO]: Time taken for normalization:', n_stop - n_start, 'seconds')\n",
    "\n",
    "performance_dict['data_scaling_time'] = n_stop-n_start\n",
    "nvtx.end_range(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "135ab8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- MODEL SETUP ------------------------------------------------\n",
    "# timing callback\n",
    "timing_cb = TimingCallback()\n",
    "\n",
    "# Stopping criteria if the training loss doesn't go down by 1e-3\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', min_delta = 1e-3, verbose = 1, mode='min', patience = 3, \n",
    "    baseline=None, restore_best_weights=True)\n",
    "\n",
    "# Create a TensorBoard Profiler\n",
    "logs = path_handler.get_absolute_path(curr_dir, config[\"model\"][\"tb_log_dir\"] + _APP_NAME + \"/\" + _DTYPE + \"/R\" + str(_REPEAT_COLS) + \"/tensorboard/\" + _SUFFIX)\n",
    "# tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logs, histogram_freq=1, embeddings_freq=1, profile_batch=(5,15))\n",
    "\n",
    "# all callbacks\n",
    "callbacks=[early_stop_cb, timing_cb]#, tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a7cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hyperparameters - we can keep it as a dict instead of creating a separate class\n",
    "hyper_param_dict = config[\"model\"][\"hyperparameters\"]\n",
    "hyper_param_dict['original_dim']       = _REPEAT_COLS * _NCOLS   # input data dimension\n",
    "hyper_param_dict['num_epochs']         = _N_EPOCHS  # Number of epochs  \n",
    "hyper_param_dict['batch_size']         = _BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1fa9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = Yp_array.shape[0]\n",
    "\n",
    "Xd_array = tf.data.Dataset.from_tensor_slices(X_array)\n",
    "Yd_array = tf.data.Dataset.from_tensor_slices(Y_array)\n",
    "Ud_array = tf.data.Dataset.from_tensor_slices(U_array)\n",
    "Vd_array = tf.data.Dataset.from_tensor_slices(V_array)\n",
    "Ydp_array = tf.data.Dataset.from_tensor_slices(Yp_array)\n",
    "Ydf_array = tf.data.Dataset.from_tensor_slices(Yf_array)\n",
    "\n",
    "hyper_param_dict['dtype']         = _DTYPE\n",
    "hp = proxyDeepDMD.HyperParameters(hyper_param_dict)\n",
    "hp.model_name         = _LABEL\n",
    "\n",
    "performance_dict[\"n_epochs\"] = hp.ep\n",
    "performance_dict[\"batch_size\"] = hp.bs\n",
    "# performance_dict[\"n_training_batches\"] = n_batches_training\n",
    "# performance_dict[\"n_val_batches\"] = n_batches - n_batches_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d539e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- MODEL TRAINING ------------------------------------------------\n",
    "# Initialize, build, and fit the model\n",
    "# with mirrored_strategy.scope():\n",
    "with mirrored_strategy.scope():\n",
    "    K_model = proxyDeepDMDMGPU.Encoder(hp)\n",
    "    optimizer = tf.optimizers.Adagrad(hp.lr)\n",
    "\n",
    "BATCH_SIZE = hp.bs # * mirrored_strategy.num_replicas_in_sync\n",
    "zip_data = tf.data.Dataset.zip((Xd_array, Yd_array)).batch(BATCH_SIZE)\n",
    "\n",
    "training_dataset = zip_data.cache()\n",
    "training_dataset = training_dataset.shuffle(buffer_size=BATCH_SIZE)\n",
    "training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "training_dataset = mirrored_strategy.experimental_distribute_dataset(training_dataset)#.cache()#.with_options(options)# .take(n_batches_training)\n",
    "# training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "K_model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cbb6ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.python.keras.engine import data_adapter\n",
    "# from tensorflow.python.keras.engine import training_utils\n",
    "# from tensorflow.python.keras import callbacks as callbacks_module\n",
    "# from tensorflow.python.profiler import trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0e418c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Neural Network\n",
    "# class NeuralNetworkModel(): \n",
    "#     def __init__(self, hp, model):\n",
    "#         self.encoder = model\n",
    "\n",
    "#         # other parameters\n",
    "#         self.rf = hp.rf \n",
    "#         self.d_type = hp.d_type\n",
    "#         self.model_name = hp.model_name\n",
    "        \n",
    "#     def fit(self, training_dataset, n_epochs, batch_size, steps_per_epoch, callbacks, initial_epoch=0, verbose=1):\n",
    "#         m_start = time.time()\n",
    "#         for epoch in range(n_epochs):\n",
    "#             print(\"\\nEpoch {}/{}\".format(epoch+1, n_epochs))\n",
    "#             pb_i = Progbar(steps_per_epoch, stateful_metrics=['loss'])\n",
    "\n",
    "#             # Iterate over the batches of the dataset.\n",
    "#             total_loss = 0.0\n",
    "#             num_batches = 0\n",
    "#             for step, inp_data in enumerate(training_dataset):\n",
    "#                 total_loss += self.distributed_train_step(inp_data)\n",
    "#                 num_batches += 1\n",
    "\n",
    "#                 loss_value = total_loss / num_batches\n",
    "\n",
    "#                 # Log every 200 batches.\n",
    "#                 pb_i.add(batch_size//batch_size, values=[('loss', loss_value)])\n",
    "                \n",
    "#         m_stop = time.time()\n",
    "#         print(m_stop-m_start)\n",
    "                    \n",
    "    \n",
    "# #     def fit(self, training_dataset, n_epochs, batch_size, steps_per_epoch, callbacks, initial_epoch=0, verbose=1):\n",
    "# #         m_start = time.time()\n",
    "# #         with self.encoder.distribute_strategy.scope(), \\\n",
    "# #          training_utils.RespectCompiledTrainableState(self.encoder):\n",
    "# #             data_handler = data_adapter.DataHandler(\n",
    "# #                 x=training_dataset,\n",
    "# #                 y=None,\n",
    "# #                 steps_per_epoch=steps_per_epoch,\n",
    "# #                 batch_size=batch_size,\n",
    "# #                 model=self.encoder,\n",
    "# #                 epochs=n_epochs,\n",
    "# #                 initial_epoch=initial_epoch,\n",
    "# #                 steps_per_execution=self.encoder._steps_per_execution)\n",
    "\n",
    "# #             if not isinstance(callbacks, callbacks_module.CallbackList):\n",
    "# #                 callbacks = callbacks_module.CallbackList(\n",
    "# #                     callbacks,\n",
    "# #                     add_history=True,\n",
    "# #                     add_progbar=verbose != 0,\n",
    "# #                     model=self.encoder,\n",
    "# #                     verbose=verbose,\n",
    "# #                     epochs=n_epochs,\n",
    "# #                     steps=data_handler.inferred_steps)\n",
    "\n",
    "# #             self.stop_training = False\n",
    "# #             callbacks.on_train_begin()\n",
    "            \n",
    "# #             training_logs = None\n",
    "# #             # Handle fault-tolerance for multi-worker.\n",
    "# #             # TODO(omalleyt): Fix the ordering issues that mean this has to\n",
    "# #             # happen after `callbacks.on_train_begin`.\n",
    "# #             data_handler._initial_epoch = (  # pylint: disable=protected-access\n",
    "# #                 self.encoder._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n",
    "# #             logs = None\n",
    "# #             for epoch, iterator in data_handler.enumerate_epochs():\n",
    "# #                 print(epoch, iterator)\n",
    "# #                 callbacks.on_epoch_begin(epoch)\n",
    "# #                 with data_handler.catch_stop_iteration():\n",
    "# #                     for step, inp_data in enumerate(training_dataset):\n",
    "# #                         print(step)\n",
    "# #                         with trace.Trace('train', epoch_num=n_epochs, step_num=step, batch_size=batch_size, _r=1):\n",
    "# #                             callbacks.on_train_batch_begin(step)\n",
    "# #                             loss = self.distributed_train_step(inp_data)\n",
    "                            \n",
    "# #                             if data_handler.should_sync:\n",
    "# #                                 context.async_wait()\n",
    "                            \n",
    "# #                             logs = tmp_logs  # No error, now safe to assign to logs.\n",
    "# #                             end_step = step + data_handler.step_increment\n",
    "# #                             callbacks.on_train_batch_end(end_step, logs)\n",
    "# #                             if self.stop_training: break\n",
    "                \n",
    "# #                 print(\"\\nepoch {}/{}\".format(epoch+1, n_epochs))\n",
    "# #                 pb_i = Progbar(math.ceil(1440000//batch_size)+1, stateful_metrics=['loss'])\n",
    "\n",
    "# #                 # Iterate over the batches of the dataset.\n",
    "# #                 total_loss = 0.0\n",
    "# #                 num_batches = 0\n",
    "# #                 for step, inp_data in enumerate(training_dataset):\n",
    "# #                     total_loss += self.distributed_train_step(inp_data)\n",
    "# #                     num_batches += 1\n",
    "\n",
    "# #                     loss_value = total_loss / num_batches\n",
    "\n",
    "# #                     # Log every 200 batches.\n",
    "# #                     pb_i.add(batch_size//batch_size, values=[('loss', loss_value)])\n",
    "\n",
    "# #         callbacks.on_train_end(logs=training_logs)\n",
    "# #         m_stop = time.time()\n",
    "# #         print(m_stop-m_start)\n",
    "        \n",
    "#     @tf.function(experimental_compile=True)\n",
    "#     def distributed_train_step(self, dist_inputs):\n",
    "#         per_replica_losses = self.encoder.distribute_strategy.run(self.train_step, args=(dist_inputs,))\n",
    "#         return self.encoder.distribute_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "#     # @tf.function(experimental_compile=True) # (input_signature=(tf.TensorSpec(shape=[None], dtype=tf.float64),))\n",
    "#     def train_step(self, inputs):       \n",
    "#         X, Y        = inputs\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             Psi_X    = self.encoder(X, training=True)\n",
    "#             Psi_Y    = self.encoder(Y, training=False)    \n",
    "\n",
    "#             PSI_X    = tf.concat([X, tf.cast(Psi_X, self.d_type)], 1)\n",
    "#             PSI_Y    = tf.concat([Y, tf.cast(Psi_Y, self.d_type)], 1) \n",
    "\n",
    "#             # 1-time step evolution on observable space:\n",
    "#             K_PSI_X  = tf.matmul(PSI_X, self.encoder.KO) \n",
    "            \n",
    "#             # 1-step Koopman loss on observable space:        \n",
    "#             K_loss   = tf.norm(PSI_Y - K_PSI_X, axis = [0,1], ord = 'fro')\n",
    "\n",
    "#             # Regularization loss on Koopman operator:\n",
    "#             Reg_loss= tf.math.scalar_mul(self.rf, tf.norm(self.encoder.KO, axis = [0,1], ord = 'fro'))      \n",
    "        \n",
    "#             # Total loss:\n",
    "#             loss = K_loss + Reg_loss\n",
    "#             if self.model_name in [\"TFDataOptMP\", \"TFDataOptMGPUMP\"]:\n",
    "#                 loss = self.optimizer.get_scaled_loss(loss)\n",
    "            \n",
    "#             # tf.print(\"K Loss: \", K_loss, \"Reg Loss: \", Reg_loss, \"Total Loss: \", loss)\n",
    "#             # loss += sum(self.encoder.losses)\n",
    "            \n",
    "#         # Compute gradients\n",
    "#         trainable_vars = self.encoder.trainable_variables\n",
    "#         gradients = tape.gradient(loss, trainable_vars)\n",
    "#         if self.model_name in [\"TFDataOptMP\", \"TFDataOptMGPUMP\"]:\n",
    "#             gradients = self.encoder.optimizer.get_unscaled_gradients(gradients)\n",
    "\n",
    "#         # Update weights\n",
    "#         self.encoder.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "#         # Return a dict mapping metric names to current value.\n",
    "#         # Note that it will include the loss (tracked in self.metrics).\n",
    "#         return loss\n",
    "        \n",
    "#     # @tf.function(experimental_compile=True)\n",
    "#     def test_step(self, inputs):       \n",
    "#         X, Y        = inputs\n",
    "\n",
    "#         Psi_X    = self.encoder(X, training=False)\n",
    "#         Psi_Y    = self.encoder(Y, training=False)    \n",
    "\n",
    "#         PSI_X    = tf.concat([X, tf.cast(Psi_X, self.d_type)], 1)\n",
    "#         PSI_Y    = tf.concat([Y, tf.cast(Psi_Y, self.d_type)], 1) \n",
    "            \n",
    "#         # 1-time step evolution on observable space:\n",
    "#         K_PSI_X  = tf.matmul(PSI_X, self.encoder.KO) \n",
    "\n",
    "#         # 1-step Koopman loss on observable space:        \n",
    "#         K_loss   = tf.norm(PSI_Y - K_PSI_X, axis = [0,1], ord = 'fro')\n",
    "\n",
    "#         # Regularization loss on Koopman operator:\n",
    "#         Reg_loss= tf.math.scalar_mul(self.rf, tf.norm(self.encoder.KO, axis = [0,1], ord = 'fro'))        \n",
    "\n",
    "#         # Total loss:\n",
    "#         loss = K_loss + Reg_loss\n",
    "#         if self.model_name in [\"TFDataOptMP\", \"TFDataOptMGPUMP\"]:\n",
    "#             loss = self.optimizer.get_scaled_loss(loss)\n",
    "#         # loss += sum(self.encoder.losses)\n",
    "            \n",
    "#         # Return a dict mapping metric names to current value.\n",
    "#         # Note that it will include the loss (tracked in self.metrics).\n",
    "#         return loss\n",
    "        \n",
    "#     # @tf.function(experimental_compile=True)\n",
    "#     def predict_step(self, inputs):       \n",
    "#         X, Y        = inputs\n",
    "\n",
    "#         Psi_X    = self.encoder(X, training=False)\n",
    "#         Psi_Y    = self.encoder(Y, training=False)    \n",
    "\n",
    "#         PSI_X    = tf.concat([X, tf.cast(Psi_X, self.d_type)], 1)\n",
    "#         PSI_Y    = tf.concat([Y, tf.cast(Psi_Y, self.d_type)], 1) \n",
    "\n",
    "#         # 1-time step evolution on observable space:\n",
    "#         K_PSI_X  = tf.matmul(PSI_X, self.encoder.KO) \n",
    "\n",
    "#         # 1-step Koopman loss on observable space:        \n",
    "#         K_loss   = tf.norm(PSI_Y - K_PSI_X, axis = [0,1], ord = 'fro')\n",
    "        \n",
    "#         # Regularization loss on Koopman operator:\n",
    "#         Reg_loss= tf.math.scalar_mul(self.rf, tf.norm(self.encoder.KO, axis = [0,1], ord = 'fro'))        \n",
    "\n",
    "#         return Psi_X, PSI_X, Psi_Y, PSI_Y, K_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b38dcdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "\n",
      "\n",
      "K Loss:  1044.7135270760493 Reg Loss:  0.10030508732002842 Total Loss:  1047.2439438038793 Model Losses:  [0, 0, 1.2577330890938361, 0, 0.72761753728951273, 0, 0.4447610141265918, 0, 0]\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "\n",
      "\n",
      "K Loss:  936.77689264865637 Reg Loss:  0.10028018220344674 Total Loss:  939.307022418587 Model Losses:  [0, 0, 1.2576593391696684, 0, 0.72749810552133232, 0, 0.44469214303629234, 0, 0]\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "\n",
      "\n",
      "K Loss:  895.81160177333652 Reg Loss:  0.10026710292335085 Total Loss:  898.34159410917516 Model Losses:  [0, 0, 1.2576075543644403, 0, 0.72746725379029664, 0, 0.44465042476059197, 0, 0]\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "\n",
      "\n",
      "K Loss:  858.82309922248839 Reg Loss:  0.10025591366859665 Total Loss:  861.35301921770952 Model Losses:  [0, 0, 1.2575825710714186, 0, 0.72745577225098512, 0, 0.44462573823019441, 0, 0]\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "\n",
      "\n",
      "K Loss:  834.86564000104738 Reg Loss:  0.10024804581880549 Total Loss:  837.39552130937989 Model Losses:  [0, 0, 1.2575724608852161, 0, 0.7274518024715223, 0, 0.44460899915698066, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  814.60577654565088 Reg Loss:  0.10024406586434084 Total Loss:  817.135647907408 Model Losses:  [0, 0, 1.2575690302795686, 0, 0.72745727570431973, 0, 0.44460098990896835, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  798.43887992729515 Reg Loss:  0.10024185994861615 Total Loss:  800.96875313720545 Model Losses:  [0, 0, 1.2575719953865276, 0, 0.72746463754699708, 0, 0.44459471702820852, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  783.78752749661226 Reg Loss:  0.10024025667395836 Total Loss:  786.31740514900025 Model Losses:  [0, 0, 1.2575742431698149, 0, 0.72747348244667709, 0, 0.44458967009758737, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  781.66876377967026 Reg Loss:  0.10023821650372497 Total Loss:  784.19863944572626 Model Losses:  [0, 0, 1.2575747250495524, 0, 0.72747921649823843, 0, 0.44458350800445545, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  756.89223977153051 Reg Loss:  0.10022858524341043 Total Loss:  759.42205339181021 Model Losses:  [0, 0, 1.2575543266382843, 0, 0.72746681163067528, 0, 0.44456389676733965, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  523.74410522918015 Reg Loss:  0.10022671286648356 Total Loss:  526.27391653859149 Model Losses:  [0, 0, 1.2575532618529934, 0, 0.72747113227607452, 0, 0.444560202415724, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  735.23606577914575 Reg Loss:  0.10022768810103296 Total Loss:  737.76589028880926 Model Losses:  [0, 0, 1.257558147832883, 0, 0.72747838843916135, 0, 0.44456028529042679, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  728.56117252483386 Reg Loss:  0.10022836614724014 Total Loss:  731.091015460584 Model Losses:  [0, 0, 1.2575652487573363, 0, 0.72748809599794462, 0, 0.4445612248475726, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  716.46466754689413 Reg Loss:  0.10022682091293574 Total Loss:  718.99451058254726 Model Losses:  [0, 0, 1.2575653907531266, 0, 0.72749303159366285, 0, 0.44455779239346849, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  701.2003675064251 Reg Loss:  0.10022405043941802 Total Loss:  703.73020672140808 Model Losses:  [0, 0, 1.257565086398196, 0, 0.72749530343078361, 0, 0.44455477471450555, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  692.15173309014926 Reg Loss:  0.10022615375093921 Total Loss:  694.68159805940627 Model Losses:  [0, 0, 1.2575732033549243, 0, 0.72750724743141593, 0, 0.44455836471974658, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  681.929892460429 Reg Loss:  0.10022714942262613 Total Loss:  684.45977754431533 Model Losses:  [0, 0, 1.2575806284193358, 0, 0.727515878257375, 0, 0.44456142778708058, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  668.88623853714216 Reg Loss:  0.10022906035976385 Total Loss:  671.41615113124431 Model Losses:  [0, 0, 1.2575891520674496, 0, 0.72752812601875816, 0, 0.44456625565614366, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  658.96243989588891 Reg Loss:  0.10023186321597935 Total Loss:  661.49238768072632 Model Losses:  [0, 0, 1.2576021460176225, 0, 0.72754052137680714, 0, 0.44457325422702743, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  648.11396563514734 Reg Loss:  0.10023363606432881 Total Loss:  650.6439408346838 Model Losses:  [0, 0, 1.2576106679223029, 0, 0.72755215694504338, 0, 0.44457873860474362, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  639.88712138184781 Reg Loss:  0.10023624070588466 Total Loss:  642.41713371571768 Model Losses:  [0, 0, 1.2576235650486656, 0, 0.7275659899920518, 0, 0.44458653812329613, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  628.3718781760349 Reg Loss:  0.10023849763896647 Total Loss:  630.90192517933974 Model Losses:  [0, 0, 1.2576353790148314, 0, 0.72757926028128717, 0, 0.44459386636970227, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  626.8051981245643 Reg Loss:  0.10024238777770929 Total Loss:  629.335290790039 Model Losses:  [0, 0, 1.2576505746427116, 0, 0.727595590827312, 0, 0.44460411222699969, 0, 0]\n",
      "tf.Tensor(748.473362800752, shape=(), dtype=float64)\n",
      "\n",
      "\n",
      "K Loss:  609.93532077001271 Reg Loss:  0.10024322053806982 Total Loss:  612.46544252353419 Model Losses:  [0, 0, 1.2576597994204903, 0, 0.72760666763437465, 0, 0.44461206592846564, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  599.72563682115742 Reg Loss:  0.10024708299246909 Total Loss:  602.25580542569151 Model Losses:  [0, 0, 1.2576755594950508, 0, 0.72762312325418366, 0, 0.44462283879237763, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  606.93933798318471 Reg Loss:  0.10025143249477009 Total Loss:  609.46955683103431 Model Losses:  [0, 0, 1.2576920470773683, 0, 0.727641224550471, 0, 0.44463414372692117, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  588.87501163216962 Reg Loss:  0.10024868216965734 Total Loss:  591.40523331964653 Model Losses:  [0, 0, 1.2576919285742452, 0, 0.72764572407726225, 0, 0.44463535265579368, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  574.38694834635385 Reg Loss:  0.10024899737122311 Total Loss:  576.91719514680676 Model Losses:  [0, 0, 1.2576992423754656, 0, 0.72765643409350833, 0, 0.44464212661278496, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  564.71793840976341 Reg Loss:  0.10025249200638885 Total Loss:  567.24823242833679 Model Losses:  [0, 0, 1.2577157776161867, 0, 0.72767331394677437, 0, 0.44465243500412172, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  554.08929030493914 Reg Loss:  0.10025596248293127 Total Loss:  556.61963069408068 Model Losses:  [0, 0, 1.2577306396709718, 0, 0.7276900002658454, 0, 0.44466378672175516, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  545.75686525715014 Reg Loss:  0.10026048039858643 Total Loss:  548.28726111313085 Model Losses:  [0, 0, 1.257749352460007, 0, 0.72770921896559848, 0, 0.4446768041565351, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  535.50728981082409 Reg Loss:  0.10026483841866078 Total Loss:  538.03774198192252 Model Losses:  [0, 0, 1.2577690905662324, 0, 0.72772844313915974, 0, 0.44468979897443117, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  528.27554670570294 Reg Loss:  0.10026983668687565 Total Loss:  530.80605723127837 Model Losses:  [0, 0, 1.2577892720501755, 0, 0.72774800192536215, 0, 0.44470341491312049, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  521.25275812718269 Reg Loss:  0.10027440533498273 Total Loss:  523.78332474065121 Model Losses:  [0, 0, 1.2578088112819923, 0, 0.72776695540978176, 0, 0.44471644144177758, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  511.61372921007626 Reg Loss:  0.10027766072201902 Total Loss:  514.14434474819814 Model Losses:  [0, 0, 1.2578269616886566, 0, 0.72778348933862824, 0, 0.44472742637256196, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  504.67470207876443 Reg Loss:  0.10028153925370054 Total Loss:  507.20537057795769 Model Losses:  [0, 0, 1.2578451254908702, 0, 0.72780193345735145, 0, 0.44473990099134597, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  492.60972940576852 Reg Loss:  0.10028536333150431 Total Loss:  495.14045199292548 Model Losses:  [0, 0, 1.2578653478231341, 0, 0.72781970427039688, 0, 0.44475217173190634, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  486.95428230143438 Reg Loss:  0.10028977760441089 Total Loss:  489.48506172349391 Model Losses:  [0, 0, 1.2578857224951299, 0, 0.727838914533151, 0, 0.4447650074268148, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  475.92773072863338 Reg Loss:  0.1002936583592772 Total Loss:  478.45856677532504 Model Losses:  [0, 0, 1.2579063753591735, 0, 0.72785875984074178, 0, 0.44477725313245015, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  475.08258039917757 Reg Loss:  0.10029865407490923 Total Loss:  477.61347744659696 Model Losses:  [0, 0, 1.2579292619864886, 0, 0.7278783224980393, 0, 0.44479080885998462, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  322.56861756558806 Reg Loss:  0.10030121528814176 Total Loss:  325.09956236457322 Model Losses:  [0, 0, 1.2579461269655383, 0, 0.72789548182019082, 0, 0.44480197491128742, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  451.85436092341376 Reg Loss:  0.10030509598438216 Total Loss:  454.38534905093906 Model Losses:  [0, 0, 1.2579631813781096, 0, 0.72790920274739035, 0, 0.44481064741544551, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  447.65066355440274 Reg Loss:  0.1003100443260927 Total Loss:  450.18171149643609 Model Losses:  [0, 0, 1.2579844407741161, 0, 0.72792954984317815, 0, 0.44482390708999653, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  438.10905656419737 Reg Loss:  0.10031372036545813 Total Loss:  440.64015552112329 Model Losses:  [0, 0, 1.25800557171849, 0, 0.72794561101580169, 0, 0.44483405382615937, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  434.99066222739384 Reg Loss:  0.10031738328749851 Total Loss:  437.52181192202613 Model Losses:  [0, 0, 1.2580222762034545, 0, 0.72796420547619511, 0, 0.44484582966513769, 0, 0]\n",
      "\n",
      "\n",
      "K Loss:  427.52068042295252 Reg Loss:  0.10032017604999265 Total Loss:  430.05188203897205 Model Losses:  [0, 0, 1.2580442410224919, 0, 0.72798094521720558, 0, 0.44485625372986409, 0, 0]\n",
      "tf.Tensor(511.18361856933404, shape=(), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([748.473362800752, 511.18361856933404],\n",
       " [33.532193660736084, 28.357341527938843],\n",
       " [1.4579214635102644, 1.23292789251908])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = proxyDeepDMDMGPU.NeuralNetworkModel(hp, K_model)\n",
    "trainer.fit(training_dataset, n_epochs=hp.ep, batch_size=hp.bs, steps_per_epoch=math.ceil(X_array.shape[0]//hp.bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4ad54e6-1407-417b-a670-f99c279a128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainer.encoder.compiled_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "081e7a6f-af86-468d-b2e8-30bfcd13f603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfd2123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.zip((Ydp_array, Ydf_array)).batch(Yp_array.shape[0], drop_remainder=True)\n",
    "test_data = test_data.cache()\n",
    "test_data = test_data.shuffle(buffer_size=Yp_array.shape[0])\n",
    "test_data = test_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "for t in test_data.take(1):\n",
    "    Psi_X, PSI_X, Psi_Y, PSI_Y, Kloss = trainer.predict_step(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f3decb5-6465-4734-b9ab-672d444bc805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440000, 136)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "707b2fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\n",
      "\n",
      "K Loss:  1052.9738396925613 Reg Loss:  0.099785088530191274 Total Loss:  1053.0736247810914 Model Losses:  [0, 0, 1.2577330890938361, 0, 0.72761753728951273, 0, 0.4447610141265918, 0, 0]\n",
      " 1/23 [>.............................] - ETA: 47s - loss: 1055.5037\n",
      "\n",
      "K Loss:  945.78030606629716 Reg Loss:  0.099763340365798286 Total Loss:  945.880069406663 Model Losses:  [0, 0, 1.2576058192015129, 0, 0.72748996245242148, 0, 0.44469484053360614, 0, 0]\n",
      " 2/23 [=>............................] - ETA: 14s - loss: 1028.7053\n",
      "\n",
      "K Loss:  896.22574158832174 Reg Loss:  0.099751409419524151 Total Loss:  896.32549299774121 Model Losses:  [0, 0, 1.2575500754567326, 0, 0.72744849678234025, 0, 0.44466541062805592, 0, 0]\n",
      " 3/23 [==>...........................] - ETA: 13s - loss: 1008.3112\n",
      "\n",
      "K Loss:  863.29514458969084 Reg Loss:  0.099743407390436384 Total Loss:  863.39488799708124 Model Losses:  [0, 0, 1.257525240435833, 0, 0.72742998679527626, 0, 0.44466415338249859, 0, 0]\n",
      " 4/23 [====>.........................] - ETA: 12s - loss: 991.7579 \n",
      "\n",
      "K Loss:  837.89769757531383 Reg Loss:  0.09973721864473728 Total Loss:  837.99743479395852 Model Losses:  [0, 0, 1.2575116734979797, 0, 0.72742473675630126, 0, 0.4446709353589125, 0, 0]\n",
      " 5/23 [=====>........................] - ETA: 12s - loss: 977.7592\n",
      "\n",
      "K Loss:  817.18396492127783 Reg Loss:  0.09973225284534723 Total Loss:  817.28369717412318 Model Losses:  [0, 0, 1.257504577093139, 0, 0.7274223136806045, 0, 0.444682269161275, 0, 0]\n",
      " 6/23 [======>.......................] - ETA: 11s - loss: 965.5919\n",
      "\n",
      "K Loss:  799.2079786877589 Reg Loss:  0.099728191846272282 Total Loss:  799.30770687960512 Model Losses:  [0, 0, 1.2575020482441228, 0, 0.72742301710910151, 0, 0.44469525641749952, 0, 0]\n",
      " 7/23 [========>.....................] - ETA: 10s - loss: 954.7986\n",
      "\n",
      "K Loss:  783.34953894755336 Reg Loss:  0.099724722740368379 Total Loss:  783.4492636702937 Model Losses:  [0, 0, 1.257503092729094, 0, 0.72742486218312641, 0, 0.4447088732905462, 0, 0]\n",
      " 8/23 [=========>....................] - ETA: 10s - loss: 945.0761\n",
      "\n",
      "K Loss:  768.93116767245328 Reg Loss:  0.099721918239431384 Total Loss:  769.03088959069271 Model Losses:  [0, 0, 1.2575057980040483, 0, 0.727428634846226, 0, 0.44472294856675471, 0, 0]\n",
      " 9/23 [==========>...................] - ETA: 9s - loss: 936.2110 \n",
      "\n",
      "K Loss:  755.5180313978052 Reg Loss:  0.099719554237623576 Total Loss:  755.6177509520428 Model Losses:  [0, 0, 1.2575099264518976, 0, 0.727433882435503, 0, 0.44473740200985379, 0, 0]\n",
      "10/23 [============>.................] - ETA: 8s - loss: 928.0465\n",
      "\n",
      "K Loss:  743.04213089487178 Reg Loss:  0.099717625529392478 Total Loss:  743.14184852040114 Model Losses:  [0, 0, 1.2575151168624277, 0, 0.7274403596303588, 0, 0.44475156656551351, 0, 0]\n",
      "11/23 [=============>................] - ETA: 8s - loss: 920.4656\n",
      "\n",
      "K Loss:  731.29915287298 Reg Loss:  0.099716001854697658 Total Loss:  731.39886887483476 Model Losses:  [0, 0, 1.2575211717609727, 0, 0.7274472788542512, 0, 0.44476547328727728, 0, 0]\n",
      "12/23 [==============>...............] - ETA: 7s - loss: 913.3786\n",
      "\n",
      "K Loss:  719.7574657635522 Reg Loss:  0.099714640715573871 Total Loss:  719.85718040426775 Model Losses:  [0, 0, 1.2575279687694247, 0, 0.72745494468657113, 0, 0.44477929431313978, 0, 0]\n",
      "13/23 [===============>..............] - ETA: 6s - loss: 906.7125\n",
      "\n",
      "K Loss:  708.96519460671345 Reg Loss:  0.099713709218638591 Total Loss:  709.06490831593214 Model Losses:  [0, 0, 1.2575351168971747, 0, 0.72746334763260934, 0, 0.4447933162390873, 0, 0]\n",
      "14/23 [=================>............] - ETA: 6s - loss: 900.4108\n",
      "\n",
      "K Loss:  698.35377485856759 Reg Loss:  0.099712998294018459 Total Loss:  698.45348785686156 Model Losses:  [0, 0, 1.2575431690237766, 0, 0.72747243949098361, 0, 0.44480693203662985, 0, 0]\n",
      "15/23 [==================>...........] - ETA: 5s - loss: 894.4266\n",
      "\n",
      "K Loss:  688.11374271623606 Reg Loss:  0.099712643876188986 Total Loss:  688.21345536011222 Model Losses:  [0, 0, 1.2575515992818012, 0, 0.72748240171594569, 0, 0.44482030863196587, 0, 0]\n",
      "16/23 [===================>..........] - ETA: 4s - loss: 888.7217\n",
      "\n",
      "K Loss:  678.12888034389709 Reg Loss:  0.099712620260758375 Total Loss:  678.22859296415788 Model Losses:  [0, 0, 1.2575606137989688, 0, 0.72749302531955307, 0, 0.44483364772758638, 0, 0]\n",
      "17/23 [=====================>........] - ETA: 4s - loss: 883.2641\n",
      "\n",
      "K Loss:  668.61737030832194 Reg Loss:  0.099712853507005011 Total Loss:  668.7170831618289 Model Losses:  [0, 0, 1.2575696744072384, 0, 0.72750415645803534, 0, 0.44484673164547522, 0, 0]\n",
      "18/23 [======================>.......] - ETA: 3s - loss: 878.0277\n",
      "\n",
      "K Loss:  659.12071000450567 Reg Loss:  0.099713238228521725 Total Loss:  659.22042324273423 Model Losses:  [0, 0, 1.2575790333912915, 0, 0.72751575593788187, 0, 0.44485945180366759, 0, 0]\n",
      "19/23 [=======================>......] - ETA: 2s - loss: 872.9898\n",
      "\n",
      "K Loss:  649.88998462516929 Reg Loss:  0.099713863811073666 Total Loss:  649.98969848898037 Model Losses:  [0, 0, 1.2575889219973801, 0, 0.72752778965071108, 0, 0.4448718974566423, 0, 0]\n",
      "20/23 [=========================>....] - ETA: 2s - loss: 868.1309\n",
      "\n",
      "K Loss:  640.71155156084228 Reg Loss:  0.099714641647018529 Total Loss:  640.81126620248926 Model Losses:  [0, 0, 1.2575991871350249, 0, 0.72754020289533894, 0, 0.44488397019239651, 0, 0]\n",
      "21/23 [==========================>...] - ETA: 1s - loss: 863.4341\n",
      "\n",
      "K Loss:  631.62243349393339 Reg Loss:  0.099715634423376123 Total Loss:  631.72214912835682 Model Losses:  [0, 0, 1.2576098058770817, 0, 0.72755304419614308, 0, 0.44489595300742918, 0, 0]\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 858.8847\n",
      "\n",
      "K Loss:  440.52182186510578 Reg Loss:  0.099716847250779742 Total Loss:  440.62153871235654 Model Losses:  [0, 0, 1.2576210041113816, 0, 0.72756679176522165, 0, 0.44490772605198936, 0, 0]\n",
      "23/23 [==============================] - 17s 665ms/step - loss: 849.7628\n",
      "Epoch 2/2\n",
      "\n",
      "\n",
      "K Loss:  616.78460811774153 Reg Loss:  0.099717755872465952 Total Loss:  616.884325873614 Model Losses:  [0, 0, 1.257627980927372, 0, 0.7275764985968165, 0, 0.44491564324072663, 0, 0]\n",
      " 1/23 [>.............................] - ETA: 13s - loss: 619.3144\n",
      "\n",
      "K Loss:  607.81750589950593 Reg Loss:  0.099719228260253576 Total Loss:  607.91722512776619 Model Losses:  [0, 0, 1.2576397844403302, 0, 0.7275911924388937, 0, 0.4449269369596886, 0, 0]\n",
      " 2/23 [=>............................] - ETA: 14s - loss: 617.0727\n",
      "\n",
      "K Loss:  599.31107519858892 Reg Loss:  0.0997209370768839 Total Loss:  599.41079613566581 Model Losses:  [0, 0, 1.2576522856901136, 0, 0.72760655264992891, 0, 0.44493818863362222, 0, 0]\n",
      " 3/23 [==>...........................] - ETA: 14s - loss: 614.8821\n",
      "\n",
      "K Loss:  590.73914174274773 Reg Loss:  0.099722838939751354 Total Loss:  590.83886458168752 Model Losses:  [0, 0, 1.2576657384231027, 0, 0.72762213781880847, 0, 0.44494897029208824, 0, 0]\n",
      " 4/23 [====>.........................] - ETA: 13s - loss: 612.7098\n",
      "\n",
      "K Loss:  582.35624402065366 Reg Loss:  0.099724881792810346 Total Loss:  582.45596890244644 Model Losses:  [0, 0, 1.2576791550080058, 0, 0.72763790746479751, 0, 0.44495966312994339, 0, 0]\n",
      " 5/23 [=====>........................] - ETA: 12s - loss: 610.5542\n",
      "\n",
      "K Loss:  574.12335336466538 Reg Loss:  0.0997270318424956 Total Loss:  574.22308039650784 Model Losses:  [0, 0, 1.2576928071456872, 0, 0.72765377618897742, 0, 0.44496995222185726, 0, 0]\n",
      " 6/23 [======>.......................] - ETA: 11s - loss: 608.4149\n",
      "\n",
      "K Loss:  565.75039785461274 Reg Loss:  0.099729315970004576 Total Loss:  565.85012717058271 Model Losses:  [0, 0, 1.2577065544159951, 0, 0.72766984100507048, 0, 0.444980185586465, 0, 0]\n",
      " 7/23 [========>.....................] - ETA: 10s - loss: 606.2861\n",
      "\n",
      "K Loss:  557.43026557987741 Reg Loss:  0.099731759977139109 Total Loss:  557.5299973398545 Model Losses:  [0, 0, 1.2577208908122233, 0, 0.7276859137443763, 0, 0.44499002375943575, 0, 0]\n",
      " 8/23 [=========>....................] - ETA: 9s - loss: 604.1652 \n",
      "\n",
      "K Loss:  549.70832924613433 Reg Loss:  0.09973432552971126 Total Loss:  549.80806357166409 Model Losses:  [0, 0, 1.257735606974143, 0, 0.7277021305603919, 0, 0.44499981301436436, 0, 0]\n",
      " 9/23 [==========>...................] - ETA: 9s - loss: 602.0579\n",
      "\n",
      "K Loss:  541.42727826895987 Reg Loss:  0.099736839282205669 Total Loss:  541.527015108242 Model Losses:  [0, 0, 1.2577505963553686, 0, 0.72771817396824223, 0, 0.44500927707350046, 0, 0]\n",
      "10/23 [============>.................] - ETA: 8s - loss: 599.9596\n",
      "\n",
      "K Loss:  533.50945849869356 Reg Loss:  0.099739553471371215 Total Loss:  533.609198052165 Model Losses:  [0, 0, 1.2577655166590254, 0, 0.72773445863045338, 0, 0.44501898623098246, 0, 0]\n",
      "11/23 [=============>................] - ETA: 8s - loss: 597.8706\n",
      "\n",
      "K Loss:  525.44513310850164 Reg Loss:  0.099742343449639434 Total Loss:  525.54487545195127 Model Losses:  [0, 0, 1.2577814043744591, 0, 0.727751111630618, 0, 0.4450283310723489, 0, 0]\n",
      "12/23 [==============>...............] - ETA: 7s - loss: 595.7894\n",
      "\n",
      "K Loss:  517.61631316357477 Reg Loss:  0.099745234532720953 Total Loss:  517.71605839810752 Model Losses:  [0, 0, 1.2577968508190878, 0, 0.7277676013815304, 0, 0.44503785486601244, 0, 0]\n",
      "13/23 [===============>..............] - ETA: 6s - loss: 593.7163\n",
      "\n",
      "K Loss:  509.91548217268456 Reg Loss:  0.0997481594413774 Total Loss:  510.01523033212595 Model Losses:  [0, 0, 1.2578132033648057, 0, 0.72778438010223556, 0, 0.44504707642673375, 0, 0]\n",
      "14/23 [=================>............] - ETA: 6s - loss: 591.6517\n",
      "\n",
      "K Loss:  502.13595406086341 Reg Loss:  0.099751123382303519 Total Loss:  502.23570518424572 Model Losses:  [0, 0, 1.2578286511123802, 0, 0.727801111784041, 0, 0.44505640243391886, 0, 0]\n",
      "15/23 [==================>...........] - ETA: 5s - loss: 589.5950\n",
      "\n",
      "K Loss:  494.29705418589947 Reg Loss:  0.099754187507244713 Total Loss:  494.39680837340671 Model Losses:  [0, 0, 1.2578454754854098, 0, 0.72781825256555388, 0, 0.44506509872366179, 0, 0]\n",
      "16/23 [===================>..........] - ETA: 4s - loss: 587.5455\n",
      "\n",
      "K Loss:  486.60771775096106 Reg Loss:  0.0997574082786024 Total Loss:  486.70747515923966 Model Losses:  [0, 0, 1.2578609023763145, 0, 0.72783529865578023, 0, 0.44507424790158218, 0, 0]\n",
      "17/23 [=====================>........] - ETA: 4s - loss: 585.5030\n",
      "\n",
      "K Loss:  479.17199840175243 Reg Loss:  0.099760630874902126 Total Loss:  479.27175903262736 Model Losses:  [0, 0, 1.2578790667208946, 0, 0.72785275267619753, 0, 0.44508253385480884, 0, 0]\n",
      "18/23 [======================>.......] - ETA: 3s - loss: 583.4679\n",
      "\n",
      "K Loss:  471.50151175026537 Reg Loss:  0.099763896162701177 Total Loss:  471.60127564642806 Model Losses:  [0, 0, 1.25789343709289, 0, 0.727869177112574, 0, 0.44509147478979472, 0, 0]\n",
      "19/23 [=======================>......] - ETA: 2s - loss: 581.4398\n",
      "\n",
      "K Loss:  464.24726830844588 Reg Loss:  0.099767121016481261 Total Loss:  464.34703542946238 Model Losses:  [0, 0, 1.2579136736417089, 0, 0.72788691766007, 0, 0.44509964925684964, 0, 0]\n",
      "20/23 [=========================>....] - ETA: 2s - loss: 579.4190\n",
      "\n",
      "K Loss:  456.69230471666106 Reg Loss:  0.099770485903484044 Total Loss:  456.79207520256455 Model Losses:  [0, 0, 1.2579248889930832, 0, 0.72790185220544024, 0, 0.44510870858519641, 0, 0]\n",
      "21/23 [==========================>...] - ETA: 1s - loss: 577.4053\n",
      "\n",
      "K Loss:  449.78708764752332 Reg Loss:  0.099773790338067714 Total Loss:  449.8868614378614 Model Losses:  [0, 0, 1.2579476915173509, 0, 0.72792082029091254, 0, 0.44511629514653861, 0, 0]\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 575.3993\n",
      "\n",
      "K Loss:  313.19781685832083 Reg Loss:  0.099777200818106884 Total Loss:  313.29759405913893 Model Losses:  [0, 0, 1.2579543261760986, 0, 0.72793232527408147, 0, 0.44512488803851719, 0, 0]\n",
      "23/23 [==============================] - 16s 681ms/step - loss: 571.1007\n",
      "[INFO]: Time taken for model training (time module): 33.96472883224487 seconds\n",
      "[INFO]: Time taken for model training (Keras): 32.411599101964384 seconds\n"
     ]
    }
   ],
   "source": [
    "hp = deepDMD.HyperParameters(hyper_param_dict)\n",
    "hp.model_name         = _LABEL\n",
    "\n",
    "performance_dict[\"n_epochs\"] = hp.ep\n",
    "performance_dict[\"batch_size\"] = hp.bs\n",
    "performance_dict[\"n_training_batches\"] = 1 - hp.vs\n",
    "performance_dict[\"n_val_batches\"] = hp.vs\n",
    "\n",
    "# ------------------------------- MODEL TRAINING ------------------------------------------------\n",
    "# Initialize, build, and fit the model\n",
    "m_start = time.time()\n",
    "BaselineModel = deepDMD.NeuralNetworkModel(hp)\n",
    "BaselineModel.compile(optimizer=tf.optimizers.Adagrad(hp.lr))\n",
    "\n",
    "history = BaselineModel.fit([X_array, Y_array], batch_size=hp.bs, \n",
    "                  epochs=hp.ep, \n",
    "                  callbacks=callbacks, \n",
    "                  shuffle=True)\n",
    "\n",
    "m_stop = time.time()\n",
    "\n",
    "# print info\n",
    "print('[INFO]: Time taken for model training (time module):', m_stop - m_start, 'seconds')\n",
    "print('[INFO]: Time taken for model training (Keras):', sum(timing_cb.logs), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaselineModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3a089-c406-475f-b4b7-2553820b9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaselineModel.encoder.output_layer.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db2a94-4b86-42c6-8ffd-80d9eb6aae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.scalar_mul(hp.rf, tf.norm(BaselineModel.KO, axis = [0,1], ord = 'fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a7406-5de9-4fb8-89d7-8a47b198da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.scalar_mul(hp.rf, tf.norm(trainer.encoder.KO, axis = [0,1], ord = 'fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d5caf-7335-44c4-9334-d740354ce89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaselineModel.K_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5399c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # @tf.function\n",
    "# def train_step(X, Y):\n",
    "#     # Open a GradientTape to record the operations run\n",
    "#     # during the forward pass, which enables auto-differentiation.\n",
    "#     with tf.GradientTape() as tape:\n",
    "\n",
    "#         # Run the forward pass of the layer.\n",
    "#         # The operations that the layer applies\n",
    "#         # to its inputs are going to be recorded\n",
    "#         # on the GradientTape.\n",
    "#         Psi_X    = K_model(X, training=True)\n",
    "#         Psi_Y    = K_model(Y, training=True)    \n",
    "\n",
    "#         PSI_X    = tf.concat([X, Psi_X], 1)\n",
    "#         PSI_Y    = tf.concat([Y, Psi_Y], 1) \n",
    "\n",
    "#         # 1-time step evolution on observable space:\n",
    "#         K_PSI_X  = tf.matmul(PSI_X, K_model.KO) \n",
    "\n",
    "#         # 1-step Koopman loss on observable space:        \n",
    "#         K_loss   = tf.norm(PSI_Y - K_PSI_X, axis = [0,1], ord = 'fro')\n",
    "\n",
    "#         # Regularization loss on Koopman operator:\n",
    "#         Reg_loss= tf.math.scalar_mul(hp.rf, tf.norm(K_model.KO, axis = [0,1], ord = 'fro'))   \n",
    "\n",
    "#         # Compute the loss value for this minibatch.\n",
    "#         loss_value = K_loss + Reg_loss\n",
    "\n",
    "#     # Use the gradient tape to automatically retrieve\n",
    "#     # the gradients of the trainable variables with respect to the loss.\n",
    "#     grads = tape.gradient(loss_value, K_model.trainable_weights)\n",
    "\n",
    "#     # Run one step of gradient descent by updating\n",
    "#     # the value of the variables to minimize the loss.\n",
    "#     optimizer.apply_gradients(zip(grads, K_model.trainable_weights))\n",
    "    \n",
    "#     return loss_value\n",
    "\n",
    "# @tf.function\n",
    "# def distributed_train_step(dist_inputs):\n",
    "#     per_replica_losses = mirrored_strategy.run(train_step, args=(dist_inputs[0],dist_inputs[1]))\n",
    "#     print(per_replica_losses)\n",
    "#     return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "#                          axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae55300",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m_start = time.time()\n",
    "# for epoch in range(hp.ep):\n",
    "#     print(\"\\nepoch {}/{}\".format(epoch+1, hp.ep))\n",
    "#     pb_i = Progbar(math.ceil(1440000//hp.bs)+1, stateful_metrics=['loss'])\n",
    "\n",
    "#     # Iterate over the batches of the dataset.\n",
    "#     total_loss = 0.0\n",
    "#     num_batches = 0\n",
    "#     for step, inp_data in enumerate(training_dataset):\n",
    "#         total_loss += distributed_train_step(inp_data)\n",
    "#         num_batches += 1\n",
    "        \n",
    "#         loss_value = total_loss / num_batches\n",
    "\n",
    "#         # Log every 200 batches.\n",
    "#         pb_i.add(hp.bs//hp.bs, values=[('loss', loss_value)])\n",
    "\n",
    "# m_stop = time.time()\n",
    "# print(m_stop-m_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d73260-6761-4a8d-8654-bc77d09cd02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784924b-b87c-416c-a531-c0d46a42da73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
