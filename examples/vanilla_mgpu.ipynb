{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import nvtx\n",
    "\n",
    "import argparse\n",
    "\n",
    "import math\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "# ------------------------------- CUSTOM FUNCTIONS ------------------------------------------------\n",
    "# Custom Library\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "    \n",
    "from proxy_apps.apps.timeseries_prediction import deepDMD, proxyDeepDMD, proxyDeepDMDMGPU, hyperparameters\n",
    "\n",
    "from proxy_apps.utils.tf import TimingCallback\n",
    "from proxy_apps.utils.data.main import NpEncoder\n",
    "from proxy_apps.utils import file_reader, path_handler\n",
    "from proxy_apps.utils.data.grid import GridNetworkDataHandler, GridNetworkTFDataHandler, GridNetworkNewGen, TransientDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = tf.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: [Training Script]: Tensorflow version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# System Setup\n",
    "config = file_reader.read_config()\n",
    "\n",
    "_N_EPOCHS = 2\n",
    "_BATCH_SIZE = 65536\n",
    "_APP_NAME = config[\"info\"][\"app_name\"]\n",
    "_NROWS = int(config[\"data\"][\"n_rows\"])\n",
    "_NCOLS = int(config[\"data\"][\"n_cols\"])\n",
    "_REPEAT_COLS = 1 # int(config[\"data\"][\"repeat_cols\"])\n",
    "_WINDOW_SIZE = int(config[\"data\"][\"window_size\"])\n",
    "_SHIFT_SIZE = int(config[\"data\"][\"shift_size\"])\n",
    "_STRIDE = int(config[\"data\"][\"stride\"])\n",
    "_N_SIGNALS = int(config[\"data\"][\"n_signals\"])\n",
    "\n",
    "_DTYPE = config[\"model\"][\"dtype\"]\n",
    "\n",
    "_LABEL = \"TFDataOpt\"\n",
    "_SUFFIX =  \"gpu\" + '_' + \\\n",
    "            \"a100\" + '_' + \\\n",
    "            'ng' + str(1) + '_' + \\\n",
    "            'nc' + str(-1) + '_' + \\\n",
    "            'e' + str(_N_EPOCHS) + '_' + \\\n",
    "            'b' + str(_BATCH_SIZE) + '_' + \\\n",
    "            'r' + str(_REPEAT_COLS) + '_' + _LABEL\n",
    "\n",
    "performance_dict = dict()\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# current directory\n",
    "curr_dir = \"./\"\n",
    "\n",
    "# output directory\n",
    "output_dir = path_handler.get_absolute_path(curr_dir, config[\"info\"][\"output_dir\"] + config[\"info\"][\"name\"] + \"/\" + config[\"info\"][\"app_name\"] + \"/\" + _DTYPE + \"/R\" + str(_REPEAT_COLS) + \"/\")\n",
    "if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "\n",
    "# TensorFlow Setup\n",
    "logger.info(\" [Training Script]: Tensorflow version: %s\", tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    log.info(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Eager mode: True\n"
     ]
    }
   ],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "logger.info(\"Eager mode: %s\", tf.executing_eagerly()) # For easy reset of notebook state.\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_floatx(_DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_dict = config[\"model\"][\"hyperparameters\"]\n",
    "hyper_param_dict['original_dim']       = _REPEAT_COLS * _NCOLS   # input data dimension\n",
    "hyper_param_dict['num_epochs']         = _N_EPOCHS  # Number of epochs  \n",
    "hyper_param_dict['batch_size']         = _BATCH_SIZE\n",
    "\n",
    "hyper_param_dict['dtype']         = _DTYPE\n",
    "hp = hyperparameters.HyperParameters(hyper_param_dict)\n",
    "hp.model_name         = _LABEL\n",
    "\n",
    "performance_dict[\"n_epochs\"] = hp.ep\n",
    "performance_dict[\"batch_size\"] = hp.bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexer(n_rows, window_size, shift_size, start_point, leave_last):\n",
    "    return np.arange(window_size)[None, :] + start_point + shift_size*np.arange(((n_rows - window_size - leave_last - start_point) // shift_size) + 1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow: [Training Script]: Time taken for loading datasets: 0.269189 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- DATA LOADING ------------------------------------------------   \n",
    "l_start = time.time()\n",
    "if _LABEL == \"Baseline\":\n",
    "    data_handler = GridNetworkDataHandler(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                            n_rows=_NROWS,\n",
    "                                            n_cols=_NCOLS,\n",
    "                                            repeat_cols=_REPEAT_COLS,\n",
    "                                            dtype=_DTYPE\n",
    "                                         ) \n",
    "\n",
    "    scenario_data = data_handler.load_grid_data()\n",
    "elif _LABEL == \"TFDataOpt\":\n",
    "    data_handler = GridNetworkNewGen(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                            n_rows=_NROWS,\n",
    "                                            n_cols=_NCOLS,\n",
    "                                            repeat_cols=_REPEAT_COLS,\n",
    "                                            d_type=_DTYPE\n",
    "                                         )\n",
    "\n",
    "    x_indexer = get_indexer(_NROWS, _WINDOW_SIZE, _SHIFT_SIZE, 0, _N_SIGNALS)\n",
    "    y_indexer = get_indexer(_NROWS, _WINDOW_SIZE, _SHIFT_SIZE, 1, 0)\n",
    "\n",
    "    scenario_data = data_handler.get_training_data(x_indexer, y_indexer)\n",
    "elif _LABEL == \"TFDataOptPrev\":\n",
    "    data_handler = GridNetworkTFDataHandler(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                            n_rows=_NROWS,\n",
    "                                            n_cols=_NCOLS,\n",
    "                                            repeat_cols=_REPEAT_COLS,\n",
    "                                            dtype=_DTYPE\n",
    "                                         ) \n",
    "\n",
    "    scenario_data = data_handler.load_grid_data()\n",
    "\n",
    "l_stop = time.time()\n",
    "logger.info(' [Training Script]: Time taken for loading datasets: %f seconds', l_stop - l_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = GridNetworkNewGen(scenario_dir=path_handler.get_absolute_path(curr_dir, config[\"info\"][\"input_dir\"]),\n",
    "                                    n_rows=_NROWS,\n",
    "                                    n_cols=_NCOLS,\n",
    "                                    repeat_cols=_REPEAT_COLS,\n",
    "                                    d_type=_DTYPE\n",
    "                                 )\n",
    "\n",
    "yp_indexer = get_indexer(_NROWS, _NROWS-1, _SHIFT_SIZE, 0, 1)\n",
    "yf_indexer = get_indexer(_NROWS, _NROWS-1, _SHIFT_SIZE, 1, 0)\n",
    "\n",
    "scenario_data = data_handler.get_training_data(yp_indexer, yf_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41970\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for a, b in scenario_data:\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indexer(_NROWS, 1399, _SHIFT_SIZE, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LABEL in [\"Baseline\", \"TFDataOptPrev\"]:\n",
    "    # ------------------------------- DATA PREPROCESSING ------------------------------------------------\n",
    "    i_start = time.time()\n",
    "    X_data, Y_data, U_data, V_data, Yp, Yf = data_handler.create_windows(scenario_data)\n",
    "    i_stop = time.time()\n",
    "    logger.info(' [Training Script]: Time taken for creating datasets: %f seconds', i_stop - i_start)\n",
    "    \n",
    "    # ------------------------------- DATA NORMALIZATION ------------------------------------------------\n",
    "    n_start = time.time()\n",
    "    X_array, Y_array, U_array, V_array, Yp_array, Yf_array = data_handler.scale_data(X_data, Y_data, U_data, V_data, Yp, Yf)\n",
    "    n_stop = time.time()\n",
    "    logger.info(' [Training Script]: Time taken for normalization: %f seconds', n_stop - n_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- MODEL TRAINING ------------------------------------------------\n",
    "model = proxyDeepDMDMGPU.Encoder(hp)\n",
    "optimizer = tf.optimizers.Adagrad(hp.lr)\n",
    "\n",
    "# compile and fit model\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "trainer = proxyDeepDMDMGPU.NeuralNetworkModel(hp, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing callback\n",
    "timing_cb = TimingCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=(None, _REPEAT_COLS * _NCOLS))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LABEL == \"Baseline\":\n",
    "    m_start = time.time()\n",
    "    trainer.fit(zip(X_array, Y_array), n_epochs=2, batch_size=_BATCH_SIZE, steps_per_epoch=22)\n",
    "    m_stop = time.time()\n",
    "elif _LABEL == \"TFDataOpt\":\n",
    "    data_options = tf.data.Options()\n",
    "    data_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    training_dataset = scenario_data.with_options(data_options).batch(hp.bs)\n",
    "\n",
    "    training_dataset = training_dataset.cache()\n",
    "    shuffle_buffer_size = x_indexer.shape[0]*x_indexer.shape[1]*len(data_handler.dir_list) // _BATCH_SIZE\n",
    "    training_dataset = training_dataset.shuffle(buffer_size=x_indexer.shape[0]*x_indexer.shape[1]*len(data_handler.dir_list)//_BATCH_SIZE)\n",
    "    training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    m_start = time.time()\n",
    "    trainer.fit(training_dataset, n_epochs=_N_EPOCHS, batch_size=_BATCH_SIZE, steps_per_epoch=22)\n",
    "    m_stop = time.time()\n",
    "elif _LABEL == \"TFDataOptPrev\":\n",
    "    # tensorflow dataset conversion\n",
    "    tf_conv_start = time.time()\n",
    "    training_dataset = tf.data.Dataset.zip((X_array, Y_array)).batch(_BATCH_SIZE)\n",
    "    training_dataset = training_dataset.cache()\n",
    "    training_dataset = training_dataset.shuffle(buffer_size=22)\n",
    "    training_dataset = training_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    tf_conv_stop = time.time()\n",
    "    logger.info('[INFO]: Tensorflow Conversion Time: %f seconds', tf_conv_stop - tf_conv_start)\n",
    "    \n",
    "    m_start = time.time()\n",
    "    trainer.fit(training_dataset, n_epochs=_N_EPOCHS, batch_size=_BATCH_SIZE, steps_per_epoch=22)\n",
    "    m_stop = time.time()\n",
    "    \n",
    "# print info\n",
    "logger.info('[INFO]: Time taken for model training (time module): %f seconds', m_stop - m_start)\n",
    "logger.info('[INFO]: Time taken for model training (Keras): %f seconds', sum(timing_cb.logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.encoder.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
