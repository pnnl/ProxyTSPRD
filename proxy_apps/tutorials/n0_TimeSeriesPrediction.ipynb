{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faced-tribe",
   "metadata": {},
   "source": [
    "# DeepDMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-authority",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decreased-patio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tensorflow version:  2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "print(\"[INFO] Tensorflow version: \", tf.__version__)\n",
    "# print(\"[INFO] Eager mode: \", tf.executing_eagerly()) # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accessory-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from proxy_apps.data_handler import grid_network\n",
    "from proxy_apps.apps.timeseries_prediction import deepDMD\n",
    "from proxy_apps.plot_lib.simple_plots import eigen_plot, validation_plot, heatmap_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "every-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1\n",
    "timing_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-skiing",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "The input data for the model should be in the form of array. The output array (Y) will have same number of features as the input array (X) but Y would be a time shifted version of the X. In the case of DeepDMD, the shift was equal to 1. In the original application, the data is not sequential and rather restructured using rolling window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "placed-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Loading the datasets from the directory: ../../../../data/TrainingDataIEEE68bus\n",
      "[INFO]: Loading data for 30 scenarios ...\n"
     ]
    }
   ],
   "source": [
    "scenario_dir = '../../../../data/TrainingDataIEEE68bus'\n",
    "print('[INFO]: Loading the datasets from the directory:', scenario_dir)\n",
    "dir_list = os.listdir(scenario_dir)\n",
    "\n",
    "# Indicate the scenario range\n",
    "Dataset = dict()\n",
    "print('[INFO]: Loading data for %d scenarios ...' % len(dir_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-singing",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "assured-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Time taken for loading datasets: 11.577550172805786 seconds\n",
      "[INFO]: Total number of scenarios loaded: 30\n",
      "[INFO]: Shape of each scenario original:  (1400, 136)\n",
      "[INFO]: Shape of each scenario loaded:  (1000, 136)\n",
      "[INFO]: Done ...\n"
     ]
    }
   ],
   "source": [
    "# step-1: load data\n",
    "l_start = time.time()\n",
    "scenario_data = []\n",
    "count = 0\n",
    "for s_dir in dir_list:\n",
    "    if s_dir.find('scenario') == -1:\n",
    "        continue\n",
    "    dataset = grid_network.TransientDataset('%s/%s/' % (scenario_dir, s_dir))\n",
    "    original_shape = np.concatenate((dataset.F, dataset.Vm), axis=1).shape\n",
    "    scenario_data.append(np.concatenate((dataset.F[:1000,:], dataset.Vm[:1000,:]), axis=1))\n",
    "    count += 1\n",
    "    if count % 50 == 0:\n",
    "        print('[INFO]: Loaded %d/%d scenarios ...' % (count, len(dir_list)))\n",
    "l_stop = time.time()\n",
    "print('[INFO]: Time taken for loading datasets:', l_stop - l_start, 'seconds')\n",
    "print('[INFO]: Total number of scenarios loaded:', len(scenario_data))\n",
    "print('[INFO]: Shape of each scenario original: ', original_shape)\n",
    "print('[INFO]: Shape of each scenario loaded: ', scenario_data[0].shape)\n",
    "print('[INFO]: Done ...')\n",
    "\n",
    "timing_dict['load_data'] = (l_stop - l_start)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-blood",
   "metadata": {},
   "source": [
    "#### Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "available-universe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Time taken for creating X datasets: 4.359763860702515 seconds\n",
      "[INFO]: Original dataset size: 1000\n",
      "[INFO]: Chosen dataset size: 800\n",
      "[INFO]: Length of X_data:  600\n",
      "[INFO]: Length of each window after down sampling:  (800, 136)\n"
     ]
    }
   ],
   "source": [
    "# step-2: create data\n",
    "i_start = time.time()\n",
    "X_data = [] # Original data\n",
    "Y_data = [] # 1 time-shifted data\n",
    "U_data = [] # 2 time-shifted data\n",
    "V_data = [] # 3 time-shifted data\n",
    "whole_data = [] # Complete data \n",
    "Yp = [] # For analytical calculations\n",
    "Yf = [] # For analytical calculations\n",
    "count  = 0\n",
    "ds  = 1\n",
    "M = 2 # signifies number of time-shifts\n",
    "N = 3 # signifies number of time-shifts\n",
    "\n",
    "for dataset in scenario_data:    \n",
    "    dataset_size = dataset.shape[0]\n",
    "    whole_data.append(dataset)      \n",
    "    Yp.append(dataset[:-1,:])\n",
    "    Yf.append(dataset[1:,:])\n",
    "    count += 1\n",
    "    if count % 50 == 0:\n",
    "        print('Done processing %d/%d datasets ...' % (count, len(scenario_data)))\n",
    "    window_size = 800 # length of moving window\n",
    "    shift_size  = 10 # separation between two moving windows\n",
    "    i = 0\n",
    "    while (i*shift_size+window_size+M+N) <= dataset_size:\n",
    "        X_indices = range(i*shift_size, i*shift_size + window_size,ds)        \n",
    "        Y_indices = range(i*shift_size+1, i*shift_size + window_size+1,ds)               \n",
    "        U_indices = range(i*shift_size+M, i*shift_size + window_size+M,ds)               \n",
    "        V_indices = range(i*shift_size+N, i*shift_size + window_size+N,ds)   \n",
    "        if count < 0:\n",
    "            print(X_indices)\n",
    "            print(Y_indices)\n",
    "            print(U_indices)\n",
    "            print(V_indices)        \n",
    "\n",
    "        i = i + 1\n",
    "        X_data.append(dataset[X_indices])\n",
    "        Y_data.append(dataset[Y_indices])\n",
    "        U_data.append(dataset[U_indices])\n",
    "        V_data.append(dataset[V_indices])\n",
    "\n",
    "i_stop = time.time()\n",
    "print('[INFO]: Time taken for creating X datasets:', i_stop - i_start, 'seconds')\n",
    "print('[INFO]: Original dataset size:', dataset_size)\n",
    "print('[INFO]: Chosen dataset size:', window_size)\n",
    "print('[INFO]: Length of X_data: ', len(X_data))\n",
    "print('[INFO]: Length of each window after down sampling: ', X_data[0].shape)\n",
    "\n",
    "timing_dict['create_data'] = (i_stop - i_start)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-founder",
   "metadata": {},
   "source": [
    "#### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "endless-baptist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: whole_data_array shape: (30000, 136)\n",
      "[INFO]: Yp_array shape:  (29970, 136)\n",
      "[INFO]: Yf_array shape:  (29970, 136)\n",
      "[INFO]: X_array shape:  (480000, 136)\n",
      "[INFO]: Y_array shape:  (480000, 136)\n",
      "[INFO]: U_array shape:  (480000, 136)\n",
      "[INFO]: V_array shape:  (480000, 136)\n",
      "[INFO]: Time taken for normalization: 42.69359278678894 seconds\n"
     ]
    }
   ],
   "source": [
    "# step-3: normalization\n",
    "n_start = time.time()\n",
    "Normalization = 1\n",
    "scale_factor = 2*np.pi \n",
    "\n",
    "X_array = np.asarray(X_data).transpose(2,0,1).reshape(dataset.shape[1],-1).transpose()\n",
    "Y_array = np.asarray(Y_data).transpose(2,0,1).reshape(dataset.shape[1],-1).transpose()\n",
    "U_array = np.asarray(U_data).transpose(2,0,1).reshape(dataset.shape[1],-1).transpose()\n",
    "V_array = np.asarray(V_data).transpose(2,0,1).reshape(dataset.shape[1],-1).transpose()\n",
    "whole_data_array = np.asarray(whole_data).transpose(2,0,1).reshape(dataset.shape[1],-1).transpose()\n",
    "Yp_array = np.asarray(Yp).transpose(2,0,1).reshape(dataset.shape[1],-1).transpose()\n",
    "Yf_array = np.asarray(Yf).transpose(2,0,1).reshape(dataset.shape[1],-1).transpose()\n",
    "print(\"[INFO]: whole_data_array shape:\", whole_data_array.shape)\n",
    "print('[INFO]: Yp_array shape: ', Yp_array.shape)\n",
    "print('[INFO]: Yf_array shape: ', Yf_array.shape)\n",
    "print('[INFO]: X_array shape: ', X_array.shape)\n",
    "print('[INFO]: Y_array shape: ', Y_array.shape)\n",
    "print('[INFO]: U_array shape: ', U_array.shape)\n",
    "print('[INFO]: V_array shape: ', V_array.shape)\n",
    "\n",
    "if Normalization:  \n",
    "    X_array_old  = X_array\n",
    "    Y_array_old  = Y_array\n",
    "    U_array_old  = U_array\n",
    "    V_array_old  = V_array\n",
    "    Yp_array_old = Yp_array\n",
    "    Yf_array_old = Yf_array\n",
    "    X_array      = np.concatenate((scale_factor*(X_array_old[:,:68] - 60), 10*(X_array_old[:,68:] - 1)), axis = 1) \n",
    "    Y_array      = np.concatenate((scale_factor*(Y_array_old[:,:68] - 60), 10*(Y_array_old[:,68:] - 1)), axis = 1) \n",
    "    U_array      = np.concatenate((scale_factor*(U_array_old[:,:68] - 60), 10*(U_array_old[:,68:] - 1)), axis = 1) \n",
    "    V_array      = np.concatenate((scale_factor*(V_array_old[:,:68] - 60), 10*(V_array_old[:,68:] - 1)), axis = 1) \n",
    "    Yp_array     = np.concatenate((scale_factor*(Yp_array_old[:,:68] - 60), 10*(Yp_array_old[:,68:] - 1)), axis = 1)\n",
    "    Yf_array     = np.concatenate((scale_factor*(Yf_array_old[:,:68] - 60), 10*(Yf_array_old[:,68:] - 1)), axis = 1)    \n",
    "        \n",
    "n_stop = time.time()\n",
    "print('[INFO]: Time taken for normalization:', n_stop - n_start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "partial-montgomery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29970, 136)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yp_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "convinced-palestinian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14165288, -0.03844356, -0.01614739, -0.02112808, -0.03753495,\n",
       "       -0.03595576, -0.05122148, -0.05883799, -0.18163594, -0.01606364,\n",
       "       -0.02258537, -0.01816804, -0.01371952, -0.00716514,  0.03021367,\n",
       "        0.04639342,  0.02124267,  0.00693749,  0.0406679 ,  0.05210838])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array[0, :20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-europe",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "illegal-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "blessed-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    def __init__(self, config):\n",
    "        self.od = config['original_dim']\n",
    "        self.h1 = config['hl_1_dim']\n",
    "        self.h2 = config['hl_2_dim']\n",
    "        self.h3 = config['hl_3_dim']\n",
    "        self.h4 = config['hl_4_dim']\n",
    "        ## hp.h5 = config['hl_5_dim']\n",
    "        self.ld = config['latent_dim']\n",
    "        self.rf = config['reg_factor']\n",
    "        self.dr = config['dropout_prob']\n",
    "        self.wr = config['weight_regularizer']\n",
    "        self.br = config['bias_regularizer']\n",
    "        self.ep = config['num_epochs']\n",
    "        self.lr = config['learning_rate']\n",
    "        self.bs = config['batch_size']\n",
    "        self.vs = config['validation_split']\n",
    "\n",
    "# Neural Network\n",
    "class NeuralNetworkModel(tf.keras.Model): \n",
    "    def __init__(self, hp):\n",
    "        super(NeuralNetworkModel, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(hp)        \n",
    "        \n",
    "        # Define and randomly initialize the Koopman operator\n",
    "        self.KO = tf.Variable(tf.random.normal(shape = (hp.ld+hp.od, hp.ld+hp.od), mean=0.0, stddev=0.05, \n",
    "                                                    dtype=tf.dtypes.float64, seed=123321, name='KoopmanOperator'),\n",
    "                                                    trainable=True)\n",
    "        self.rf = hp.rf \n",
    "        \n",
    "    def call(self, inputs):       \n",
    "        X        = inputs[0]\n",
    "        Y        = inputs[1]   \n",
    "        \n",
    "        Psi_X    = self.encoder(X)\n",
    "        Psi_Y    = self.encoder(Y)        \n",
    "        \n",
    "        PSI_X    = tf.concat([X, Psi_X], 1)\n",
    "        PSI_Y    = tf.concat([Y, Psi_Y], 1) \n",
    "        \n",
    "        # 1-time step evolution on observable space:\n",
    "        K_PSI_X  = tf.matmul(PSI_X, self.KO) \n",
    "        \n",
    "        # 1-step Koopman loss on observable space:        \n",
    "        K_loss   = tf.norm(PSI_Y - K_PSI_X, axis = [0,1], ord = 'fro')\n",
    "        \n",
    "        # Regularization loss on Koopman operator:\n",
    "        Reg_loss= tf.cast(tf.math.scalar_mul(self.rf,tf.norm(self.KO, axis = [0,1], ord = 'fro')), dtype = 'float64')        \n",
    "        \n",
    "        # Total loss:\n",
    "        loss = K_loss + Reg_loss \n",
    "        \n",
    "        self.add_loss(loss)\n",
    "        return Psi_X, PSI_X, Psi_Y, PSI_Y, K_loss\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hps):\n",
    "        super(Encoder, self).__init__(dtype = 'float64', name = 'Encoder')\n",
    "        self.input_layer   = DenseLayer(hps.h1, hps.od, 0.0, 0.0)\n",
    "        self.hidden_layer1 = DenseLayer(hps.h2, hps.h1, hps.wr, hps.br)\n",
    "        self.dropout_laye1 = tf.keras.layers.Dropout(hps.dr)\n",
    "        self.hidden_layer2 = DenseLayer(hps.h3, hps.h2, hps.wr, hps.br)        \n",
    "        self.dropout_laye2 = tf.keras.layers.Dropout(hps.dr)\n",
    "        self.hidden_layer3 = DenseLayer(hps.h4, hps.h3, hps.wr, hps.br)\n",
    "        self.dropout_laye3 = tf.keras.layers.Dropout(hps.dr)           \n",
    "#         self.hidden_layer4 = DenseLayer(hps.h5, hps.h4, hps.wr, hps.br)\n",
    "#         self.dropout_laye4 = layers.Dropout(hps.dr)             \n",
    "        self.output_layer  = LinearLayer(hps.ld, hps.h4, hps.wr, hps.br)\n",
    "        \n",
    "    def call(self, input_data, training):\n",
    "        fx = self.input_layer(input_data)        \n",
    "        fx = self.hidden_layer1(fx)\n",
    "        if training:\n",
    "            fx = self.dropout_laye1(fx)     \n",
    "        fx = self.hidden_layer2(fx)\n",
    "        if training:\n",
    "            fx = self.dropout_laye2(fx) \n",
    "        fx = self.hidden_layer3(fx)\n",
    "        if training:\n",
    "            fx = self.dropout_laye3(fx) \n",
    "#         fx = self.hidden_layer4(fx)\n",
    "#         if training:\n",
    "#             fx = self.dropout_laye4(fx)\n",
    "        return self.output_layer(fx)    \n",
    "\n",
    "class LinearLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, input_dim, weights_regularizer, bias_regularizer):\n",
    "        super(LinearLayer, self).__init__(dtype = 'float64')\n",
    "        self.w = self.add_weight(shape = (input_dim, units), \n",
    "                                initializer = tf.keras.initializers.RandomUniform(\n",
    "                                minval=-tf.cast(tf.math.sqrt(6/(input_dim+units)), dtype = 'float64'), \n",
    "                                maxval=tf.cast(tf.math.sqrt(6/(input_dim+units)), dtype = 'float64'), \n",
    "                                seed=16751),                                                                   \n",
    "#                               regularizer = tf.keras.regularizers.l1(weights_regularizer), \n",
    "                                trainable = True)\n",
    "        self.b = self.add_weight(shape = (units,),    \n",
    "                                 initializer = tf.zeros_initializer(),\n",
    "                                 regularizer = tf.keras.regularizers.l1(bias_regularizer),\n",
    "                                 trainable = True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, input_dim, weights_regularizer, bias_regularizer):\n",
    "        super(DenseLayer, self).__init__(dtype = 'float64')\n",
    "        self.w = self.add_weight(shape = (input_dim, units), \n",
    "                                 initializer = tf.keras.initializers.RandomUniform(\n",
    "                                     minval=-tf.cast(tf.math.sqrt(6.0/(input_dim+units)), dtype = 'float64'),  \n",
    "                                     maxval=tf.cast(tf.math.sqrt(6.0/(input_dim+units)) , dtype = 'float64'),  \n",
    "                                     seed=16751), \n",
    "                                 regularizer = tf.keras.regularizers.l1(weights_regularizer), \n",
    "                                 trainable = True)\n",
    "        self.b = self.add_weight(shape = (units,),    \n",
    "                                 initializer = tf.zeros_initializer(),\n",
    "                                 regularizer = tf.keras.regularizers.l1(bias_regularizer),\n",
    "                                 trainable = True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, self.w) + self.b\n",
    "        return tf.nn.elu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyper_param_dict = dict()\n",
    "hyper_param_dict['original_dim']       = 136   # input data dimension\n",
    "hyper_param_dict['hl_1_dim']           = 128   # Hidden layer dimension\n",
    "hyper_param_dict['hl_2_dim']           = 128   # Hidden layer dimension\n",
    "hyper_param_dict['hl_3_dim']           = 64   # Hidden layer dimension\n",
    "hyper_param_dict['hl_4_dim']           = 64   # Hidden layer dimension\n",
    "# hyper_param_dict['hl_5_dim']           = 64   # Hidden layer dimension\n",
    "hyper_param_dict['latent_dim']         = 64 # Latent space dimension \n",
    "hyper_param_dict['reg_factor']         = 0 # regularization weight for Koopman\n",
    "hyper_param_dict['dropout_prob']       = 0.005  # dropout rate [regularization between hidden layers]\n",
    "hyper_param_dict['weight_regularizer'] = 0.001  # [L1] regularization weight for dense layer weights \n",
    "hyper_param_dict['bias_regularizer']   = 0    # [L1] regularization bias for dense layer biases\n",
    "hyper_param_dict['num_epochs']         = 2  # Number of epochs \n",
    "hyper_param_dict['learning_rate']      = 5e-4 # learning rate for optimizer \n",
    "hyper_param_dict['validation_split']   = 0.25\n",
    "hyper_param_dict['batch_size']         = 32\n",
    "\n",
    "# Initialize Hyperparameters - we can keep it as a dict instead of creating a separate class\n",
    "hp = deepDMD.HyperParameters(hyper_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-resource",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jain432\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "11250/11250 [==============================] - 1011s 90ms/step - loss: 4.7748 - val_loss: 2.7804\n",
      "Epoch 2/2\n",
      "  127/11250 [..............................] - ETA: 13:51 - loss: 3.5199"
     ]
    }
   ],
   "source": [
    "# Stopping criteria if the training loss doesn't go down by 1e-3\n",
    "CallBack = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta = 1e-3, verbose = 1, mode='min', patience = 3, \n",
    "    baseline=None, restore_best_weights=True)\n",
    "K_model = NeuralNetworkModel(hp)\n",
    "K_model.compile(optimizer=tf.optimizers.Adagrad(hp.lr))\n",
    "history = K_model.fit([X_array, Y_array], validation_split = hp.vs, batch_size = hp.bs, \n",
    "                   epochs=hp.ep, callbacks=[CallBack], shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-vertical",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi_X, PSI_X, Psi_Y, PSI_Y, Kloss = K_model([Yp_array, Yf_array], training=False)\n",
    "print(\"Koopman loss: %.4f\" %Kloss.numpy())\n",
    "\n",
    "print('Psi_X shape:', Psi_X.numpy().shape)\n",
    "print('Psi_Y shape:', Psi_Y.numpy().shape)\n",
    "print('PSI_X shape:', PSI_X.numpy().shape)\n",
    "print('PSI_X shape:', PSI_Y.numpy().shape)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "validation_plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-photograph",
   "metadata": {},
   "source": [
    "### Find the Koopman operator and check its eigenvalues and sparsity structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_deepDMD = K_model.KO.numpy()\n",
    "\n",
    "print('[INFO]: Shape of Koopman operator', K_deepDMD.shape)\n",
    "print('[INFO]: Norm of Koopman operator', np.linalg.norm(K_deepDMD))\n",
    "print('[INFO]: Trace of K_deepDMD:',np.trace(K_deepDMD))\n",
    "print('[INFO]: One time-step error with K_deepDMD:', np.linalg.norm(PSI_Y - np.matmul(PSI_X, K_deepDMD), ord = 'fro'))\n",
    "\n",
    "[eigenvaluesK, eigenvectorsK] = np.linalg.eig(K_deepDMD)\n",
    "eigen_plot(eigenvaluesK.real, eigenvaluesK.imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_matplotlib(K_deepDMD, title='Koopman operator structure')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
